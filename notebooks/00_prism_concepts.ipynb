{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# PRISM — Mathematical and Algorithmic Concepts\n",
    "\n",
    "This notebook explains the mathematical foundations of the PRISM project in an interactive way.\n",
    "Each section introduces a concept with its formula, then offers a widget to explore it visually.\n",
    "\n",
    "**Dependencies:** numpy, matplotlib, ipywidgets (no need for MiniGrid).\n",
    "\n",
    "**Appendices:**\n",
    "- `00a_spectral_deep_dive.ipynb` — Eigendecomposition of M\n",
    "- `00b_calibration_methods.ipynb` — Calibration metrics (ECE, MI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec0-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0 — Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec0-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import sys, os\n",
    "\n",
    "# Ensure prism package is importable\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from prism.pedagogy.toy_grid import ToyGrid\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec0-grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grille de référence : deux pièces connectées par un passage\n",
    "grid = ToyGrid.two_rooms()\n",
    "print(f\"Grille {grid.rows}×{grid.cols}, {grid.n_states} états accessibles, 4 murs\")\n",
    "print(f\"Goal : {grid.goal}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "grid.plot(ax=axes[0], title=\"Structure de la grille\")\n",
    "\n",
    "# Numéroter les états sur fond neutre\n",
    "grid.plot(ax=axes[1], title=\"Index des états\")\n",
    "for i, pos in grid.idx_to_pos.items():\n",
    "    axes[1].text(pos[1], pos[0], str(i), ha='center', va='center',\n",
    "                 fontsize=7, fontweight='bold', color='steelblue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — The Successor Representation (SR)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine an agent randomly walking around the grid. At each step, it can move in 4 directions (up, down, left, right).\n",
    "\n",
    "The **Successor Representation** ($M$) is a large table that answers this question for **every pair of cells**:\n",
    "\n",
    "> \"If I start from cell $s$, **how many times** will I visit cell $s'$ in the future?\"\n",
    "\n",
    "Concretely, $M$ is a **matrix** (a two-dimensional table):\n",
    "- **Row** = starting cell $s$\n",
    "- **Column** = destination cell $s'$\n",
    "- **Value** $M(s, s')$ = expected visitation frequency of $s'$ starting from $s$\n",
    "\n",
    "Our grid has 21 accessible cells, so $M$ is a 21×21 = 441-value table. Each row is a \"heat map\" that says: \"from this starting cell, here are the cells I will visit the most\".\n",
    "\n",
    "For example, the row of $M$ for the top-left corner will show:\n",
    "- **Nearby** cells with high values (visited often)\n",
    "- **Distant** cells or those behind a wall with low values (rarely reached)\n",
    "\n",
    "### Why this is useful\n",
    "\n",
    "$M$ allows the agent to **evaluate any goal instantly**.\n",
    "\n",
    "Example: food is placed in cell 15. The agent wants to know \"is cell $s$ a good starting point for reaching the food?\". It simply reads $M(s, 15)$: if the value is high, it will pass through cell 15 often, so it is a good starting point.\n",
    "\n",
    "And if the food **changes location** (say cell 3)? The agent does not need to relearn everything — it reads $M(s, 3)$ instead. The table $M$ stays the same; only the goal changes.\n",
    "\n",
    "This is the strength of the SR: it separates **\"where I can go\"** ($M$, the structure) from **\"where I want to go\"** (the reward). We will formalize this idea in Section 3 with $V = M \\cdot R$.\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$M(s, s') = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t \\, \\mathbb{1}(s_t = s') \\;\\middle|\\; s_0 = s\\right]$$\n",
    "\n",
    "Let us break it down term by term:\n",
    "- $s$: the starting cell\n",
    "- $s'$: any cell whose visitation frequency we want to know\n",
    "- $\\mathbb{1}(s_t = s')$: equals 1 if the agent is at $s'$ at time $t$, 0 otherwise\n",
    "- $\\gamma^t$: a weight that **decreases with time** ($\\gamma < 1$), so distant visits count less\n",
    "- $\\sum_{t=0}^{\\infty}$: we sum over the entire future\n",
    "- $\\mathbb{E}[...]$: we average over all possible trajectories\n",
    "\n",
    "The parameter $\\gamma$ controls **how far ahead the agent looks**:\n",
    "- $\\gamma = 0.5$ → the agent only considers its immediate neighbors (horizon ≈ 2 steps)\n",
    "- $\\gamma = 0.95$ → the agent anticipates ~20 steps into the future\n",
    "- $\\gamma = 0.99$ → the agent sees very far (horizon ≈ 100 steps)\n",
    "\n",
    "### The transition matrix $T$\n",
    "\n",
    "To compute $M$, we need to know **how the agent moves**. This is what the transition matrix $T$ describes:\n",
    "\n",
    "$$T_{ij} = P(s' = j \\mid s = i)$$\n",
    "\n",
    "In plain terms: $T_{ij}$ is the **probability of going from cell $i$ to cell $j$** in one step.\n",
    "\n",
    "In our grid, the policy is uniform (4 directions equally probable), so:\n",
    "- If $j$ is an accessible neighbor of $i$: $T_{ij} = 0.25$\n",
    "- If $j$ is a wall or not adjacent: $T_{ij} = 0$\n",
    "- If the agent tries to go into a wall, it stays in place: this increases $T_{ii}$\n",
    "\n",
    "$T$ summarizes the entire **environment structure** + the **agent's policy**.\n",
    "\n",
    "### Computing $M$\n",
    "\n",
    "We can decompose $M$ by summing the contributions of each time step:\n",
    "\n",
    "$$M = \\underbrace{I}_{t=0} + \\underbrace{\\gamma \\, T}_{t=1} + \\underbrace{\\gamma^2 T^2}_{t=2} + \\underbrace{\\gamma^3 T^3}_{t=3} + \\ldots$$\n",
    "\n",
    "Each term has a concrete meaning:\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| $I$ | At $t=0$, the agent is at its starting cell (contribution = 1) |\n",
    "| $\\gamma \\, T$ | At $t=1$, it has taken one step → $T$ gives the probabilities of reaching each cell |\n",
    "| $\\gamma^2 T^2$ | At $t=2$, it has taken two steps → $T^2$ gives the probabilities in 2 steps |\n",
    "| $\\gamma^t T^t$ | At any $t$, weighted by $\\gamma^t$ (distant steps count less) |\n",
    "\n",
    "This infinite sum converges (because $\\gamma < 1$) and equals:\n",
    "\n",
    "$$M = (I - \\gamma T)^{-1}$$\n",
    "\n",
    "The plot below shows **one row** of this table: for a chosen starting state, the visitation frequency of each cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec1-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sr_interactive(gamma, state):\n",
    "    \"\"\"Affiche M*[s, :] pour un état de départ donné.\"\"\"\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    horizon = 1 / (1 - gamma)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "    row = M_star[state]\n",
    "    row_norm = row / row.max()\n",
    "    grid.plot(values=row_norm, ax=ax,\n",
    "              title=f\"Occupancy future depuis s={state} (γ={gamma:.2f})\",\n",
    "              cmap='plasma', vmin=0, vmax=1)\n",
    "    pos = grid.idx_to_pos[state]\n",
    "    ax.plot(pos[1], pos[0], 'wo', markersize=12, zorder=5,\n",
    "            markeredgecolor='black', markeredgewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"\\\"Si l'agent part de ⚪, quelles cases va-t-il visiter ?\\\"\")\n",
    "    print(f\"Jaune = souvent, violet = rarement, gris = mur\")\n",
    "    print(f\"γ = {gamma:.2f} → horizon ≈ {horizon:.0f} steps\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_sr_interactive,\n",
    "    gamma=widgets.FloatSlider(value=0.95, min=0.5, max=0.99, step=0.01,\n",
    "                              description='γ', continuous_update=False),\n",
    "    state=widgets.IntSlider(value=0, min=0, max=grid.n_states-1,\n",
    "                            description='s (départ)')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlxl474e8ro",
   "metadata": {},
   "source": [
    "### Reading the plot\n",
    "\n",
    "> \"If the agent starts from ⚪, which cells will it visit in the future?\"\n",
    "\n",
    "- **Yellow** = cell visited often (close to ⚪ or easy to access)\n",
    "- **Purple** = cell rarely reached (far away, or on the other side of the wall)\n",
    "- **Gray** = wall (inaccessible)\n",
    "\n",
    "**Try this:**\n",
    "- Move **s** → the yellow pattern follows the starting state\n",
    "- Increase **γ toward 0.99** → the agent \"sees\" far, yellow everywhere\n",
    "- Decrease **γ toward 0.5** → the agent only sees its immediate neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — TD(0) Learning of M\n",
    "\n",
    "### The problem\n",
    "\n",
    "In Section 1, we computed $M = (I - \\gamma T)^{-1}$. But this computation requires knowing $T$ — all the transition probabilities of the environment.\n",
    "\n",
    "In practice, **the agent does not know $T$**. It does not know in advance where walls, passages, etc. lead. It must discover the structure by **moving around and observing**.\n",
    "\n",
    "It is as if you arrived in an unknown city without a map: you cannot compute the best path in advance. You must explore and **build your mental map as you go**.\n",
    "\n",
    "### Why the agent needs a good $M$\n",
    "\n",
    "Recall from Section 1: $M$ allows evaluating any goal (food at cell 15 → read $M(s, 15)$). But this only works **if $M$ correctly reflects the environment structure**.\n",
    "\n",
    "A wrong $M$ would give bad evaluations: the agent might believe a cell is easy to reach when there is actually a wall between the two.\n",
    "\n",
    "We denote:\n",
    "- $M^*$ = the result of the formula $(I - \\gamma T)^{-1}$ (Section 1). This is the **target** — the perfect $M$ one would obtain if $T$ were known.\n",
    "- $M$ = what the agent has **learned so far**. Initially wrong, but with experience $M$ approaches $M^*$.\n",
    "\n",
    "The goal of learning is that $M \\to M^*$: the agent's mental map becomes as good as if it had had the complete map from the start.\n",
    "\n",
    "### How to learn: Temporal Difference (TD)\n",
    "\n",
    "At each step, the agent is at $s$ and transitions to $s'$. It can compare **what it predicts** with **what it observes**:\n",
    "\n",
    "- **Predicted**: $M(s, :)$ — its current map of frequencies from $s$\n",
    "- **Observed**: $e(s') + \\gamma \\cdot M(s', :)$ — \"I am at $s'$ now (= $e(s')$), and from $s'$ I predict $M(s', :)$ for the rest\"\n",
    "\n",
    "The difference is the **TD error**:\n",
    "\n",
    "$$\\delta_M(s) = \\underbrace{e(s') + \\gamma \\cdot M(s', :)}_{\\text{observed}} - \\underbrace{M(s, :)}_{\\text{predicted}}$$\n",
    "\n",
    "The agent corrects its prediction in the direction of the error:\n",
    "\n",
    "$$M(s, :) \\leftarrow M(s, :) + \\alpha_M \\cdot \\delta_M(s)$$\n",
    "\n",
    "### The learning rate $\\alpha_M$\n",
    "\n",
    "$\\alpha_M$ controls **how much the agent corrects its prediction** after each observation.\n",
    "\n",
    "**Analogy**: you think it takes 30 minutes to get to work. Today it took 40 minutes. How do you update your estimate?\n",
    "\n",
    "| $\\alpha_M$ | Correction | New estimate | Behavior |\n",
    "|-----------|-----------|-------------------|-------------|\n",
    "| 0.01 | 1% of the error | 30 + 0.01 × (40-30) = **30.1 min** | Very cautious, slow to adapt |\n",
    "| 0.1 | 10% of the error | 30 + 0.1 × (40-30) = **31 min** | Balanced |\n",
    "| 1.0 | 100% of the error | 30 + 1.0 × (40-30) = **40 min** | Blindly trusts the latest observation |\n",
    "\n",
    "An $\\alpha_M$ too large causes the agent to oscillate (it overreacts to each observation). An $\\alpha_M$ too small makes it very slow to learn.\n",
    "\n",
    "$M$ is initialized to $I$ (the identity matrix: each state predicts only itself, no knowledge of the structure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec2-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M* = le vrai M calculé en Section 1 (sert de référence)\n",
    "M_star_ref = grid.true_sr(0.95)\n",
    "\n",
    "def td_learning_demo(alpha_M, n_steps, state):\n",
    "    \"\"\"Simule n_steps de TD(0) et montre la convergence.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M = np.eye(grid.n_states)  # Init à I\n",
    "    errors = []\n",
    "\n",
    "    # Random walk\n",
    "    traj = grid.random_walk(n_steps, seed=42)\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        if t % 10 == 0:\n",
    "            err = np.linalg.norm(M - M_star_ref, 'fro') / grid.n_states\n",
    "            errors.append((t, err))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "    # Gauche : courbe de convergence\n",
    "    steps, errs = zip(*errors)\n",
    "    axes[0].plot(steps, errs, 'b-', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Steps (expérience)')\n",
    "    axes[0].set_ylabel('Erreur M vs M*')\n",
    "    axes[0].set_title(f'Convergence M → M*')\n",
    "    axes[0].set_ylim(bottom=0)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Droite : M appris pour l'état choisi (normalisé, comme Section 1)\n",
    "    row = M[state]\n",
    "    row_norm = row / max(row.max(), 1e-8)\n",
    "    grid.plot(values=row_norm, ax=axes[1],\n",
    "              title=f\"M appris depuis s={state}\", cmap='plasma', vmin=0, vmax=1)\n",
    "    pos = grid.idx_to_pos[state]\n",
    "    axes[1].plot(pos[1], pos[0], 'wo', markersize=12, zorder=5,\n",
    "                 markeredgecolor='black', markeredgewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Erreur finale : {errs[-1]:.4f}\")\n",
    "    print()\n",
    "    print(\"Gauche : écart entre le M appris et le vrai M (Section 1) — descend avec l'expérience\")\n",
    "    print(\"Droite : avec assez de steps, cette carte converge vers celle de la Section 1 (= M*)\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  α petit (0.01) → apprentissage lent mais stable\")\n",
    "    print(\"  α grand (0.5)  → apprentissage rapide mais instable (courbe qui oscille)\")\n",
    "\n",
    "widgets.interact(\n",
    "    td_learning_demo,\n",
    "    alpha_M=widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "                                description='α_M', continuous_update=False),\n",
    "    n_steps=widgets.IntSlider(value=500, min=50, max=3000, step=50,\n",
    "                               description='Steps', continuous_update=False),\n",
    "    state=widgets.IntSlider(value=0, min=0, max=grid.n_states-1,\n",
    "                            description='s (départ)')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Value Decomposition V = M · R\n",
    "\n",
    "### The question\n",
    "\n",
    "The agent knows the structure of the environment (via $M$). Now it wants to **make decisions**: \"which cell is the best for me right now?\"\n",
    "\n",
    "For this, it needs two pieces of information:\n",
    "1. **Where is the reward?** → this is the vector $R$\n",
    "2. **Can I reach it easily?** → this is already in $M$\n",
    "\n",
    "### The reward vector $R$\n",
    "\n",
    "$R$ is a simple vector: one value per cell.\n",
    "\n",
    "$$R(s) = \\text{reward obtained upon arriving at } s$$\n",
    "\n",
    "In our grid, there is food at a single cell (the \"goal\"):\n",
    "- $R(\\text{goal}) = 1$\n",
    "- $R(\\text{everywhere else}) = 0$\n",
    "\n",
    "### The value $V(s)$\n",
    "\n",
    "$V(s)$ answers the question: **\"how much reward will I accumulate starting from $s$?\"**\n",
    "\n",
    "- High $V$ = good position (close to the food, easy access)\n",
    "- Low $V$ = bad position (far away, or blocked by a wall)\n",
    "\n",
    "### The formula: V = M · R\n",
    "\n",
    "$$V(s) = \\sum_{s'} M(s, s') \\cdot R(s')$$\n",
    "\n",
    "In plain terms: the value of $s$ = sum over all cells of (visitation frequency × reward at that cell).\n",
    "\n",
    "This is a simple matrix-vector product: $V = M \\cdot R$.\n",
    "\n",
    "### Why this is powerful\n",
    "\n",
    "- **If the goal moves** (R changes): simply recompute $V = M \\cdot R$. **M stays the same** — the environment structure has not changed.\n",
    "- **If a wall appears** (structure changes): then $M$ must be relearned (→ Section 8).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec3-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_v_decomposition(goal_row, goal_col, gamma):\n",
    "    \"\"\"Montre V = M · R pour différentes positions du goal.\"\"\"\n",
    "    goal = (goal_row, goal_col)\n",
    "    if goal in grid.walls or goal_row >= grid.rows or goal_col >= grid.cols:\n",
    "        print(f\"Position ({goal_row}, {goal_col}) est un mur ou hors grille.\")\n",
    "        return\n",
    "\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    R = grid.reward_vector(goal)\n",
    "    V = M_star @ R\n",
    "\n",
    "    # Normaliser pour échelles fixes\n",
    "    row0 = M_star[0]\n",
    "    row0_norm = row0 / max(row0.max(), 1e-8)\n",
    "    V_norm = V / max(V.max(), 1e-8)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 3.5))\n",
    "\n",
    "    # Gauche : M[0, :] + goal\n",
    "    grid.plot(values=row0_norm, ax=axes[0], show_goal=False,\n",
    "              title='M depuis s=0 (ne change pas)', cmap='plasma', vmin=0, vmax=1)\n",
    "    axes[0].plot(goal[1], goal[0], 'r*', markersize=18, zorder=5)\n",
    "\n",
    "    # Droite : V = M · R + goal — même colormap que M\n",
    "    grid.plot(values=V_norm, ax=axes[1], show_goal=False,\n",
    "              title='V = M · R', cmap='plasma', vmin=0, vmax=1)\n",
    "    axes[1].plot(goal[1], goal[0], 'r*', markersize=18, zorder=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"★ rouge = position de la nourriture (déplacez-la avec les sliders)\")\n",
    "    print(\"Gauche : M ne bouge pas — la structure de l'environnement ne change pas\")\n",
    "    print(\"Droite : V change — la valeur des cases dépend de la position du goal\")\n",
    "    print()\n",
    "    print(\"→ C'est la force de la SR : un seul M sert pour n'importe quel objectif.\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_v_decomposition,\n",
    "    goal_row=widgets.IntSlider(value=0, min=0, max=grid.rows-1,\n",
    "                                description='Goal ligne'),\n",
    "    goal_col=widgets.IntSlider(value=4, min=0, max=grid.cols-1,\n",
    "                                description='Goal colonne'),\n",
    "    gamma=widgets.FloatSlider(value=0.95, min=0.5, max=0.99, step=0.01,\n",
    "                              description='γ', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Prediction Error $\\|\\delta_M\\|$\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In Section 2, we saw that the agent learns $M$ by correcting its errors ($\\delta_M$). We know that the overall error decreases with experience.\n",
    "\n",
    "But the agent needs more than that: it needs to know **where** its predictions are still poor. \"Do I know this area of the grid well, or am I still making frequent mistakes here?\"\n",
    "\n",
    "The TD error $\\delta_M(s)$ answers exactly this question:\n",
    "- **Large error** at $s$ → \"I was wrong arriving here, I know this area poorly\"\n",
    "- **Small error** at $s$ → \"no surprise, my prediction was good\"\n",
    "\n",
    "This is the basic signal for the agent's **metacognition** (Sections 5-9): its ability to know what it knows and what it does not know.\n",
    "\n",
    "### From vector to scalar\n",
    "\n",
    "$\\delta_M(s)$ is a **vector** of 21 values (one per cell). To get a single number summarizing \"did the agent make a mistake at $s$?\", we take the **norm** (the \"length\" of the vector):\n",
    "\n",
    "$$\\|\\delta_M(s)\\| = \\sqrt{\\sum_{s'} \\delta_M(s, s')^2}$$\n",
    "\n",
    "- $\\|\\delta_M\\| = 0$ → perfect prediction\n",
    "- $\\|\\delta_M\\|$ large → big surprise\n",
    "\n",
    "### Normalization by the 99th percentile\n",
    "\n",
    "Raw errors can range from 0.003 to 47 depending on $\\gamma$ and grid size. To make them comparable and bring them into $[0, 1]$, we need a **reference**: \"what counts as a large error?\"\n",
    "\n",
    "We use the **99th percentile** (p99) of all errors observed so far:\n",
    "\n",
    "$$\\delta_{norm} = \\min\\left(\\frac{\\|\\delta_M\\|}{\\text{p99}},\\; 1\\right)$$\n",
    "\n",
    "**Why p99?**\n",
    "\n",
    "Imagine the observed errors sorted from smallest to largest. The p99 is the value below which 99% of errors fall. By dividing by p99:\n",
    "- A \"normal\" error gives $\\delta_{norm}$ between 0 and 1\n",
    "- Only the top 1% most extreme errors are clipped to 1\n",
    "\n",
    "**Why not the maximum?** The max is often an isolated spike (for example, the very first error of the agent that knows nothing yet). Dividing by that spike would squash all other errors toward 0, making the map unreadable.\n",
    "\n",
    "**Why not the mean?** The mean would give normalized values > 1 for any error above the mean — half the cells would be clipped to 1.\n",
    "\n",
    "The p99 is a good compromise: it ignores extreme spikes while preserving contrast between cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec4-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_td_errors(n_steps, alpha_M):\n",
    "    \"\"\"Montre les erreurs TD par état au cours de l'apprentissage.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M = np.eye(grid.n_states)\n",
    "\n",
    "    # Accumuler les erreurs par état\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "\n",
    "    traj = grid.random_walk(n_steps, seed=42)\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "\n",
    "    # Erreur moyenne par état\n",
    "    mean_error = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_error[visited] = error_sum[visited] / error_count[visited]\n",
    "\n",
    "    # Normalisation p99\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    normalized = np.clip(mean_error / max(p99, 1e-8), 0, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "    # Erreur normalisée p99\n",
    "    grid.plot(values=normalized, ax=axes[0], show_goal=False,\n",
    "              title='Erreur normalisée par case', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "    # Nombre de visites\n",
    "    grid.plot(values=error_count, ax=axes[1], show_goal=False,\n",
    "              title='Nombre de visites', cmap='Blues')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Gauche : rouge = l'agent se trompe souvent ici (zone mal connue)\")\n",
    "    print(\"         jaune pâle = peu d'erreurs (zone bien apprise)\")\n",
    "    print(\"Droite : bleu foncé = case souvent visitée, clair = rarement\")\n",
    "    print()\n",
    "    print(f\"p99 = {p99:.4f}, états non visités : {(~visited).sum()}/{grid.n_states}\")\n",
    "    print()\n",
    "    print(\"→ Les cases peu visitées ont souvent plus d'erreurs (l'agent ne les connaît pas)\")\n",
    "    print(\"→ Augmentez les steps : les erreurs diminuent partout\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_td_errors,\n",
    "    n_steps=widgets.IntSlider(value=500, min=50, max=3000, step=50,\n",
    "                               description='Steps', continuous_update=False),\n",
    "    alpha_M=widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "                                description='α_M (cf. Sec. 2)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Uncertainty Map U(s)\n",
    "\n",
    "### The need\n",
    "\n",
    "In Section 4, we saw that the error $\\|\\delta_M(s)\\|$ measures \"did the agent make a mistake at this instant, at this cell\". But this error is **noisy**: a single observation can produce an error spike just by bad luck.\n",
    "\n",
    "The agent needs a more **stable** measure: \"do I know this area well, **in general**?\" — not just \"did I make a mistake this time\".\n",
    "\n",
    "This is the role of $U(s)$: transforming punctual errors into a **durable uncertainty map**, cell by cell.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine you are visiting a neighborhood in a city:\n",
    "- The **first time**, you often take the wrong turn → high uncertainty\n",
    "- After **a few visits**, you make fewer mistakes → uncertainty decreases\n",
    "- After **many visits**, you know the neighborhood well → low and stable uncertainty\n",
    "\n",
    "$U(s)$ reproduces exactly this behavior. For each cell $s$, the agent stores a **history of its recent errors** and averages them.\n",
    "\n",
    "### Three regimes\n",
    "\n",
    "$U(s)$ depends on how many times the agent has visited cell $s$:\n",
    "\n",
    "| Regime | Condition | What the agent does | Formula |\n",
    "|---------|-----------|-------------------|----------|\n",
    "| **Never visited** | $visits(s) = 0$ | \"I have never been here → maximum uncertainty\" | $U(s) = U_{prior}$ |\n",
    "| **Few visits** | $0 < visits(s) < K$ | \"I have a few observations but not enough for a reliable average\" | $U(s) = U_{prior} \\cdot decay^{visits(s)}$ |\n",
    "| **Enough visits** | $visits(s) \\geq K$ | \"I have enough data → I average my last $K$ errors\" | $U(s) = \\text{mean}(\\text{buffer}(s))$ |\n",
    "\n",
    "The parameters:\n",
    "- **$U_{prior}$** (default 0.8): initial uncertainty. \"Before seeing anything, I am 80% uncertain.\"\n",
    "- **$decay$** (default 0.85): speed of the initial descent. Each visit multiplies the uncertainty by $decay$.\n",
    "- **$K$** (default 20): buffer size. After $K$ visits, the agent switches to the average of its actual errors.\n",
    "\n",
    "### Why 3 regimes?\n",
    "\n",
    "- **Regime 1** (never visited): without data, we are cautious → high $U$.\n",
    "- **Regime 2** (cold-start): we do not yet have $K$ errors to compute a reliable average. The formula $U_{prior} \\cdot decay^{visits}$ decreases $U$ progressively, like a fading prior.\n",
    "- **Regime 3** (converged): the buffer is full → the average reflects actual errors. If the agent knows the area well, errors are small → low $U$. If the area has changed, errors rise → $U$ rises too.\n",
    "\n",
    "### How to choose the buffer size $K$?\n",
    "\n",
    "The buffer is a **sliding window**: the agent keeps its last $K$ errors for each cell and averages them. Older errors (beyond $K$) are forgotten.\n",
    "\n",
    "The choice of $K$ is a tradeoff between **stability** and **reactivity**:\n",
    "\n",
    "- **Small $K$** (e.g. 5): the average covers only 5 errors → it is **noisy** (a single spike pushes it up), but it **reacts quickly** if the area changes.\n",
    "- **Large $K$** (e.g. 50): the average is smoothed over 50 errors → it is **stable**, but it takes time to rise if the environment changes (the old small errors \"dilute\" the new large ones).\n",
    "\n",
    "**In practice:** we choose $K$ based on how often the environment can change.\n",
    "- **Stable** environment (walls never move) → large $K$ (30–50) for a well-smoothed $U$.\n",
    "- **Changing** environment (obstacles that appear/disappear) → small $K$ (10–15) to quickly detect that \"I no longer know this area\".\n",
    "\n",
    "The plot below shows $U(s)$ as a function of the number of visits for a given cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec5-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty_regimes(U_prior, decay, K):\n",
    "    \"\"\"Visualise les 3 régimes de U(s) en fonction du nombre de visites.\"\"\"\n",
    "    visits = np.arange(0, 60)\n",
    "\n",
    "    # Simuler des erreurs de buffer décroissantes (convergence réaliste)\n",
    "    U_values = []\n",
    "    buffer = []\n",
    "    for v in visits:\n",
    "        if v == 0:\n",
    "            U_values.append(U_prior)\n",
    "        elif v < K:\n",
    "            U_values.append(U_prior * (decay ** v))\n",
    "        else:\n",
    "            # Simuler buffer : erreurs décroissantes + bruit\n",
    "            simulated_error = max(0.01, 0.5 * np.exp(-0.05 * v) + 0.02 * np.sin(v))\n",
    "            buffer.append(simulated_error)\n",
    "            if len(buffer) > K:\n",
    "                buffer = buffer[-K:]\n",
    "            U_values.append(np.mean(buffer))\n",
    "\n",
    "    U_values = np.array(U_values)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(9, 4))\n",
    "\n",
    "    # Zones colorées pour les régimes\n",
    "    ax.axvspan(0, 0.5, alpha=0.15, color='red', label='Non visité')\n",
    "    ax.axvspan(0.5, K, alpha=0.15, color='orange', label=f'Cold-start (< K={K})')\n",
    "    ax.axvspan(K, 60, alpha=0.15, color='green', label=f'Convergé (≥ K={K})')\n",
    "\n",
    "    ax.plot(visits, U_values, 'b-', linewidth=2, label='U(s)')\n",
    "    ax.axhline(y=U_prior, color='red', linestyle='--', alpha=0.5, label=f'U_prior={U_prior}')\n",
    "    ax.axvline(x=K, color='gray', linestyle=':', alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('Nombre de visites')\n",
    "    ax.set_ylabel('U(s)')\n",
    "    ax.set_title('Incertitude U(s) : 3 régimes')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Lecture du graphe :\")\n",
    "    print(f\"  Axe horizontal = nombre de fois que l'agent a visité cette case\")\n",
    "    print(f\"  Axe vertical   = incertitude U(s) (1 = ne sait rien, 0 = connaît parfaitement)\")\n",
    "    print(f\"  Courbe bleue   = évolution de U au fil des visites\")\n",
    "    print()\n",
    "    print(\"Les 3 zones colorées :\")\n",
    "    print(f\"  Rouge  (0 visite)  : U = {U_prior} — l'agent n'est jamais venu ici, incertitude maximale\")\n",
    "    print(f\"  Orange (1 à {K-1} visites) : U descend vite — chaque visite réduit l'incertitude\")\n",
    "    print(f\"    Exemple à 5 visites : U = {U_prior} × {decay}⁵ = {U_prior * decay**5:.3f}\")\n",
    "    print(f\"  Vert   (≥ {K} visites)  : U = moyenne des {K} dernières erreurs réelles\")\n",
    "    print(f\"    → U se stabilise à un niveau bas si l'agent connaît bien la zone\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  U_prior haut (0.95) → l'agent démarre très incertain, la descente est plus longue\")\n",
    "    print(\"  decay bas (0.7)     → chaque visite réduit beaucoup l'incertitude (descente rapide)\")\n",
    "    print(\"  K grand (50)        → il faut plus de visites avant de basculer sur les erreurs réelles\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_uncertainty_regimes,\n",
    "    U_prior=widgets.FloatSlider(value=0.8, min=0.5, max=0.95, step=0.05,\n",
    "                                 description='U_prior', continuous_update=False),\n",
    "    decay=widgets.FloatSlider(value=0.85, min=0.7, max=0.95, step=0.05,\n",
    "                               description='decay', continuous_update=False),\n",
    "    K=widgets.IntSlider(value=20, min=5, max=50, step=5,\n",
    "                         description='K (buffer)')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Confidence Signal C(s)\n",
    "\n",
    "### Why not use U directly?\n",
    "\n",
    "In Section 5, we constructed $U(s) \\in [0, 1]$: an uncertainty map per cell. We could use it directly for decision-making (\"if $U > 0.5$, explore\"). But $U$ has two problems:\n",
    "\n",
    "1. **No sharp threshold.** $U = 0.35$ — is that uncertain or not? The boundary between \"I know\" and \"I don't know\" is fuzzy. Yet the agent needs to **decide**: \"do I trust my prediction here, yes or no?\"\n",
    "\n",
    "2. **Sensitivity to parameters.** $U$ depends on $U_{prior}$, $decay$, $K$... Its raw values vary widely depending on the configuration. We want a confidence signal whose scale is always the same: 0 = no confidence, 1 = full confidence.\n",
    "\n",
    "### The idea\n",
    "\n",
    "We transform $U$ into a **confidence** signal $C(s)$ with two properties:\n",
    "- **Inverted**: high uncertainty → low confidence (and vice versa)\n",
    "- **Sharp decision**: the transition between \"confident\" and \"not confident\" is rapid, not gradual\n",
    "\n",
    "For this, we use a **sigmoid** — an S-shaped function that squashes values toward 0 or 1:\n",
    "\n",
    "$$C(s) = \\frac{1}{1 + \\exp\\left(\\beta \\cdot (U(s) - \\theta_C)\\right)}$$\n",
    "\n",
    "### The parameters\n",
    "\n",
    "| Parameter | What it controls | Default |\n",
    "|-----------|------------------|--------|\n",
    "| $\\theta_C$ | The **center**: at what uncertainty level confidence equals exactly 0.5. \"Below $\\theta_C$, I am rather confident. Above, rather not.\" | 0.3 |\n",
    "| $\\beta$ | The **slope**: how abrupt the transition is. Large $\\beta$ = nearly instantaneous switch between 0 and 1. Small $\\beta$ = smooth and gradual transition. | 10 |\n",
    "\n",
    "**Concrete examples** (with $\\beta = 10$, $\\theta_C = 0.3$):\n",
    "- $U = 0.1$ (few errors) → $C \\approx 0.88$ — the agent is **confident**\n",
    "- $U = 0.3$ (moderate errors) → $C = 0.50$ — the agent **hesitates**\n",
    "- $U = 0.6$ (many errors) → $C \\approx 0.05$ — the agent **does not trust** its prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec6-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_sigmoid(beta, theta_C):\n",
    "    \"\"\"Visualise la sigmoïde C(U) et la heatmap C sur la grille.\"\"\"\n",
    "    U_range = np.linspace(0, 1, 200)\n",
    "    C_range = 1 / (1 + np.exp(beta * (U_range - theta_C)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Courbe sigmoïde\n",
    "    axes[0].plot(U_range, C_range, 'b-', linewidth=2)\n",
    "    axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].axvline(x=theta_C, color='red', linestyle='--', alpha=0.5,\n",
    "                    label=f'θ_C = {theta_C}')\n",
    "    axes[0].fill_between(U_range, C_range, alpha=0.1)\n",
    "    axes[0].set_xlabel('U(s) — Incertitude')\n",
    "    axes[0].set_ylabel('C(s) — Confiance')\n",
    "    axes[0].set_title(f'Sigmoïde : β={beta}, θ_C={theta_C}')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Simuler U après apprentissage partiel (1000 steps)\n",
    "    # Assez pour que les zones fréquentées soient bien apprises (vert)\n",
    "    # mais pas assez pour que tout soit appris (contraste vert/rouge)\n",
    "    gamma = 0.95\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    M_partial = np.eye(grid.n_states)\n",
    "    n_sim = 1000\n",
    "    traj = grid.random_walk(n_sim, seed=42)\n",
    "\n",
    "    # Accumuler erreurs par état (comme Section 4)\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t+1]\n",
    "        delta, M_partial = grid.td_update(M_partial, s, s_next, gamma, 0.1)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "\n",
    "    # U = erreur moyenne normalisée par état\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "    U_sim = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    C_sim = 1 / (1 + np.exp(beta * (U_sim - theta_C)))\n",
    "\n",
    "    grid.plot(values=C_sim, ax=axes[1], show_goal=False,\n",
    "              title='C(s) sur la grille (après 1000 steps)', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    n_green = (C_sim > 0.5).sum()\n",
    "    n_red = (C_sim <= 0.5).sum()\n",
    "    C_at_0 = 1/(1+np.exp(beta*(0-theta_C)))\n",
    "    C_at_1 = 1/(1+np.exp(beta*(1-theta_C)))\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — la sigmoïde C(U) :\")\n",
    "    print(f\"  Axe horizontal = incertitude U (0 = sûr, 1 = ne sait rien)\")\n",
    "    print(f\"  Axe vertical   = confiance C (0 = pas confiant, 1 = confiant)\")\n",
    "    print(f\"  Ligne rouge pointillée = θ_C = {theta_C} → à ce U, la confiance vaut exactement 0.5\")\n",
    "    print(f\"  À gauche de θ_C : C monte vers {C_at_0:.2f} (confiant)\")\n",
    "    print(f\"  À droite de θ_C : C descend vers {C_at_1:.3f} (pas confiant)\")\n",
    "    print()\n",
    "    print(\"Droite — la confiance sur la grille :\")\n",
    "    print(f\"  Vert  = confiance élevée (zone bien apprise) — {n_green} cases\")\n",
    "    print(f\"  Rouge = confiance faible (zone mal connue) — {n_red} cases\")\n",
    "    print(f\"  Gris  = mur\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  β petit (2)  → transition douce, beaucoup de cases en jaune (intermédiaire)\")\n",
    "    print(\"  β grand (25) → transition brutale, les cases sont soit vertes soit rouges\")\n",
    "    print(\"  θ_C bas (0.1) → l'agent exige très peu d'erreurs pour être confiant → plus de rouge\")\n",
    "    print(\"  θ_C haut (0.6) → l'agent est indulgent → plus de vert\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_confidence_sigmoid,\n",
    "    beta=widgets.FloatSlider(value=10, min=2, max=25, step=1,\n",
    "                              description='β (pente)', continuous_update=False),\n",
    "    theta_C=widgets.FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05,\n",
    "                                 description='θ_C (centre)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec7-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 — Adaptive Exploration\n",
    "\n",
    "### The problem\n",
    "\n",
    "In Section 3, we saw that the agent chooses its actions based on $V(s) = M \\cdot R$: it moves toward cells with high value (close to the goal). This is **exploitation** — doing what is known to be good.\n",
    "\n",
    "But there is a trap: if the agent only exploits, it stays in the areas it already knows and **never discovers the rest of the grid**. It could miss a better path, or never learn what lies on the other side of the wall.\n",
    "\n",
    "The agent must therefore also **explore** — visit unknown areas to improve its $M$. This is the classic exploration vs. exploitation dilemma.\n",
    "\n",
    "### The naive solution: epsilon-greedy\n",
    "\n",
    "The standard approach is to choose a random action with probability $\\varepsilon$, and the best action otherwise. But with a fixed $\\varepsilon$ (e.g. 0.1), the agent explores **equally** in areas it knows well and in unknown areas. This is wasteful.\n",
    "\n",
    "### Explore where it matters\n",
    "\n",
    "The idea of exploring more in uncertain areas is a classic principle in RL (UCB, intrinsic curiosity, etc.). What PRISM brings is using the map $U(s)$ built from the SR's TD errors (Section 5) as the uncertainty signal. The agent knows **where** it is uncertain, and uses it in two complementary ways:\n",
    "\n",
    "### 1. Adaptive epsilon\n",
    "\n",
    "$$\\varepsilon(s) = \\varepsilon_{min} + (\\varepsilon_{max} - \\varepsilon_{min}) \\cdot U(s)$$\n",
    "\n",
    "Instead of a fixed $\\varepsilon$, each cell has its own exploration rate:\n",
    "- **Well-known area** (low $U$) → $\\varepsilon \\approx \\varepsilon_{min}$ → the agent exploits (it knows what to do)\n",
    "- **Unknown area** (high $U$) → $\\varepsilon \\approx \\varepsilon_{max}$ → the agent explores (it needs to learn)\n",
    "\n",
    "### 2. Exploration bonus\n",
    "\n",
    "$$V_{explore}(s) = V(s) + \\lambda \\cdot U(s)$$\n",
    "\n",
    "Uncertainty acts as a **bonus reward**: unknown areas become artificially attractive. The agent is \"curious\" — it is drawn to what it does not yet know.\n",
    "\n",
    "- $\\lambda = 0$ → no bonus, the agent only exploits\n",
    "- Large $\\lambda$ → the agent prioritizes exploration, even if the goal is elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec7-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exploration(eps_min, eps_max, lam):\n",
    "    \"\"\"Visualise epsilon adaptatif et V_explore.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    R = grid.reward_vector()\n",
    "    V = M_star @ R\n",
    "\n",
    "    # Simuler U après apprentissage partiel (1000 steps, comme Section 6)\n",
    "    M_partial = np.eye(grid.n_states)\n",
    "    traj = grid.random_walk(1000, seed=42)\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t+1]\n",
    "        delta, M_partial = grid.td_update(M_partial, s, s_next, gamma, 0.1)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "    U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    # Epsilon adaptatif\n",
    "    eps = eps_min + (eps_max - eps_min) * U\n",
    "\n",
    "    # V_explore\n",
    "    V_norm = V / max(V.max(), 1e-8)\n",
    "    V_explore = V_norm + lam * U\n",
    "    V_explore_norm = V_explore / max(V_explore.max(), 1e-8)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13, 3.5))\n",
    "\n",
    "    grid.plot(values=eps, ax=axes[0], show_goal=False,\n",
    "              title=f'ε(s) — taux d\\'exploration',\n",
    "              cmap='YlOrRd', vmin=eps_min, vmax=eps_max)\n",
    "\n",
    "    grid.plot(values=V_norm, ax=axes[1], show_goal=False,\n",
    "              title='V(s) — sans bonus', cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "    grid.plot(values=V_explore_norm, ax=axes[2], show_goal=False,\n",
    "              title=f'V + λ·U — avec bonus (λ={lam})', cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_V = grid.idx_to_pos[np.argmax(V)]\n",
    "    best_Vx = grid.idx_to_pos[np.argmax(V_explore)]\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — ε(s), le taux d'exploration par case :\")\n",
    "    print(f\"  Rouge = exploration forte (zone inconnue, U élevé → ε ≈ {eps_max})\")\n",
    "    print(f\"  Jaune pâle = exploration faible (zone connue, U bas → ε ≈ {eps_min})\")\n",
    "    print(f\"  → L'agent fait des actions aléatoires plus souvent dans les zones rouges\")\n",
    "    print()\n",
    "    print(\"Centre — V(s), la valeur sans bonus :\")\n",
    "    print(f\"  Jaune = case de haute valeur (proche du goal)\")\n",
    "    print(f\"  Violet = case de faible valeur (loin du goal)\")\n",
    "    print(f\"  → L'agent est attiré uniquement vers le goal\")\n",
    "    print()\n",
    "    print(\"Droite — V + λ·U, la valeur avec bonus d'exploration :\")\n",
    "    print(f\"  Les zones inconnues (U élevé) reçoivent un bonus → elles deviennent plus jaunes\")\n",
    "    print(f\"  → L'agent est attiré à la fois vers le goal ET vers les zones inconnues\")\n",
    "    print()\n",
    "    print(f\"Case la plus attractive : sans bonus = {best_V}, avec bonus (λ={lam}) = {best_Vx}\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  λ = 0   → droite identique au centre (pas de curiosité)\")\n",
    "    print(\"  λ grand (2.0) → les zones inconnues dominent, l'agent ignore presque le goal\")\n",
    "    print(\"  ε_max haut (0.9) → exploration très agressive dans les zones rouges\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_exploration,\n",
    "    eps_min=widgets.FloatSlider(value=0.01, min=0.001, max=0.1, step=0.01,\n",
    "                                 description='ε_min', continuous_update=False),\n",
    "    eps_max=widgets.FloatSlider(value=0.5, min=0.2, max=0.9, step=0.1,\n",
    "                                 description='ε_max', continuous_update=False),\n",
    "    lam=widgets.FloatSlider(value=0.5, min=0.0, max=2.0, step=0.1,\n",
    "                             description='λ (bonus)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec8-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 — Change Detection\n",
    "\n",
    "### The problem\n",
    "\n",
    "Until now, we assumed the environment does not change: walls stay in place, passages remain open. But what happens if a **wall appears** and blocks the passage between the two rooms?\n",
    "\n",
    "The $M$ the agent has learned reflects the **old** structure. It predicts, for example, \"from the left room, I will often pass through the corridor to reach the right room\". But the passage is blocked — this prediction is now **wrong**.\n",
    "\n",
    "The agent needs to **detect** that something has changed, so it knows it must relearn its $M$.\n",
    "\n",
    "### How to detect it?\n",
    "\n",
    "The mechanism is already in place thanks to the previous sections:\n",
    "\n",
    "1. **The environment changes** → the learned $M$ no longer matches reality\n",
    "2. **TD errors increase** (Section 4) → the agent makes mistakes when passing through the modified area\n",
    "3. **$U(s)$ rises** (Section 5) → the error buffer fills with new, high errors\n",
    "4. **$C(s)$ drops** (Section 6) → confidence falls in the affected area\n",
    "\n",
    "All that remains is to summarize this information into a single signal: \"has the environment just changed?\"\n",
    "\n",
    "### The change score\n",
    "\n",
    "We look at the average uncertainty of **recently visited** cells:\n",
    "\n",
    "$$\\text{score} = \\frac{1}{|S_{recent}|} \\sum_{s \\in S_{recent}} U(s)$$\n",
    "\n",
    "- $S_{recent}$ = the cells visited in the last ~50 steps\n",
    "- In normal times, these cells are well known → low $U$ → low score\n",
    "- After a change, errors rise → $U$ increases → **the score spikes**\n",
    "\n",
    "We compare this score to a threshold:\n",
    "\n",
    "$$\\text{change\\_detected} = \\mathbb{1}(\\text{score} > \\theta_{change})$$\n",
    "\n",
    "### Why \"recent\" states?\n",
    "\n",
    "We do not look at the entire grid, only the **recently visited** cells, because:\n",
    "- The change only affects the modified area — cells far from the change keep a low $U$\n",
    "- The agent can only notice the change where it passes — if it has not visited the modified area, it cannot know yet\n",
    "- By averaging over recent cells, the signal is **localized**: it reflects what the agent is experiencing right now\n",
    "\n",
    "The plot below simulates a scenario: the agent learns for 300 steps, then the passage is blocked. We observe the change score over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec8-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_change_detection(theta_change):\n",
    "    \"\"\"Simule apprentissage, perturbation, et détection de changement.\"\"\"\n",
    "    gamma, alpha = 0.95, 0.1\n",
    "    K = 10  # buffer size pour U\n",
    "\n",
    "    grid_normal = ToyGrid.two_rooms()\n",
    "    M = np.eye(grid_normal.n_states)\n",
    "    n_states = grid_normal.n_states\n",
    "\n",
    "    from collections import deque\n",
    "    buffers = [deque(maxlen=K) for _ in range(n_states)]\n",
    "    visit_counts = np.zeros(n_states)\n",
    "    recent_states = deque(maxlen=50)\n",
    "    all_deltas = []\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    total_steps = 600\n",
    "    change_at = 300\n",
    "\n",
    "    traj = grid_normal.random_walk(total_steps, seed=42)\n",
    "\n",
    "    for t in range(total_steps):\n",
    "        s = traj[t]\n",
    "        s_next = traj[t + 1] if t + 1 < len(traj) else s\n",
    "\n",
    "        delta, M = grid_normal.td_update(M, s, s_next, gamma, alpha)\n",
    "        norm = np.linalg.norm(delta)\n",
    "\n",
    "        # Après step 300 : on simule le blocage du passage.\n",
    "        # En vrai, les transitions changeraient. Ici on simule l'effet :\n",
    "        # les erreurs TD deviennent plus grandes car M reflète l'ancienne structure.\n",
    "        if t >= change_at:\n",
    "            norm *= 3.0\n",
    "\n",
    "        all_deltas.append(norm)\n",
    "        p99 = np.percentile(all_deltas[-500:], 99) if len(all_deltas) > 10 else 1.0\n",
    "        normalized = min(norm / max(p99, 1e-8), 1.0)\n",
    "\n",
    "        visit_counts[s] += 1\n",
    "        buffers[s].append(normalized)\n",
    "        recent_states.append(s)\n",
    "\n",
    "        # Calculer le score toutes les 5 steps\n",
    "        if t % 5 == 0 and len(recent_states) > 5:\n",
    "            unique_recent = set(recent_states)\n",
    "            U_values = []\n",
    "            for rs in unique_recent:\n",
    "                if visit_counts[rs] == 0:\n",
    "                    U_values.append(0.8)\n",
    "                elif visit_counts[rs] < K:\n",
    "                    U_values.append(0.8 * (0.85 ** visit_counts[rs]))\n",
    "                else:\n",
    "                    U_values.append(np.mean(buffers[rs]))\n",
    "            score = np.mean(U_values)\n",
    "            scores.append((t, score))\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "    steps, change_scores = zip(*scores)\n",
    "    ax.plot(steps, change_scores, 'b-', linewidth=1.5, label='Score de changement')\n",
    "    ax.axhline(y=theta_change, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Seuil θ = {theta_change}')\n",
    "    ax.axvline(x=change_at, color='orange', linestyle='-', linewidth=2,\n",
    "               alpha=0.7, label='Passage bloqué (step 300)')\n",
    "\n",
    "    # Colorier la zone de détection (score > seuil)\n",
    "    for i in range(len(steps)-1):\n",
    "        if change_scores[i] > theta_change:\n",
    "            ax.axvspan(steps[i], steps[i+1], alpha=0.15, color='red')\n",
    "\n",
    "    ax.set_xlabel('Steps (temps)')\n",
    "    ax.set_ylabel('Score de changement')\n",
    "    ax.set_title('Scénario : l\\'agent apprend, puis le passage est bloqué')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    # Annotations sur le graphe\n",
    "    ax.annotate('Apprentissage normal\\n(erreurs diminuent)',\n",
    "                xy=(150, 0.15), fontsize=9, ha='center', color='steelblue')\n",
    "    ax.annotate('Passage bloqué !\\n(erreurs remontent)',\n",
    "                xy=(450, 0.85), fontsize=9, ha='center', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Latence de détection\n",
    "    post_change = [(s, sc) for s, sc in scores if s >= change_at and sc > theta_change]\n",
    "\n",
    "    print(\"Lecture du graphe :\")\n",
    "    print()\n",
    "    print(\"C'est un film : le temps avance de gauche à droite (0 → 600 steps).\")\n",
    "    print()\n",
    "    print(\"  Axe horizontal = le temps (nombre de steps d'expérience)\")\n",
    "    print(\"  Axe vertical   = le score de changement (= moyenne de U sur les cases récentes)\")\n",
    "    print(\"  Courbe bleue   = le score au fil du temps\")\n",
    "    print(f\"  Ligne rouge horizontale = seuil θ_change = {theta_change}\")\n",
    "    print(\"  Ligne orange verticale  = step 300, le moment où le passage est bloqué\")\n",
    "    print(\"  Zone rouge transparente = le score dépasse le seuil → changement détecté\")\n",
    "    print()\n",
    "    print(\"Le scénario en 2 phases :\")\n",
    "    print(\"  Steps 0–300   : l'agent explore la grille normalement.\")\n",
    "    print(\"                  Il apprend M, ses erreurs diminuent, le score baisse.\")\n",
    "    print(\"  Step 300       : le passage entre les deux pièces est bloqué par un mur.\")\n",
    "    print(\"                  Le M appris est maintenant faux → les erreurs remontent → le score monte.\")\n",
    "    print()\n",
    "    if post_change:\n",
    "        latency = post_change[0][0] - change_at\n",
    "        print(f\"  → Détection après {latency} steps (le score franchit le seuil rouge)\")\n",
    "    else:\n",
    "        print(f\"  → Pas de détection : le score ne franchit jamais le seuil (trop haut ?)\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  θ bas (0.2)  → détection très rapide, mais risque de fausses alarmes avant step 300\")\n",
    "    print(\"  θ haut (0.7) → détection lente ou absente, mais pas de fausses alarmes\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_change_detection,\n",
    "    theta_change=widgets.FloatSlider(value=0.5, min=0.1, max=0.8, step=0.05,\n",
    "                                      description='θ_change', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec9-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 — The \"I Don't Know\" Signal (IDK)\n",
    "\n",
    "### The need\n",
    "\n",
    "In Section 6, we constructed $C(s)$: a confidence value between 0 and 1 for each cell. But the agent must make a **binary decision**: \"do I trust my prediction here, or not?\"\n",
    "\n",
    "It is like a doctor looking at an X-ray: they may be more or less sure of their diagnosis, but at some point they must decide — \"I know what this is\" or \"I don't know, I will ask for a second opinion\".\n",
    "\n",
    "### The rule\n",
    "\n",
    "$$\\text{IDK}(s) = \\mathbb{1}\\left(C(s) < \\theta_{idk}\\right)$$\n",
    "\n",
    "- If $C(s) \\geq \\theta_{idk}$ → the agent is confident enough → it **exploits** (does what $V$ tells it)\n",
    "- If $C(s) < \\theta_{idk}$ → the agent is not confident enough → it signals **\"I don't know\"** and explores instead\n",
    "\n",
    "$\\theta_{idk}$ (default 0.3) is the **minimum confidence threshold**. It is a design choice:\n",
    "- **Low $\\theta_{idk}$** (0.1) → the agent rarely says \"I don't know\" — it trusts itself even with little certainty\n",
    "- **High $\\theta_{idk}$** (0.6) → the agent is very cautious — it says \"I don't know\" as soon as confidence is not high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec9-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_idk_signal(theta_idk, beta, theta_C):\n",
    "    \"\"\"Visualise les zones où l'agent dit 'je ne sais pas'.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M_star = grid.true_sr(gamma)\n",
    "\n",
    "    # Simuler U après apprentissage partiel (1000 steps, comme Sections 6-7)\n",
    "    M_partial = np.eye(grid.n_states)\n",
    "    traj = grid.random_walk(1000, seed=42)\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t+1]\n",
    "        delta, M_partial = grid.td_update(M_partial, s, s_next, gamma, 0.1)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "    U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    # Confiance\n",
    "    C = 1 / (1 + np.exp(beta * (U - theta_C)))\n",
    "\n",
    "    # IDK signal\n",
    "    idk = (C < theta_idk).astype(float)\n",
    "    pct_idk = 100 * idk.mean()\n",
    "    n_idk = int(idk.sum())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Gauche : C(s) sur la grille\n",
    "    grid.plot(values=C, ax=axes[0], show_goal=False,\n",
    "              title='C(s) — Confiance', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    # Droite : IDK binaire (vert = confiant, rouge = IDK)\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    cmap_idk = ListedColormap(['#2ecc71', '#e74c3c'])\n",
    "    idk_grid = grid.to_grid(idk)\n",
    "    axes[1].imshow(idk_grid, cmap=cmap_idk, vmin=0, vmax=1,\n",
    "                   origin='upper', interpolation='nearest')\n",
    "    axes[1].set_title(f'IDK (θ = {theta_idk}) — {n_idk}/{grid.n_states} cases en IDK')\n",
    "    for w in grid.walls:\n",
    "        axes[1].add_patch(plt.Rectangle((w[1]-0.5, w[0]-0.5), 1, 1,\n",
    "                          fill=True, color='gray', alpha=0.8))\n",
    "    axes[1].set_xticks(np.arange(-0.5, grid.cols, 1), minor=True)\n",
    "    axes[1].set_yticks(np.arange(-0.5, grid.rows, 1), minor=True)\n",
    "    axes[1].grid(which='minor', color='black', linewidth=0.5, alpha=0.3)\n",
    "    axes[1].tick_params(which='both', bottom=False, left=False,\n",
    "                        labelbottom=False, labelleft=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — C(s), la confiance par case (même carte que Section 6) :\")\n",
    "    print(\"  Vert  = confiance élevée (zone bien apprise)\")\n",
    "    print(\"  Rouge = confiance faible (zone mal connue)\")\n",
    "    print()\n",
    "    print(\"Droite — la décision binaire IDK :\")\n",
    "    print(f\"  Vert  = C ≥ {theta_idk} → l'agent fait confiance → il exploite\")\n",
    "    print(f\"  Rouge = C < {theta_idk} → l'agent dit \\\"je ne sais pas\\\" → il explore\")\n",
    "    print(f\"  Gris  = mur\")\n",
    "    print()\n",
    "    print(f\"  → {n_idk} cases en IDK sur {grid.n_states} ({pct_idk:.0f}%)\")\n",
    "    print()\n",
    "    print(\"Le lien entre les deux graphes :\")\n",
    "    print(f\"  Le seuil θ_idk = {theta_idk} coupe la carte de confiance en deux.\")\n",
    "    print(\"  Toutes les cases rouges/oranges à gauche (C faible) deviennent rouges à droite (IDK).\")\n",
    "    print(\"  Toutes les cases vertes à gauche (C élevée) restent vertes à droite.\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  θ_idk bas (0.1)  → presque tout est vert (l'agent fait confiance facilement)\")\n",
    "    print(\"  θ_idk haut (0.6) → beaucoup de rouge (l'agent est très prudent)\")\n",
    "    print(\"  β et θ_C changent la carte de confiance à gauche → ça change aussi l'IDK à droite\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_idk_signal,\n",
    "    theta_idk=widgets.FloatSlider(value=0.3, min=0.1, max=0.7, step=0.05,\n",
    "                                   description='θ_idk', continuous_update=False),\n",
    "    beta=widgets.FloatSlider(value=10, min=2, max=25, step=1,\n",
    "                              description='β (cf. Sec. 6)', continuous_update=False),\n",
    "    theta_C=widgets.FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05,\n",
    "                                 description='θ_C (cf. Sec. 6)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec10-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10 — Interactive Summary\n",
    "\n",
    "This dashboard combines the 4 maps from the previous sections on a single simulation. Everything is connected: the agent learns $M$ via TD (Sec. 2), computes $V = M \\cdot R$ (Sec. 3), measures its errors to estimate $U$ (Sec. 4-5), and derives its confidence $C$ (Sec. 6).\n",
    "\n",
    "| Map | Section | What it shows |\n",
    "|-------|---------|------------------|\n",
    "| **M[s, :]** | Sec. 1-2 | Predicted visitation frequency from state $s$ |\n",
    "| **V(s)** | Sec. 3 | Value of each cell ($M \\cdot R$) |\n",
    "| **U(s)** | Sec. 4-5 | Uncertainty (average errors per cell) |\n",
    "| **C(s)** | Sec. 6 | Confidence (inverse sigmoid of U) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec10-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_full_dashboard(gamma, alpha_M, n_steps, state, beta, theta_C):\n",
    "    \"\"\"Tableau de bord complet : M, V, U, C.\"\"\"\n",
    "    # Apprentissage TD\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    M = np.eye(grid.n_states)\n",
    "\n",
    "    traj = grid.random_walk(n_steps, seed=42)\n",
    "    error_sums = np.zeros(grid.n_states)\n",
    "    visit_counts = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sums[s] += norm\n",
    "        visit_counts[s] += 1\n",
    "        all_norms.append(norm)\n",
    "\n",
    "    # V = M · R\n",
    "    R = grid.reward_vector()\n",
    "    V = M @ R\n",
    "\n",
    "    # U = erreur moyenne normalisée (comme Sections 4-7)\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = visit_counts > 0\n",
    "    mean_err[visited] = error_sums[visited] / visit_counts[visited]\n",
    "    U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    # C = sigmoïde (comme Section 6)\n",
    "    C = 1 / (1 + np.exp(beta * (U - theta_C)))\n",
    "\n",
    "    # Normaliser M et V pour échelles fixes (comme Sections 1-3)\n",
    "    row = M[state]\n",
    "    row_norm = row / max(row.max(), 1e-8)\n",
    "    V_norm = V / max(V.max(), 1e-8)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 3.5))\n",
    "\n",
    "    # M[s, :] — plasma, normalisé (comme Section 1)\n",
    "    grid.plot(values=row_norm, ax=axes[0], show_goal=False,\n",
    "              title=f'M depuis s={state}', cmap='plasma', vmin=0, vmax=1)\n",
    "    pos = grid.idx_to_pos[state]\n",
    "    axes[0].plot(pos[1], pos[0], 'wo', markersize=10, zorder=5,\n",
    "                 markeredgecolor='black', markeredgewidth=1.5)\n",
    "\n",
    "    # V(s) — plasma (comme Section 3)\n",
    "    grid.plot(values=V_norm, ax=axes[1], show_goal=False,\n",
    "              title='V = M · R', cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "    # U(s) — YlOrRd (comme Section 4)\n",
    "    grid.plot(values=U, ax=axes[2], show_goal=False,\n",
    "              title='U(s)', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "    # C(s) — RdYlGn (comme Section 6)\n",
    "    grid.plot(values=C, ax=axes[3], show_goal=False,\n",
    "              title='C(s)', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    err_m = np.linalg.norm(M - M_star, 'fro') / grid.n_states\n",
    "    n_confident = (C > 0.5).sum()\n",
    "\n",
    "    print(\"Lecture des 4 cartes :\")\n",
    "    print()\n",
    "    print(f\"  M depuis s={state} : fréquence de visite prédite (jaune = souvent, violet = rarement)\")\n",
    "    print(f\"    ⚪ = état de départ choisi\")\n",
    "    print(f\"  V = M · R : valeur des cases (jaune = proche du goal, violet = loin)\")\n",
    "    print(f\"  U(s) : incertitude (rouge = mal connu, jaune pâle = bien appris)\")\n",
    "    print(f\"  C(s) : confiance (vert = confiant, rouge = pas confiant)\")\n",
    "    print()\n",
    "    print(f\"Résumé : erreur M = {err_m:.4f}, U moyen = {U.mean():.3f}, \"\n",
    "          f\"{n_confident}/{grid.n_states} cases confiantes\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  Steps bas (100)   → M mal appris, V faux, U élevé partout, C rouge partout\")\n",
    "    print(\"  Steps haut (3000) → M converge, V correct, U bas, C vert partout\")\n",
    "    print(\"  α_M grand (0.5)   → apprentissage instable, U reste élevé\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_full_dashboard,\n",
    "    gamma=widgets.FloatSlider(value=0.95, min=0.5, max=0.99, step=0.01,\n",
    "                              description='γ', continuous_update=False),\n",
    "    alpha_M=widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "                                description='α_M (cf. Sec. 2)', continuous_update=False),\n",
    "    n_steps=widgets.IntSlider(value=500, min=50, max=3000, step=50,\n",
    "                               description='Steps', continuous_update=False),\n",
    "    state=widgets.IntSlider(value=0, min=0, max=grid.n_states-1,\n",
    "                            description='s (départ)'),\n",
    "    beta=widgets.FloatSlider(value=10, min=2, max=25, step=1,\n",
    "                              description='β (cf. Sec. 6)', continuous_update=False),\n",
    "    theta_C=widgets.FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05,\n",
    "                                 description='θ_C (cf. Sec. 6)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rakaua7ilts",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11 — Hyperparameter Sweep\n",
    "\n",
    "### The problem\n",
    "\n",
    "In the previous sections, we introduced **4 hyperparameters** that control PRISM's metacognitive behavior:\n",
    "\n",
    "| Parameter | Introduced in | What it controls |\n",
    "|-----------|-------------|-------------------|\n",
    "| `U_prior` | Section 5 | Initial uncertainty (before any visit) |\n",
    "| `decay` | Section 5 | Speed of U's descent in the cold-start regime |\n",
    "| `beta` | Section 7 | Weight of the exploration bonus in $V_{explore} = V + \\beta \\cdot U$ |\n",
    "| `theta_C` | Section 6 | Center of the confidence sigmoid |\n",
    "\n",
    "Each section presented its parameter in isolation, with a slider. But in reality, **these parameters interact**:\n",
    "\n",
    "- A high `beta` (strong exploration) can compensate for a slow `decay` (uncertainty that decreases slowly) — the agent explores a lot everywhere, even in areas it knows poorly\n",
    "- A low `U_prior` (agent not very uncertain at the start) can mask a too-permissive `theta_C` — if U never gets high, the confidence threshold is never reached\n",
    "- A fast `decay` (0.7) with a low `beta` (5) gives an agent that stops exploring very quickly — potentially before having learned well\n",
    "\n",
    "**Testing one parameter at a time is not enough.** You may find the \"best\" `beta` while keeping the others fixed, but this \"best\" depends on the fixed values. Changing another parameter could make this `beta` suboptimal.\n",
    "\n",
    "### The solution: the sweep (systematic search)\n",
    "\n",
    "A **sweep** (or grid search) explores **all combinations** of hyperparameters on a predefined grid.\n",
    "\n",
    "**Principle**:\n",
    "\n",
    "1. **Define a grid**: choose 3 values per parameter\n",
    "   - `U_prior` ∈ {0.5, 0.8, 1.0}\n",
    "   - `decay` ∈ {0.7, 0.85, 0.95}\n",
    "   - `beta` ∈ {5, 10, 20}\n",
    "   - `theta_C` ∈ {0.2, 0.3, 0.5}\n",
    "\n",
    "2. **Enumerate**: 3 × 3 × 3 × 3 = **81 configurations**\n",
    "\n",
    "3. **Evaluate** each configuration with **multiple runs** (different seeds) to measure variability\n",
    "\n",
    "4. **Compare** configurations on a common metric (e.g. ECE in Phase 3)\n",
    "\n",
    "### Why multiple runs per configuration?\n",
    "\n",
    "A single run can be lucky or unlucky (the agent stumbles on the goal quickly, or gets stuck in a corner). By running **10 runs** with different seeds, we obtain a distribution: median, standard deviation, confidence interval.\n",
    "\n",
    "It is like testing a medication: you do not give it to a single person and conclude that it works. You run a trial on 10 (or 100, or 1000) patients, and look at the trend.\n",
    "\n",
    "### What to look for in the results\n",
    "\n",
    "The sweep produces a table of 81 rows (one per configuration), with for each row:\n",
    "- **Median ECE**: calibration error (lower = better)\n",
    "- **Median MI**: metacognitive index (higher = better)\n",
    "- **Standard deviation**: stability of the configuration\n",
    "\n",
    "We look for:\n",
    "1. **The best configuration**: the one with the lowest median ECE\n",
    "2. **The rank of the defaults**: is the default configuration (U_prior=0.8, decay=0.85, beta=10, theta_C=0.3) in the top 10, or at the bottom?\n",
    "3. **Sensitivity**: which parameter has the most impact? If changing `beta` makes the ECE vary from 0.05 to 0.40 while changing `theta_C` only varies it from 0.10 to 0.12, then `beta` is **critical** and `theta_C` is **robust**.\n",
    "\n",
    "### Analogy: buying a bicycle\n",
    "\n",
    "You want to buy a bicycle. There are 4 features to choose:\n",
    "- Frame size (S, M, L)\n",
    "- Tire type (road, hybrid, off-road)\n",
    "- Number of gears (7, 14, 21)\n",
    "- Brake type (rim, disc, hydraulic)\n",
    "\n",
    "3 × 3 × 3 × 3 = 81 possible combinations. You cannot test an M frame with road tires and conclude that M is the best size — maybe with off-road tires, an L frame would be better.\n",
    "\n",
    "The sweep amounts to trying **all 81 combinations**, scoring each on 10 different rides, then looking at which one gives the best average score.\n",
    "\n",
    "### Visualizing the interactions\n",
    "\n",
    "The widget below simulates a mini-sweep on the ToyGrid. We fix 2 parameters and vary the other 2 to see how the ECE changes. Red cells indicate bad configurations, green ones indicate good configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9xdlenhck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import si la cellule est exécutée isolément\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def simulate_calibration(grid, gamma, alpha_M, n_steps, seed,\n",
    "                         U_prior, decay, K, beta, theta_C):\n",
    "    \"\"\"Simulate learning + compute a simplified ECE-like metric.\"\"\"\n",
    "    M = np.eye(grid.n_states)\n",
    "    traj = grid.random_walk(n_steps, seed=seed)\n",
    "\n",
    "    buffers = [deque(maxlen=K) for _ in range(grid.n_states)]\n",
    "    visit_counts = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        all_norms.append(norm)\n",
    "        visit_counts[s] += 1\n",
    "\n",
    "        p99 = np.percentile(all_norms[-200:], 99) if len(all_norms) > 10 else 1.0\n",
    "        normalized = min(norm / max(p99, 1e-8), 1.0)\n",
    "        buffers[s].append(normalized)\n",
    "\n",
    "    # Compute U(s) for each state\n",
    "    U = np.zeros(grid.n_states)\n",
    "    for s in range(grid.n_states):\n",
    "        if visit_counts[s] == 0:\n",
    "            U[s] = U_prior\n",
    "        elif visit_counts[s] < K:\n",
    "            U[s] = U_prior * (decay ** visit_counts[s])\n",
    "        else:\n",
    "            U[s] = np.mean(buffers[s])\n",
    "\n",
    "    # Confidence C(s)\n",
    "    C = 1.0 / (1.0 + np.exp(beta * (U - theta_C)))\n",
    "\n",
    "    # True error: ||M[s,:] - M*[s,:]||\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    true_err = np.array([np.linalg.norm(M[s] - M_star[s]) for s in range(grid.n_states)])\n",
    "    true_err_norm = true_err / max(true_err.max(), 1e-8)\n",
    "\n",
    "    # Simplified ECE: |C(s) - (1 - true_error(s))| averaged\n",
    "    accuracy = 1.0 - true_err_norm\n",
    "    ece = np.mean(np.abs(C - accuracy))\n",
    "    return ece\n",
    "\n",
    "\n",
    "def plot_sweep_heatmap(fixed_U_prior, fixed_theta_C):\n",
    "    \"\"\"Sweep beta x decay, fixing U_prior and theta_C.\"\"\"\n",
    "    gamma, alpha_M, K = 0.95, 0.1, 15\n",
    "    n_steps = 800\n",
    "    n_seeds = 3\n",
    "\n",
    "    beta_vals = [5, 10, 20]\n",
    "    decay_vals = [0.7, 0.85, 0.95]\n",
    "\n",
    "    results = np.zeros((len(decay_vals), len(beta_vals)))\n",
    "\n",
    "    for i, d in enumerate(decay_vals):\n",
    "        for j, b in enumerate(beta_vals):\n",
    "            eces = []\n",
    "            for seed in range(n_seeds):\n",
    "                ece = simulate_calibration(\n",
    "                    grid, gamma, alpha_M, n_steps, seed=seed,\n",
    "                    U_prior=fixed_U_prior, decay=d, K=K,\n",
    "                    beta=b, theta_C=fixed_theta_C\n",
    "                )\n",
    "                eces.append(ece)\n",
    "            results[i, j] = np.median(eces)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Heatmap\n",
    "    im = axes[0].imshow(results, cmap='RdYlGn_r', vmin=0, vmax=0.5,\n",
    "                        origin='upper', aspect='auto')\n",
    "    axes[0].set_xticks(range(len(beta_vals)))\n",
    "    axes[0].set_xticklabels([str(b) for b in beta_vals])\n",
    "    axes[0].set_yticks(range(len(decay_vals)))\n",
    "    axes[0].set_yticklabels([str(d) for d in decay_vals])\n",
    "    axes[0].set_xlabel('beta')\n",
    "    axes[0].set_ylabel('decay')\n",
    "    axes[0].set_title(f'ECE (U_prior={fixed_U_prior}, theta_C={fixed_theta_C})')\n",
    "\n",
    "    for i in range(len(decay_vals)):\n",
    "        for j in range(len(beta_vals)):\n",
    "            axes[0].text(j, i, f'{results[i,j]:.3f}', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold',\n",
    "                        color='white' if results[i,j] > 0.25 else 'black')\n",
    "\n",
    "    plt.colorbar(im, ax=axes[0], label='ECE (bas = mieux)')\n",
    "\n",
    "    # Bar chart: sensitivity per parameter\n",
    "    param_ranges = {}\n",
    "    center = {\"U_prior\": 0.8, \"decay\": 0.85, \"beta\": 10, \"theta_C\": 0.3}\n",
    "    sweep_vals = {\n",
    "        \"U_prior\": [0.5, 0.8, 1.0],\n",
    "        \"decay\": [0.7, 0.85, 0.95],\n",
    "        \"beta\": [5, 10, 20],\n",
    "        \"theta_C\": [0.2, 0.3, 0.5],\n",
    "    }\n",
    "\n",
    "    for param, vals in sweep_vals.items():\n",
    "        eces_for_param = []\n",
    "        for v in vals:\n",
    "            kwargs = dict(center)\n",
    "            kwargs[param] = v\n",
    "            ece = simulate_calibration(\n",
    "                grid, gamma, alpha_M, n_steps, seed=0,\n",
    "                U_prior=kwargs[\"U_prior\"], decay=kwargs[\"decay\"], K=K,\n",
    "                beta=kwargs[\"beta\"], theta_C=kwargs[\"theta_C\"]\n",
    "            )\n",
    "            eces_for_param.append(ece)\n",
    "        param_ranges[param] = max(eces_for_param) - min(eces_for_param)\n",
    "\n",
    "    params = list(param_ranges.keys())\n",
    "    ranges = [param_ranges[p] for p in params]\n",
    "    colors = ['#e74c3c' if r > 0.05 else '#f39c12' if r > 0.02 else '#2ecc71'\n",
    "              for r in ranges]\n",
    "    axes[1].barh(params, ranges, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    axes[1].set_xlabel('Plage ECE (max - min)')\n",
    "    axes[1].set_title('Sensibilite par parametre')\n",
    "    axes[1].set_xlim(0, max(ranges) * 1.3 if ranges else 0.1)\n",
    "\n",
    "    for k, (p, r) in enumerate(zip(params, ranges)):\n",
    "        axes[1].text(r + 0.002, k, f'{r:.3f}', va='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_i, best_j = np.unravel_index(results.argmin(), results.shape)\n",
    "    worst_i, worst_j = np.unravel_index(results.argmax(), results.shape)\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche - la heatmap beta x decay :\")\n",
    "    print(\"  Chaque cellule = ECE mediane pour une combinaison (beta, decay)\")\n",
    "    print(f\"  Vert = bonne calibration (ECE bas), rouge = mauvaise (ECE haut)\")\n",
    "    print(f\"  Meilleure : beta={beta_vals[best_j]}, decay={decay_vals[best_i]} \"\n",
    "          f\"(ECE={results[best_i, best_j]:.3f})\")\n",
    "    print(f\"  Pire :      beta={beta_vals[worst_j]}, decay={decay_vals[worst_i]} \"\n",
    "          f\"(ECE={results[worst_i, worst_j]:.3f})\")\n",
    "    print()\n",
    "    print(\"Droite - la sensibilite :\")\n",
    "    print(\"  Barre longue = le parametre a un fort impact sur l'ECE\")\n",
    "    print(\"  Barre courte = le parametre est robuste (peu d'impact)\")\n",
    "    most_sensitive = max(param_ranges, key=param_ranges.get)\n",
    "    print(f\"  Parametre le plus critique : {most_sensitive} \"\n",
    "          f\"(plage = {param_ranges[most_sensitive]:.3f})\")\n",
    "    print()\n",
    "    print(\"A essayer :\")\n",
    "    print(\"  Changez U_prior et theta_C avec les sliders :\")\n",
    "    print(\"  la heatmap change -> les interactions entre beta et decay\")\n",
    "    print(\"  dependent des valeurs fixees pour les 2 autres parametres.\")\n",
    "    print(\"  C'est exactement pour ca qu'on fait un sweep complet (81 configs)\")\n",
    "    print(\"  plutot que d'optimiser chaque parametre isolement.\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_sweep_heatmap,\n",
    "    fixed_U_prior=widgets.FloatSlider(value=0.8, min=0.5, max=1.0, step=0.1,\n",
    "                                       description='U_prior (fixe)',\n",
    "                                       continuous_update=False),\n",
    "    fixed_theta_C=widgets.FloatSlider(value=0.3, min=0.2, max=0.5, step=0.1,\n",
    "                                       description='theta_C (fixe)',\n",
    "                                       continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Formula Summary\n",
    "\n",
    "| Concept | Formula | Key Parameters |\n",
    "|---------|---------|----------------|\n",
    "| SR (definition) | $M(s,s') = \\mathbb{E}[\\sum_t \\gamma^t \\mathbb{1}(s_t=s')]$ | $\\gamma$ |\n",
    "| SR (analytical) | $M^* = (I - \\gamma T)^{-1}$ | $\\gamma$, $T$ |\n",
    "| TD error | $\\delta_M = e(s') + \\gamma M(s',:) - M(s,:)$ | $\\gamma$ |\n",
    "| SR update | $M(s,:) \\leftarrow M(s,:) + \\alpha_M \\delta_M$ | $\\alpha_M$ |\n",
    "| Value | $V(s) = M(s,:) \\cdot R$ | |\n",
    "| Reward | $R(s) \\leftarrow R(s) + \\alpha_R (r - R(s))$ | $\\alpha_R$ |\n",
    "| Uncertainty | $U(s) = \\text{mean}(\\text{buffer}(s))$ | $K$, $U_{prior}$, $decay$ |\n",
    "| Confidence | $C(s) = \\frac{1}{1+\\exp(\\beta(U-\\theta_C))}$ | $\\beta$, $\\theta_C$ |\n",
    "| Adaptive epsilon | $\\varepsilon(s) = \\varepsilon_{min} + (\\varepsilon_{max}-\\varepsilon_{min}) U(s)$ | $\\varepsilon_{min}$, $\\varepsilon_{max}$ |\n",
    "| Exploration bonus | $V_{explore} = V + \\lambda U$ | $\\lambda$ |\n",
    "| Change detection | $\\text{score} = \\text{mean}(U(s_{recent}))$ | $\\theta_{change}$ |\n",
    "| IDK | $\\mathbb{1}(C(s) < \\theta_{idk})$ | $\\theta_{idk}$ |\n",
    "| Sweep | Grid search over $(U_{prior}, decay, \\beta, \\theta_C)$: 81 configs × $n$ runs | Grid of values |\n",
    "\n",
    "**Appendices**: [Spectral](00a_spectral_deep_dive.ipynb) | [Calibration](00b_calibration_methods.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
