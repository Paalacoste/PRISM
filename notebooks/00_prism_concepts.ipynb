{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# PRISM — Concepts mathématiques et algorithmiques\n",
    "\n",
    "Ce notebook explique les fondements mathématiques du projet PRISM de manière interactive.\n",
    "Chaque section introduit un concept avec sa formule, puis propose un widget pour l'explorer visuellement.\n",
    "\n",
    "**Dépendances :** numpy, matplotlib, ipywidgets (pas besoin de MiniGrid).\n",
    "\n",
    "**Annexes :**\n",
    "- `00a_spectral_deep_dive.ipynb` — Eigendecomposition de M\n",
    "- `00b_calibration_methods.ipynb` — Métriques de calibration (ECE, MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec0-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sec0-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import sys, os\n",
    "\n",
    "# Ensure prism package is importable\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from prism.pedagogy.toy_grid import ToyGrid\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sec0-grid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grille 5×5, 21 états accessibles, 4 murs\n",
      "Goal : (0, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grille de référence : deux pièces connectées par un passage\n",
    "grid = ToyGrid.two_rooms()\n",
    "print(f\"Grille {grid.rows}×{grid.cols}, {grid.n_states} états accessibles, 4 murs\")\n",
    "print(f\"Goal : {grid.goal}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "grid.plot(ax=axes[0], title=\"Structure de la grille\")\n",
    "\n",
    "# Numéroter les états sur fond neutre\n",
    "grid.plot(ax=axes[1], title=\"Index des états\")\n",
    "for i, pos in grid.idx_to_pos.items():\n",
    "    axes[1].text(pos[1], pos[0], str(i), ha='center', va='center',\n",
    "                 fontsize=7, fontweight='bold', color='steelblue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — La Successor Representation (SR)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imaginez un agent qui se promène aléatoirement dans la grille. À chaque pas, il peut aller dans 4 directions (haut, bas, gauche, droite).\n",
    "\n",
    "La **Successor Representation** ($M$) est un grand tableau qui répond à cette question pour **chaque paire de cases** :\n",
    "\n",
    "> \"Si je pars de la case $s$, **combien de fois** vais-je passer par la case $s'$ dans le futur ?\"\n",
    "\n",
    "Concrètement, $M$ est une **matrice** (un tableau à double entrée) :\n",
    "- **Ligne** = case de départ $s$\n",
    "- **Colonne** = case de destination $s'$\n",
    "- **Valeur** $M(s, s')$ = fréquence de visite attendue de $s'$ en partant de $s$\n",
    "\n",
    "Notre grille a 21 cases accessibles, donc $M$ est un tableau 21×21 = 441 valeurs. Chaque ligne est une \"carte de chaleur\" qui dit : \"depuis cette case de départ, voici les cases que je visiterai le plus\".\n",
    "\n",
    "Par exemple, la ligne de $M$ pour le coin haut-gauche montrera :\n",
    "- Les cases **proches** avec des valeurs élevées (visitées souvent)\n",
    "- Les cases **loin** ou derrière un mur avec des valeurs faibles (rarement atteintes)\n",
    "\n",
    "### Pourquoi c'est utile\n",
    "\n",
    "$M$ permet à l'agent d'**évaluer n'importe quel objectif instantanément**.\n",
    "\n",
    "Exemple : on place de la nourriture en case 15. L'agent veut savoir \"est-ce que la case $s$ est un bon point de départ pour atteindre la nourriture ?\". Il lui suffit de lire $M(s, 15)$ : si la valeur est élevée, il passera souvent par la case 15, donc c'est un bon départ.\n",
    "\n",
    "Et si la nourriture **change de place** (disons case 3) ? L'agent n'a pas besoin de tout réapprendre — il lit $M(s, 3)$ à la place. Le tableau $M$ reste le même, seul l'objectif change.\n",
    "\n",
    "C'est la force de la SR : elle sépare **\"où je peux aller\"** ($M$, la structure) de **\"où je veux aller\"** (la récompense). On formalisera cette idée en Section 3 avec $V = M \\cdot R$.\n",
    "\n",
    "### Formule\n",
    "\n",
    "$$M(s, s') = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t \\, \\mathbb{1}(s_t = s') \\;\\middle|\\; s_0 = s\\right]$$\n",
    "\n",
    "Décomposons terme par terme :\n",
    "- $s$ : la case de départ\n",
    "- $s'$ : une case quelconque dont on veut connaître la fréquence de visite\n",
    "- $\\mathbb{1}(s_t = s')$ : vaut 1 si l'agent est en $s'$ au temps $t$, 0 sinon\n",
    "- $\\gamma^t$ : un poids qui **diminue avec le temps** ($\\gamma < 1$), donc les visites lointaines comptent moins\n",
    "- $\\sum_{t=0}^{\\infty}$ : on additionne sur tout le futur\n",
    "- $\\mathbb{E}[...]$ : on moyenne sur toutes les trajectoires possibles\n",
    "\n",
    "Le paramètre $\\gamma$ contrôle **jusqu'où l'agent regarde** :\n",
    "- $\\gamma = 0.5$ → l'agent ne considère que ses voisins immédiats (horizon ≈ 2 steps)\n",
    "- $\\gamma = 0.95$ → l'agent anticipe ~20 steps dans le futur\n",
    "- $\\gamma = 0.99$ → l'agent voit très loin (horizon ≈ 100 steps)\n",
    "\n",
    "### La matrice de transition $T$\n",
    "\n",
    "Pour calculer $M$, on a besoin de savoir **comment l'agent se déplace**. C'est ce que décrit la matrice de transition $T$ :\n",
    "\n",
    "$$T_{ij} = P(s' = j \\mid s = i)$$\n",
    "\n",
    "En français : $T_{ij}$ est la **probabilité d'aller de la case $i$ à la case $j$** en un pas.\n",
    "\n",
    "Dans notre grille, la politique est uniforme (4 directions équiprobables), donc :\n",
    "- Si $j$ est un voisin accessible de $i$ : $T_{ij} = 0.25$\n",
    "- Si $j$ est un mur ou n'est pas adjacent : $T_{ij} = 0$\n",
    "- Si l'agent essaie d'aller dans un mur, il reste sur place : ça augmente $T_{ii}$\n",
    "\n",
    "$T$ résume toute la **structure de l'environnement** + la **politique de l'agent**.\n",
    "\n",
    "### Calcul de $M$\n",
    "\n",
    "On peut décomposer $M$ en additionnant les contributions de chaque pas de temps :\n",
    "\n",
    "$$M = \\underbrace{I}_{t=0} + \\underbrace{\\gamma \\, T}_{t=1} + \\underbrace{\\gamma^2 T^2}_{t=2} + \\underbrace{\\gamma^3 T^3}_{t=3} + \\ldots$$\n",
    "\n",
    "Chaque terme a un sens concret :\n",
    "\n",
    "| Terme | Signification |\n",
    "|-------|--------------|\n",
    "| $I$ | À $t=0$, l'agent est à sa case de départ (contribution = 1) |\n",
    "| $\\gamma \\, T$ | À $t=1$, il a fait un pas → $T$ donne les probas d'arriver à chaque case |\n",
    "| $\\gamma^2 T^2$ | À $t=2$, il a fait deux pas → $T^2$ donne les probas en 2 étapes |\n",
    "| $\\gamma^t T^t$ | À $t$ quelconque, pondéré par $\\gamma^t$ (les pas lointains comptent moins) |\n",
    "\n",
    "Cette somme infinie converge (car $\\gamma < 1$) et vaut :\n",
    "\n",
    "$$M = (I - \\gamma T)^{-1}$$\n",
    "\n",
    "Le graphe ci-dessous montre **une ligne** de ce tableau : pour un état de départ choisi, la fréquence de visite de chaque case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "sec1-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ebbbda3f42488fa00e70fcf0a6bab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.95, continuous_update=False, description='γ', max=0.99, min=0.5, ste…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_sr_interactive(gamma, state):\n",
    "    \"\"\"Affiche M*[s, :] pour un état de départ donné.\"\"\"\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    horizon = 1 / (1 - gamma)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "    row = M_star[state]\n",
    "    row_norm = row / row.max()\n",
    "    grid.plot(values=row_norm, ax=ax,\n",
    "              title=f\"Occupancy future depuis s={state} (γ={gamma:.2f})\",\n",
    "              cmap='plasma', vmin=0, vmax=1)\n",
    "    pos = grid.idx_to_pos[state]\n",
    "    ax.plot(pos[1], pos[0], 'wo', markersize=12, zorder=5,\n",
    "            markeredgecolor='black', markeredgewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"\\\"Si l'agent part de ⚪, quelles cases va-t-il visiter ?\\\"\")\n",
    "    print(f\"Jaune = souvent, violet = rarement, gris = mur\")\n",
    "    print(f\"γ = {gamma:.2f} → horizon ≈ {horizon:.0f} steps\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_sr_interactive,\n",
    "    gamma=widgets.FloatSlider(value=0.95, min=0.5, max=0.99, step=0.01,\n",
    "                              description='γ', continuous_update=False),\n",
    "    state=widgets.IntSlider(value=0, min=0, max=grid.n_states-1,\n",
    "                            description='s (départ)')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlxl474e8ro",
   "metadata": {},
   "source": [
    "### Lecture du graphe\n",
    "\n",
    "> \"Si l'agent part de ⚪, quelles cases va-t-il visiter dans le futur ?\"\n",
    "\n",
    "- **Jaune** = case visitée souvent (proche de ⚪ ou facile d'accès)\n",
    "- **Violet** = case rarement atteinte (loin, ou de l'autre côté du mur)\n",
    "- **Gris** = mur (inaccessible)\n",
    "\n",
    "**À essayer :**\n",
    "- Déplacez **s** → le pattern jaune suit l'état de départ\n",
    "- Montez **γ vers 0.99** → l'agent \"voit\" loin, jaune partout\n",
    "- Baissez **γ vers 0.5** → l'agent ne voit que ses voisins immédiats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Apprentissage TD(0) de M\n",
    "\n",
    "### Le problème\n",
    "\n",
    "En Section 1, on a calculé $M = (I - \\gamma T)^{-1}$. Mais ce calcul nécessite de connaître $T$ — toutes les probabilités de transition de l'environnement.\n",
    "\n",
    "En pratique, **l'agent ne connaît pas $T$**. Il ne sait pas à l'avance où mènent les murs, les passages, etc. Il doit découvrir la structure en **se déplaçant et en observant**.\n",
    "\n",
    "C'est comme si vous arriviez dans une ville inconnue sans carte : vous ne pouvez pas calculer le meilleur chemin à l'avance. Vous devez explorer, et **construire votre carte mentale au fur et à mesure**.\n",
    "\n",
    "### Pourquoi l'agent a besoin d'un bon $M$\n",
    "\n",
    "Rappel Section 1 : $M$ permet d'évaluer n'importe quel objectif (la nourriture en case 15 → lire $M(s, 15)$). Mais ça ne marche que **si $M$ reflète correctement la structure de l'environnement**.\n",
    "\n",
    "Un $M$ faux donnerait de mauvaises évaluations : l'agent pourrait croire qu'une case est facile à atteindre alors qu'il y a un mur entre les deux.\n",
    "\n",
    "On note :\n",
    "- $M^*$ = le résultat de la formule $(I - \\gamma T)^{-1}$ (Section 1). C'est la **cible** — le $M$ parfait qu'on obtiendrait si on connaissait $T$.\n",
    "- $M$ = ce que l'agent a **appris jusqu'ici**. Au début c'est faux, mais avec l'expérience $M$ se rapproche de $M^*$.\n",
    "\n",
    "Le but de l'apprentissage est que $M \\to M^*$ : que la carte mentale de l'agent devienne aussi bonne que s'il avait eu la carte complète depuis le début.\n",
    "\n",
    "### Comment apprendre : Temporal Difference (TD)\n",
    "\n",
    "À chaque pas, l'agent est en $s$ et arrive en $s'$. Il peut comparer **ce qu'il prédit** avec **ce qu'il observe** :\n",
    "\n",
    "- **Prédit** : $M(s, :)$ — sa carte actuelle des fréquences depuis $s$\n",
    "- **Observé** : $e(s') + \\gamma \\cdot M(s', :)$ — \"je suis en $s'$ maintenant (= $e(s')$), et depuis $s'$ je prédis $M(s', :)$ pour la suite\"\n",
    "\n",
    "La différence est l'**erreur TD** :\n",
    "\n",
    "$$\\delta_M(s) = \\underbrace{e(s') + \\gamma \\cdot M(s', :)}_{\\text{observé}} - \\underbrace{M(s, :)}_{\\text{prédit}}$$\n",
    "\n",
    "L'agent corrige sa prédiction dans la direction de l'erreur :\n",
    "\n",
    "$$M(s, :) \\leftarrow M(s, :) + \\alpha_M \\cdot \\delta_M(s)$$\n",
    "\n",
    "### Le taux d'apprentissage $\\alpha_M$\n",
    "\n",
    "$\\alpha_M$ contrôle **de combien l'agent corrige sa prédiction** après chaque observation.\n",
    "\n",
    "**Analogie** : vous pensez qu'il faut 30 minutes pour aller au travail. Aujourd'hui il a fallu 40 minutes. Comment mettez-vous à jour votre estimation ?\n",
    "\n",
    "| $\\alpha_M$ | Correction | Nouvelle estimation | Comportement |\n",
    "|-----------|-----------|-------------------|-------------|\n",
    "| 0.01 | 1% de l'erreur | 30 + 0.01 × (40-30) = **30.1 min** | Très prudent, lent à s'adapter |\n",
    "| 0.1 | 10% de l'erreur | 30 + 0.1 × (40-30) = **31 min** | Équilibré |\n",
    "| 1.0 | 100% de l'erreur | 30 + 1.0 × (40-30) = **40 min** | Fait confiance aveuglément à la dernière observation |\n",
    "\n",
    "Un $\\alpha_M$ trop grand fait osciller l'agent (il surréagit à chaque observation). Un $\\alpha_M$ trop petit le rend très lent à apprendre.\n",
    "\n",
    "$M$ est initialisé à $I$ (la matrice identité : chaque état prédit uniquement lui-même, aucune connaissance de la structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sec2-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c7f03148d14cc68a1433dd6fcec8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, continuous_update=False, description='α_M', max=0.5, min=0.01, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# M* = le vrai M calculé en Section 1 (sert de référence)\n",
    "M_star_ref = grid.true_sr(0.95)\n",
    "\n",
    "def td_learning_demo(alpha_M, n_steps, state):\n",
    "    \"\"\"Simule n_steps de TD(0) et montre la convergence.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M = np.eye(grid.n_states)  # Init à I\n",
    "    errors = []\n",
    "\n",
    "    # Random walk\n",
    "    traj = grid.random_walk(n_steps, seed=42)\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        if t % 10 == 0:\n",
    "            err = np.linalg.norm(M - M_star_ref, 'fro') / grid.n_states\n",
    "            errors.append((t, err))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "    # Gauche : courbe de convergence\n",
    "    steps, errs = zip(*errors)\n",
    "    axes[0].plot(steps, errs, 'b-', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Steps (expérience)')\n",
    "    axes[0].set_ylabel('Erreur M vs M*')\n",
    "    axes[0].set_title(f'Convergence M → M*')\n",
    "    axes[0].set_ylim(bottom=0)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Droite : M appris pour l'état choisi (normalisé, comme Section 1)\n",
    "    row = M[state]\n",
    "    row_norm = row / max(row.max(), 1e-8)\n",
    "    grid.plot(values=row_norm, ax=axes[1],\n",
    "              title=f\"M appris depuis s={state}\", cmap='plasma', vmin=0, vmax=1)\n",
    "    pos = grid.idx_to_pos[state]\n",
    "    axes[1].plot(pos[1], pos[0], 'wo', markersize=12, zorder=5,\n",
    "                 markeredgecolor='black', markeredgewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Erreur finale : {errs[-1]:.4f}\")\n",
    "    print()\n",
    "    print(\"Gauche : écart entre le M appris et le vrai M (Section 1) — descend avec l'expérience\")\n",
    "    print(\"Droite : avec assez de steps, cette carte converge vers celle de la Section 1 (= M*)\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  α petit (0.01) → apprentissage lent mais stable\")\n",
    "    print(\"  α grand (0.5)  → apprentissage rapide mais instable (courbe qui oscille)\")\n",
    "\n",
    "widgets.interact(\n",
    "    td_learning_demo,\n",
    "    alpha_M=widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "                                description='α_M', continuous_update=False),\n",
    "    n_steps=widgets.IntSlider(value=500, min=50, max=3000, step=50,\n",
    "                               description='Steps', continuous_update=False),\n",
    "    state=widgets.IntSlider(value=0, min=0, max=grid.n_states-1,\n",
    "                            description='s (départ)')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Décomposition V = M · R\n",
    "\n",
    "### La question\n",
    "\n",
    "L'agent connaît la structure de l'environnement (via $M$). Maintenant il veut **prendre des décisions** : \"quelle case est la meilleure pour moi en ce moment ?\"\n",
    "\n",
    "Pour ça, il a besoin de deux informations :\n",
    "1. **Où est la récompense ?** → c'est le vecteur $R$\n",
    "2. **Est-ce que je peux y accéder facilement ?** → c'est déjà dans $M$\n",
    "\n",
    "### Le vecteur de récompense $R$\n",
    "\n",
    "$R$ est un vecteur simple : une valeur par case.\n",
    "\n",
    "$$R(s) = \\text{récompense obtenue en arrivant en } s$$\n",
    "\n",
    "Dans notre grille, il y a de la nourriture à une seule case (le \"goal\") :\n",
    "- $R(\\text{goal}) = 1$\n",
    "- $R(\\text{partout ailleurs}) = 0$\n",
    "\n",
    "### La valeur $V(s)$\n",
    "\n",
    "$V(s)$ répond à la question : **\"combien de récompense vais-je accumuler en partant de $s$ ?\"**\n",
    "\n",
    "- $V$ élevé = bonne position (proche de la nourriture, accès facile)\n",
    "- $V$ faible = mauvaise position (loin, ou bloqué par un mur)\n",
    "\n",
    "### La formule : V = M · R\n",
    "\n",
    "$$V(s) = \\sum_{s'} M(s, s') \\cdot R(s')$$\n",
    "\n",
    "En français : la valeur de $s$ = somme sur toutes les cases de (fréquence de visite × récompense de cette case).\n",
    "\n",
    "C'est un simple produit matrice-vecteur : $V = M \\cdot R$.\n",
    "\n",
    "### Pourquoi c'est puissant\n",
    "\n",
    "- **Si le goal bouge** (R change) : il suffit de recalculer $V = M \\cdot R$. **M reste le même** — la structure de l'environnement n'a pas changé.\n",
    "- **Si un mur apparaît** (structure change) : là, $M$ doit être réappris (→ Section 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sec3-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7408d3d38fe420ba50363666838729c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Goal ligne', max=4), IntSlider(value=4, description='Goa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_v_decomposition(goal_row, goal_col, gamma):\n",
    "    \"\"\"Montre V = M · R pour différentes positions du goal.\"\"\"\n",
    "    goal = (goal_row, goal_col)\n",
    "    if goal in grid.walls or goal_row >= grid.rows or goal_col >= grid.cols:\n",
    "        print(f\"Position ({goal_row}, {goal_col}) est un mur ou hors grille.\")\n",
    "        return\n",
    "\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    R = grid.reward_vector(goal)\n",
    "    V = M_star @ R\n",
    "\n",
    "    # Normaliser pour échelles fixes\n",
    "    row0 = M_star[0]\n",
    "    row0_norm = row0 / max(row0.max(), 1e-8)\n",
    "    V_norm = V / max(V.max(), 1e-8)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 3.5))\n",
    "\n",
    "    # Gauche : M[0, :] + goal\n",
    "    grid.plot(values=row0_norm, ax=axes[0], show_goal=False,\n",
    "              title='M depuis s=0 (ne change pas)', cmap='plasma', vmin=0, vmax=1)\n",
    "    axes[0].plot(goal[1], goal[0], 'r*', markersize=18, zorder=5)\n",
    "\n",
    "    # Droite : V = M · R + goal — même colormap que M\n",
    "    grid.plot(values=V_norm, ax=axes[1], show_goal=False,\n",
    "              title='V = M · R', cmap='plasma', vmin=0, vmax=1)\n",
    "    axes[1].plot(goal[1], goal[0], 'r*', markersize=18, zorder=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"★ rouge = position de la nourriture (déplacez-la avec les sliders)\")\n",
    "    print(\"Gauche : M ne bouge pas — la structure de l'environnement ne change pas\")\n",
    "    print(\"Droite : V change — la valeur des cases dépend de la position du goal\")\n",
    "    print()\n",
    "    print(\"→ C'est la force de la SR : un seul M sert pour n'importe quel objectif.\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_v_decomposition,\n",
    "    goal_row=widgets.IntSlider(value=0, min=0, max=grid.rows-1,\n",
    "                                description='Goal ligne'),\n",
    "    goal_col=widgets.IntSlider(value=4, min=0, max=grid.cols-1,\n",
    "                                description='Goal colonne'),\n",
    "    gamma=widgets.FloatSlider(value=0.95, min=0.5, max=0.99, step=0.01,\n",
    "                              description='γ', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Erreur de prédiction $\\|\\delta_M\\|$\n",
    "\n",
    "### Motivation\n",
    "\n",
    "En Section 2, on a vu que l'agent apprend $M$ en corrigeant ses erreurs ($\\delta_M$). On sait que l'erreur globale diminue avec l'expérience.\n",
    "\n",
    "Mais l'agent a besoin de plus que ça : il a besoin de savoir **où** ses prédictions sont encore mauvaises. \"Est-ce que je connais bien cette zone de la grille, ou est-ce que je me trompe encore souvent ici ?\"\n",
    "\n",
    "L'erreur TD $\\delta_M(s)$ répond exactement à cette question :\n",
    "- **Erreur grande** en $s$ → \"je me suis trompé en arrivant ici, je connais mal cette zone\"\n",
    "- **Erreur petite** en $s$ → \"pas de surprise, ma prédiction était bonne\"\n",
    "\n",
    "C'est le signal de base pour la **métacognition** de l'agent (Sections 5-9) : sa capacité à savoir ce qu'il sait et ce qu'il ne sait pas.\n",
    "\n",
    "### Du vecteur au nombre\n",
    "\n",
    "$\\delta_M(s)$ est un **vecteur** de 21 valeurs (une par case). Pour obtenir un seul nombre qui résume \"l'agent s'est-il trompé en $s$ ?\", on prend la **norme** (la \"longueur\" du vecteur) :\n",
    "\n",
    "$$\\|\\delta_M(s)\\| = \\sqrt{\\sum_{s'} \\delta_M(s, s')^2}$$\n",
    "\n",
    "- $\\|\\delta_M\\| = 0$ → prédiction parfaite\n",
    "- $\\|\\delta_M\\|$ grand → grosse surprise\n",
    "\n",
    "### Normalisation par le 99e percentile\n",
    "\n",
    "Les erreurs brutes peuvent valoir 0.003 ou 47 selon $\\gamma$ et la taille de la grille. Pour les rendre comparables et les ramener dans $[0, 1]$, on a besoin d'une **référence** : \"c'est quoi une grosse erreur ?\"\n",
    "\n",
    "On utilise le **99e percentile** (p99) de toutes les erreurs observées jusqu'ici :\n",
    "\n",
    "$$\\delta_{norm} = \\min\\left(\\frac{\\|\\delta_M\\|}{\\text{p99}},\\; 1\\right)$$\n",
    "\n",
    "**Pourquoi p99 ?**\n",
    "\n",
    "Imaginez les erreurs observées triées de la plus petite à la plus grande. Le p99 est la valeur en dessous de laquelle se trouvent 99% des erreurs. En divisant par p99 :\n",
    "- Une erreur \"normale\" donne $\\delta_{norm}$ entre 0 et 1\n",
    "- Seul le 1% d'erreurs les plus extrêmes est clippé à 1\n",
    "\n",
    "**Pourquoi pas le maximum ?** Le max est souvent un pic isolé (par exemple, la toute première erreur de l'agent qui ne sait encore rien). Diviser par ce pic écraserait toutes les autres erreurs vers 0, rendant la carte illisible.\n",
    "\n",
    "**Pourquoi pas la moyenne ?** La moyenne donnerait des valeurs normalisées > 1 pour toute erreur au-dessus de la moyenne — la moitié des cases seraient clippées à 1.\n",
    "\n",
    "Le p99 est un bon compromis : il ignore les pics extrêmes tout en conservant le contraste entre les cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sec4-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861bac74591d487da36094cedc8cfb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=500, continuous_update=False, description='Steps', max=3000, min=50, ste…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_td_errors(n_steps, alpha_M):\n",
    "    \"\"\"Montre les erreurs TD par état au cours de l'apprentissage.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M = np.eye(grid.n_states)\n",
    "\n",
    "    # Accumuler les erreurs par état\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "\n",
    "    traj = grid.random_walk(n_steps, seed=42)\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "\n",
    "    # Erreur moyenne par état\n",
    "    mean_error = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_error[visited] = error_sum[visited] / error_count[visited]\n",
    "\n",
    "    # Normalisation p99\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    normalized = np.clip(mean_error / max(p99, 1e-8), 0, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "    # Erreur normalisée p99\n",
    "    grid.plot(values=normalized, ax=axes[0], show_goal=False,\n",
    "              title='Erreur normalisée par case', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "    # Nombre de visites\n",
    "    grid.plot(values=error_count, ax=axes[1], show_goal=False,\n",
    "              title='Nombre de visites', cmap='Blues')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Gauche : rouge = l'agent se trompe souvent ici (zone mal connue)\")\n",
    "    print(\"         jaune pâle = peu d'erreurs (zone bien apprise)\")\n",
    "    print(\"Droite : bleu foncé = case souvent visitée, clair = rarement\")\n",
    "    print()\n",
    "    print(f\"p99 = {p99:.4f}, états non visités : {(~visited).sum()}/{grid.n_states}\")\n",
    "    print()\n",
    "    print(\"→ Les cases peu visitées ont souvent plus d'erreurs (l'agent ne les connaît pas)\")\n",
    "    print(\"→ Augmentez les steps : les erreurs diminuent partout\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_td_errors,\n",
    "    n_steps=widgets.IntSlider(value=500, min=50, max=3000, step=50,\n",
    "                               description='Steps', continuous_update=False),\n",
    "    alpha_M=widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "                                description='α_M (cf. Sec. 2)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Carte d'incertitude U(s)\n",
    "\n",
    "### Le besoin\n",
    "\n",
    "En Section 4, on a vu que l'erreur $\\|\\delta_M(s)\\|$ mesure \"l'agent s'est-il trompé à cet instant, en cette case\". Mais cette erreur est **bruitée** : une seule observation peut donner un pic d'erreur juste par malchance.\n",
    "\n",
    "L'agent a besoin d'une mesure plus **stable** : \"est-ce que je connais bien cette zone, **en général** ?\" — pas juste \"est-ce que je me suis trompé cette fois-ci\".\n",
    "\n",
    "C'est le rôle de $U(s)$ : transformer les erreurs ponctuelles en une **carte de confiance durable**, case par case.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imaginez que vous visitez un quartier de la ville :\n",
    "- La **première fois**, vous vous trompez souvent de chemin → incertitude haute\n",
    "- Après **quelques visites**, vous vous trompez de moins en moins → incertitude diminue\n",
    "- Après **beaucoup de visites**, vous connaissez bien le quartier → incertitude faible et stable\n",
    "\n",
    "$U(s)$ reproduit exactement ce comportement. Pour chaque case $s$, l'agent stocke un **historique de ses erreurs récentes** et en fait la moyenne.\n",
    "\n",
    "### Trois régimes\n",
    "\n",
    "$U(s)$ dépend du nombre de fois que l'agent a visité la case $s$ :\n",
    "\n",
    "| Régime | Condition | Ce que fait l'agent | Formule |\n",
    "|---------|-----------|-------------------|----------|\n",
    "| **Jamais visité** | $visits(s) = 0$ | \"Je ne suis jamais allé ici → incertitude maximale\" | $U(s) = U_{prior}$ |\n",
    "| **Peu de visites** | $0 < visits(s) < K$ | \"J'ai quelques observations mais pas assez pour faire une moyenne fiable\" | $U(s) = U_{prior} \\cdot decay^{visits(s)}$ |\n",
    "| **Assez de visites** | $visits(s) \\geq K$ | \"J'ai assez de données → je fais la moyenne de mes $K$ dernières erreurs\" | $U(s) = \\text{mean}(\\text{buffer}(s))$ |\n",
    "\n",
    "Les paramètres :\n",
    "- **$U_{prior}$** (défaut 0.8) : incertitude de départ. \"Avant de voir quoi que ce soit, je suis incertain à 80%.\"\n",
    "- **$decay$** (défaut 0.85) : vitesse de la descente initiale. Chaque visite multiplie l'incertitude par $decay$.\n",
    "- **$K$** (défaut 20) : taille du buffer. Après $K$ visites, l'agent bascule sur la moyenne de ses erreurs réelles.\n",
    "\n",
    "### Pourquoi 3 régimes ?\n",
    "\n",
    "- **Régime 1** (jamais visité) : sans données, on est prudent → $U$ élevé.\n",
    "- **Régime 2** (cold-start) : on n'a pas encore $K$ erreurs pour faire une moyenne fiable. La formule $U_{prior} \\cdot decay^{visits}$ fait baisser $U$ progressivement, comme un a priori qui s'estompe.\n",
    "- **Régime 3** (convergé) : le buffer est plein → la moyenne reflète les erreurs réelles. Si l'agent connaît bien la zone, les erreurs sont petites → $U$ bas. Si la zone a changé, les erreurs remontent → $U$ remonte aussi.\n",
    "\n",
    "### Comment choisir la taille du buffer $K$ ?\n",
    "\n",
    "Le buffer est une **mémoire glissante** : l'agent garde ses $K$ dernières erreurs pour chaque case et en fait la moyenne. Les anciennes erreurs (au-delà de $K$) sont oubliées.\n",
    "\n",
    "Le choix de $K$ est un compromis entre **stabilité** et **réactivité** :\n",
    "\n",
    "- **$K$ petit** (ex. 5) : la moyenne ne porte que sur 5 erreurs → elle est **bruitée** (un seul pic la fait monter), mais elle **réagit vite** si la zone change.\n",
    "- **$K$ grand** (ex. 50) : la moyenne est lissée sur 50 erreurs → elle est **stable**, mais elle met du temps à remonter si l'environnement change (les anciennes erreurs, petites, \"diluent\" les nouvelles erreurs, grandes).\n",
    "\n",
    "**En pratique :** on choisit $K$ en fonction de la fréquence à laquelle l'environnement peut changer.\n",
    "- Environnement **stable** (les murs ne bougent jamais) → $K$ grand (30–50) pour un $U$ bien lissé.\n",
    "- Environnement **changeant** (obstacles qui apparaissent/disparaissent) → $K$ petit (10–15) pour détecter vite que \"cette zone, je ne la connais plus\".\n",
    "\n",
    "Le graphe ci-dessous montre $U(s)$ en fonction du nombre de visites pour une case donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "sec5-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9ddcc4c8e9433bb6373e479f03d4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.8, continuous_update=False, description='U_prior', max=0.95, min=0.5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_uncertainty_regimes(U_prior, decay, K):\n",
    "    \"\"\"Visualise les 3 régimes de U(s) en fonction du nombre de visites.\"\"\"\n",
    "    visits = np.arange(0, 60)\n",
    "\n",
    "    # Simuler des erreurs de buffer décroissantes (convergence réaliste)\n",
    "    U_values = []\n",
    "    buffer = []\n",
    "    for v in visits:\n",
    "        if v == 0:\n",
    "            U_values.append(U_prior)\n",
    "        elif v < K:\n",
    "            U_values.append(U_prior * (decay ** v))\n",
    "        else:\n",
    "            # Simuler buffer : erreurs décroissantes + bruit\n",
    "            simulated_error = max(0.01, 0.5 * np.exp(-0.05 * v) + 0.02 * np.sin(v))\n",
    "            buffer.append(simulated_error)\n",
    "            if len(buffer) > K:\n",
    "                buffer = buffer[-K:]\n",
    "            U_values.append(np.mean(buffer))\n",
    "\n",
    "    U_values = np.array(U_values)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(9, 4))\n",
    "\n",
    "    # Zones colorées pour les régimes\n",
    "    ax.axvspan(0, 0.5, alpha=0.15, color='red', label='Non visité')\n",
    "    ax.axvspan(0.5, K, alpha=0.15, color='orange', label=f'Cold-start (< K={K})')\n",
    "    ax.axvspan(K, 60, alpha=0.15, color='green', label=f'Convergé (≥ K={K})')\n",
    "\n",
    "    ax.plot(visits, U_values, 'b-', linewidth=2, label='U(s)')\n",
    "    ax.axhline(y=U_prior, color='red', linestyle='--', alpha=0.5, label=f'U_prior={U_prior}')\n",
    "    ax.axvline(x=K, color='gray', linestyle=':', alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('Nombre de visites')\n",
    "    ax.set_ylabel('U(s)')\n",
    "    ax.set_title('Incertitude U(s) : 3 régimes')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Lecture du graphe :\")\n",
    "    print(f\"  Axe horizontal = nombre de fois que l'agent a visité cette case\")\n",
    "    print(f\"  Axe vertical   = incertitude U(s) (1 = ne sait rien, 0 = connaît parfaitement)\")\n",
    "    print(f\"  Courbe bleue   = évolution de U au fil des visites\")\n",
    "    print()\n",
    "    print(\"Les 3 zones colorées :\")\n",
    "    print(f\"  Rouge  (0 visite)  : U = {U_prior} — l'agent n'est jamais venu ici, incertitude maximale\")\n",
    "    print(f\"  Orange (1 à {K-1} visites) : U descend vite — chaque visite réduit l'incertitude\")\n",
    "    print(f\"    Exemple à 5 visites : U = {U_prior} × {decay}⁵ = {U_prior * decay**5:.3f}\")\n",
    "    print(f\"  Vert   (≥ {K} visites)  : U = moyenne des {K} dernières erreurs réelles\")\n",
    "    print(f\"    → U se stabilise à un niveau bas si l'agent connaît bien la zone\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  U_prior haut (0.95) → l'agent démarre très incertain, la descente est plus longue\")\n",
    "    print(\"  decay bas (0.7)     → chaque visite réduit beaucoup l'incertitude (descente rapide)\")\n",
    "    print(\"  K grand (50)        → il faut plus de visites avant de basculer sur les erreurs réelles\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_uncertainty_regimes,\n",
    "    U_prior=widgets.FloatSlider(value=0.8, min=0.5, max=0.95, step=0.05,\n",
    "                                 description='U_prior', continuous_update=False),\n",
    "    decay=widgets.FloatSlider(value=0.85, min=0.7, max=0.95, step=0.05,\n",
    "                               description='decay', continuous_update=False),\n",
    "    K=widgets.IntSlider(value=20, min=5, max=50, step=5,\n",
    "                         description='K (buffer)')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Signal de confiance C(s)\n",
    "\n",
    "### Pourquoi ne pas utiliser U directement ?\n",
    "\n",
    "En Section 5, on a construit $U(s) \\in [0, 1]$ : une carte d'incertitude par case. On pourrait s'en servir directement pour prendre des décisions (\"si $U > 0.5$, explorer\"). Mais $U$ pose deux problèmes :\n",
    "\n",
    "1. **Pas de seuil net.** $U = 0.35$, c'est incertain ou pas ? La frontière entre \"je sais\" et \"je ne sais pas\" est floue. Or l'agent a besoin de **trancher** : \"est-ce que je fais confiance à ma prédiction ici, oui ou non ?\"\n",
    "\n",
    "2. **Sensibilité aux paramètres.** $U$ dépend de $U_{prior}$, $decay$, $K$... Ses valeurs brutes varient beaucoup selon la configuration. On veut un signal de confiance dont l'échelle est toujours la même : 0 = aucune confiance, 1 = confiance totale.\n",
    "\n",
    "### L'idée\n",
    "\n",
    "On transforme $U$ en un signal de **confiance** $C(s)$ qui a deux propriétés :\n",
    "- **Inversé** : incertitude haute → confiance basse (et inversement)\n",
    "- **Décision nette** : la transition entre \"confiant\" et \"pas confiant\" est rapide, pas graduelle\n",
    "\n",
    "Pour ça, on utilise une **sigmoïde** — une fonction en forme de S qui écrase les valeurs vers 0 ou 1 :\n",
    "\n",
    "$$C(s) = \\frac{1}{1 + \\exp\\left(\\beta \\cdot (U(s) - \\theta_C)\\right)}$$\n",
    "\n",
    "### Les paramètres\n",
    "\n",
    "| Paramètre | Ce qu'il contrôle | Défaut |\n",
    "|-----------|------------------|--------|\n",
    "| $\\theta_C$ | Le **centre** : à quelle incertitude la confiance vaut exactement 0.5. \"En dessous de $\\theta_C$, je suis plutôt confiant. Au-dessus, plutôt pas.\" | 0.3 |\n",
    "| $\\beta$ | La **pente** : à quel point la transition est brutale. $\\beta$ grand = passage quasi instantané entre 0 et 1. $\\beta$ petit = transition douce et graduelle. | 10 |\n",
    "\n",
    "**Exemples concrets** (avec $\\beta = 10$, $\\theta_C = 0.3$) :\n",
    "- $U = 0.1$ (peu d'erreurs) → $C \\approx 0.88$ — l'agent est **confiant**\n",
    "- $U = 0.3$ (erreurs moyennes) → $C = 0.50$ — l'agent **hésite**\n",
    "- $U = 0.6$ (beaucoup d'erreurs) → $C \\approx 0.05$ — l'agent **ne fait pas confiance** à sa prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sec6-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56871bc992cd441c95d9dad1bbcffc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=10.0, continuous_update=False, description='β (pente)', max=25.0, min=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confidence_sigmoid(beta, theta_C):\n",
    "    \"\"\"Visualise la sigmoïde C(U) et la heatmap C sur la grille.\"\"\"\n",
    "    U_range = np.linspace(0, 1, 200)\n",
    "    C_range = 1 / (1 + np.exp(beta * (U_range - theta_C)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Courbe sigmoïde\n",
    "    axes[0].plot(U_range, C_range, 'b-', linewidth=2)\n",
    "    axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].axvline(x=theta_C, color='red', linestyle='--', alpha=0.5,\n",
    "                    label=f'θ_C = {theta_C}')\n",
    "    axes[0].fill_between(U_range, C_range, alpha=0.1)\n",
    "    axes[0].set_xlabel('U(s) — Incertitude')\n",
    "    axes[0].set_ylabel('C(s) — Confiance')\n",
    "    axes[0].set_title(f'Sigmoïde : β={beta}, θ_C={theta_C}')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Simuler U après apprentissage partiel (1000 steps)\n",
    "    # Assez pour que les zones fréquentées soient bien apprises (vert)\n",
    "    # mais pas assez pour que tout soit appris (contraste vert/rouge)\n",
    "    gamma = 0.95\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    M_partial = np.eye(grid.n_states)\n",
    "    n_sim = 1000\n",
    "    traj = grid.random_walk(n_sim, seed=42)\n",
    "\n",
    "    # Accumuler erreurs par état (comme Section 4)\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t+1]\n",
    "        delta, M_partial = grid.td_update(M_partial, s, s_next, gamma, 0.1)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "\n",
    "    # U = erreur moyenne normalisée par état\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "    U_sim = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    C_sim = 1 / (1 + np.exp(beta * (U_sim - theta_C)))\n",
    "\n",
    "    grid.plot(values=C_sim, ax=axes[1], show_goal=False,\n",
    "              title='C(s) sur la grille (après 1000 steps)', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    n_green = (C_sim > 0.5).sum()\n",
    "    n_red = (C_sim <= 0.5).sum()\n",
    "    C_at_0 = 1/(1+np.exp(beta*(0-theta_C)))\n",
    "    C_at_1 = 1/(1+np.exp(beta*(1-theta_C)))\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — la sigmoïde C(U) :\")\n",
    "    print(f\"  Axe horizontal = incertitude U (0 = sûr, 1 = ne sait rien)\")\n",
    "    print(f\"  Axe vertical   = confiance C (0 = pas confiant, 1 = confiant)\")\n",
    "    print(f\"  Ligne rouge pointillée = θ_C = {theta_C} → à ce U, la confiance vaut exactement 0.5\")\n",
    "    print(f\"  À gauche de θ_C : C monte vers {C_at_0:.2f} (confiant)\")\n",
    "    print(f\"  À droite de θ_C : C descend vers {C_at_1:.3f} (pas confiant)\")\n",
    "    print()\n",
    "    print(\"Droite — la confiance sur la grille :\")\n",
    "    print(f\"  Vert  = confiance élevée (zone bien apprise) — {n_green} cases\")\n",
    "    print(f\"  Rouge = confiance faible (zone mal connue) — {n_red} cases\")\n",
    "    print(f\"  Gris  = mur\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  β petit (2)  → transition douce, beaucoup de cases en jaune (intermédiaire)\")\n",
    "    print(\"  β grand (25) → transition brutale, les cases sont soit vertes soit rouges\")\n",
    "    print(\"  θ_C bas (0.1) → l'agent exige très peu d'erreurs pour être confiant → plus de rouge\")\n",
    "    print(\"  θ_C haut (0.6) → l'agent est indulgent → plus de vert\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_confidence_sigmoid,\n",
    "    beta=widgets.FloatSlider(value=10, min=2, max=25, step=1,\n",
    "                              description='β (pente)', continuous_update=False),\n",
    "    theta_C=widgets.FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05,\n",
    "                                 description='θ_C (centre)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec7-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 — Exploration adaptative\n",
    "\n",
    "### Le problème\n",
    "\n",
    "En Section 3, on a vu que l'agent choisit ses actions en fonction de $V(s) = M \\cdot R$ : il va vers les cases de haute valeur (proches du goal). C'est l'**exploitation** — faire ce qu'on sait être bon.\n",
    "\n",
    "Mais il y a un piège : si l'agent ne fait qu'exploiter, il reste dans les zones qu'il connaît déjà et **ne découvre jamais le reste de la grille**. Il pourrait passer à côté d'un meilleur chemin, ou ne jamais apprendre ce qu'il y a de l'autre côté du mur.\n",
    "\n",
    "L'agent doit donc aussi **explorer** — aller dans des zones inconnues pour améliorer son $M$. C'est le dilemme classique exploration vs exploitation.\n",
    "\n",
    "### La solution naïve : epsilon-greedy\n",
    "\n",
    "L'approche standard est de choisir une action aléatoire avec probabilité $\\varepsilon$, et la meilleure action sinon. Mais avec un $\\varepsilon$ fixe (ex. 0.1), l'agent explore **autant** dans les zones qu'il connaît bien que dans les zones inconnues. C'est du gaspillage.\n",
    "\n",
    "### Explorer là où c'est utile\n",
    "\n",
    "L'idée d'explorer davantage dans les zones incertaines est un principe classique en RL (UCB, curiosité intrinsèque, etc.). Ce que PRISM apporte, c'est d'utiliser la carte $U(s)$ construite à partir des erreurs TD de la SR (Section 5) comme signal d'incertitude. L'agent sait **où** il est incertain, et il s'en sert de deux façons complémentaires :\n",
    "\n",
    "### 1. Epsilon adaptatif\n",
    "\n",
    "$$\\varepsilon(s) = \\varepsilon_{min} + (\\varepsilon_{max} - \\varepsilon_{min}) \\cdot U(s)$$\n",
    "\n",
    "Au lieu d'un $\\varepsilon$ fixe, chaque case a son propre taux d'exploration :\n",
    "- **Zone bien connue** ($U$ bas) → $\\varepsilon \\approx \\varepsilon_{min}$ → l'agent exploite (il sait quoi faire)\n",
    "- **Zone inconnue** ($U$ haut) → $\\varepsilon \\approx \\varepsilon_{max}$ → l'agent explore (il a besoin d'apprendre)\n",
    "\n",
    "### 2. Bonus d'exploration\n",
    "\n",
    "$$V_{explore}(s) = V(s) + \\lambda \\cdot U(s)$$\n",
    "\n",
    "L'incertitude agit comme une **récompense bonus** : les zones inconnues deviennent artificiellement attractives. L'agent est \"curieux\" — il est attiré par ce qu'il ne connaît pas encore.\n",
    "\n",
    "- $\\lambda = 0$ → pas de bonus, l'agent ne fait qu'exploiter\n",
    "- $\\lambda$ grand → l'agent privilégie l'exploration, même si le goal est ailleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "sec7-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28844c99428f414a851ccbf3ffc531af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, continuous_update=False, description='ε_min', max=0.1, min=0.001…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_exploration(eps_min, eps_max, lam):\n",
    "    \"\"\"Visualise epsilon adaptatif et V_explore.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    R = grid.reward_vector()\n",
    "    V = M_star @ R\n",
    "\n",
    "    # Simuler U après apprentissage partiel (1000 steps, comme Section 6)\n",
    "    M_partial = np.eye(grid.n_states)\n",
    "    traj = grid.random_walk(1000, seed=42)\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t+1]\n",
    "        delta, M_partial = grid.td_update(M_partial, s, s_next, gamma, 0.1)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "    U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    # Epsilon adaptatif\n",
    "    eps = eps_min + (eps_max - eps_min) * U\n",
    "\n",
    "    # V_explore\n",
    "    V_norm = V / max(V.max(), 1e-8)\n",
    "    V_explore = V_norm + lam * U\n",
    "    V_explore_norm = V_explore / max(V_explore.max(), 1e-8)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13, 3.5))\n",
    "\n",
    "    grid.plot(values=eps, ax=axes[0], show_goal=False,\n",
    "              title=f'ε(s) — taux d\\'exploration',\n",
    "              cmap='YlOrRd', vmin=eps_min, vmax=eps_max)\n",
    "\n",
    "    grid.plot(values=V_norm, ax=axes[1], show_goal=False,\n",
    "              title='V(s) — sans bonus', cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "    grid.plot(values=V_explore_norm, ax=axes[2], show_goal=False,\n",
    "              title=f'V + λ·U — avec bonus (λ={lam})', cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_V = grid.idx_to_pos[np.argmax(V)]\n",
    "    best_Vx = grid.idx_to_pos[np.argmax(V_explore)]\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — ε(s), le taux d'exploration par case :\")\n",
    "    print(f\"  Rouge = exploration forte (zone inconnue, U élevé → ε ≈ {eps_max})\")\n",
    "    print(f\"  Jaune pâle = exploration faible (zone connue, U bas → ε ≈ {eps_min})\")\n",
    "    print(f\"  → L'agent fait des actions aléatoires plus souvent dans les zones rouges\")\n",
    "    print()\n",
    "    print(\"Centre — V(s), la valeur sans bonus :\")\n",
    "    print(f\"  Jaune = case de haute valeur (proche du goal)\")\n",
    "    print(f\"  Violet = case de faible valeur (loin du goal)\")\n",
    "    print(f\"  → L'agent est attiré uniquement vers le goal\")\n",
    "    print()\n",
    "    print(\"Droite — V + λ·U, la valeur avec bonus d'exploration :\")\n",
    "    print(f\"  Les zones inconnues (U élevé) reçoivent un bonus → elles deviennent plus jaunes\")\n",
    "    print(f\"  → L'agent est attiré à la fois vers le goal ET vers les zones inconnues\")\n",
    "    print()\n",
    "    print(f\"Case la plus attractive : sans bonus = {best_V}, avec bonus (λ={lam}) = {best_Vx}\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  λ = 0   → droite identique au centre (pas de curiosité)\")\n",
    "    print(\"  λ grand (2.0) → les zones inconnues dominent, l'agent ignore presque le goal\")\n",
    "    print(\"  ε_max haut (0.9) → exploration très agressive dans les zones rouges\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_exploration,\n",
    "    eps_min=widgets.FloatSlider(value=0.01, min=0.001, max=0.1, step=0.01,\n",
    "                                 description='ε_min', continuous_update=False),\n",
    "    eps_max=widgets.FloatSlider(value=0.5, min=0.2, max=0.9, step=0.1,\n",
    "                                 description='ε_max', continuous_update=False),\n",
    "    lam=widgets.FloatSlider(value=0.5, min=0.0, max=2.0, step=0.1,\n",
    "                             description='λ (bonus)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec8-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 — Détection de changement\n",
    "\n",
    "### Le problème\n",
    "\n",
    "Jusqu'ici, on a supposé que l'environnement ne change pas : les murs restent en place, les passages restent ouverts. Mais que se passe-t-il si un **mur apparaît** et bloque le passage entre les deux pièces ?\n",
    "\n",
    "Le $M$ que l'agent a appris reflète l'**ancienne** structure. Il prédit par exemple \"depuis la pièce gauche, je passerai souvent par le passage pour atteindre la pièce droite\". Mais le passage est bloqué — cette prédiction est maintenant **fausse**.\n",
    "\n",
    "L'agent a besoin de **détecter** que quelque chose a changé, pour savoir qu'il doit réapprendre son $M$.\n",
    "\n",
    "### Comment le détecter ?\n",
    "\n",
    "Le mécanisme est déjà en place grâce aux sections précédentes :\n",
    "\n",
    "1. **L'environnement change** → le $M$ appris ne correspond plus à la réalité\n",
    "2. **Les erreurs TD augmentent** (Section 4) → l'agent se trompe quand il passe par la zone modifiée\n",
    "3. **$U(s)$ remonte** (Section 5) → le buffer d'erreurs se remplit de nouvelles erreurs élevées\n",
    "4. **$C(s)$ chute** (Section 6) → la confiance baisse dans la zone affectée\n",
    "\n",
    "Il ne reste qu'à résumer cette information en un seul signal : \"est-ce que l'environnement vient de changer ?\"\n",
    "\n",
    "### Le score de changement\n",
    "\n",
    "On regarde l'incertitude moyenne des cases **récemment visitées** :\n",
    "\n",
    "$$\\text{score} = \\frac{1}{|S_{recent}|} \\sum_{s \\in S_{recent}} U(s)$$\n",
    "\n",
    "- $S_{recent}$ = les cases visitées dans les ~50 derniers pas\n",
    "- En temps normal, ces cases sont bien connues → $U$ bas → score bas\n",
    "- Après un changement, les erreurs remontent → $U$ monte → **le score fait un pic**\n",
    "\n",
    "On compare ce score à un seuil :\n",
    "\n",
    "$$\\text{change\\_detected} = \\mathbb{1}(\\text{score} > \\theta_{change})$$\n",
    "\n",
    "### Pourquoi les états \"récents\" ?\n",
    "\n",
    "On ne regarde pas toute la grille, seulement les cases **récemment visitées**, parce que :\n",
    "- Le changement n'affecte que la zone modifiée — les cases loin du changement gardent un $U$ bas\n",
    "- L'agent ne peut constater le changement que là où il passe — s'il n'est pas allé dans la zone modifiée, il ne peut pas encore le savoir\n",
    "- En moyennant sur les cases récentes, le signal est **localisé** : il reflète ce que l'agent vit en ce moment\n",
    "\n",
    "Le graphe ci-dessous simule un scénario : l'agent apprend pendant 300 steps, puis le passage est bloqué. On observe le score de changement au fil du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sec8-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff84a916c3eb4b8194e05f35939bc003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, continuous_update=False, description='θ_change', max=0.8, min=0.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_change_detection(theta_change):\n",
    "    \"\"\"Simule apprentissage, perturbation, et détection de changement.\"\"\"\n",
    "    gamma, alpha = 0.95, 0.1\n",
    "    K = 10  # buffer size pour U\n",
    "\n",
    "    grid_normal = ToyGrid.two_rooms()\n",
    "    M = np.eye(grid_normal.n_states)\n",
    "    n_states = grid_normal.n_states\n",
    "\n",
    "    from collections import deque\n",
    "    buffers = [deque(maxlen=K) for _ in range(n_states)]\n",
    "    visit_counts = np.zeros(n_states)\n",
    "    recent_states = deque(maxlen=50)\n",
    "    all_deltas = []\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    total_steps = 600\n",
    "    change_at = 300\n",
    "\n",
    "    traj = grid_normal.random_walk(total_steps, seed=42)\n",
    "\n",
    "    for t in range(total_steps):\n",
    "        s = traj[t]\n",
    "        s_next = traj[t + 1] if t + 1 < len(traj) else s\n",
    "\n",
    "        delta, M = grid_normal.td_update(M, s, s_next, gamma, alpha)\n",
    "        norm = np.linalg.norm(delta)\n",
    "\n",
    "        # Après step 300 : on simule le blocage du passage.\n",
    "        # En vrai, les transitions changeraient. Ici on simule l'effet :\n",
    "        # les erreurs TD deviennent plus grandes car M reflète l'ancienne structure.\n",
    "        if t >= change_at:\n",
    "            norm *= 3.0\n",
    "\n",
    "        all_deltas.append(norm)\n",
    "        p99 = np.percentile(all_deltas[-500:], 99) if len(all_deltas) > 10 else 1.0\n",
    "        normalized = min(norm / max(p99, 1e-8), 1.0)\n",
    "\n",
    "        visit_counts[s] += 1\n",
    "        buffers[s].append(normalized)\n",
    "        recent_states.append(s)\n",
    "\n",
    "        # Calculer le score toutes les 5 steps\n",
    "        if t % 5 == 0 and len(recent_states) > 5:\n",
    "            unique_recent = set(recent_states)\n",
    "            U_values = []\n",
    "            for rs in unique_recent:\n",
    "                if visit_counts[rs] == 0:\n",
    "                    U_values.append(0.8)\n",
    "                elif visit_counts[rs] < K:\n",
    "                    U_values.append(0.8 * (0.85 ** visit_counts[rs]))\n",
    "                else:\n",
    "                    U_values.append(np.mean(buffers[rs]))\n",
    "            score = np.mean(U_values)\n",
    "            scores.append((t, score))\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "    steps, change_scores = zip(*scores)\n",
    "    ax.plot(steps, change_scores, 'b-', linewidth=1.5, label='Score de changement')\n",
    "    ax.axhline(y=theta_change, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Seuil θ = {theta_change}')\n",
    "    ax.axvline(x=change_at, color='orange', linestyle='-', linewidth=2,\n",
    "               alpha=0.7, label='Passage bloqué (step 300)')\n",
    "\n",
    "    # Colorier la zone de détection (score > seuil)\n",
    "    for i in range(len(steps)-1):\n",
    "        if change_scores[i] > theta_change:\n",
    "            ax.axvspan(steps[i], steps[i+1], alpha=0.15, color='red')\n",
    "\n",
    "    ax.set_xlabel('Steps (temps)')\n",
    "    ax.set_ylabel('Score de changement')\n",
    "    ax.set_title('Scénario : l\\'agent apprend, puis le passage est bloqué')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    # Annotations sur le graphe\n",
    "    ax.annotate('Apprentissage normal\\n(erreurs diminuent)',\n",
    "                xy=(150, 0.15), fontsize=9, ha='center', color='steelblue')\n",
    "    ax.annotate('Passage bloqué !\\n(erreurs remontent)',\n",
    "                xy=(450, 0.85), fontsize=9, ha='center', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Latence de détection\n",
    "    post_change = [(s, sc) for s, sc in scores if s >= change_at and sc > theta_change]\n",
    "\n",
    "    print(\"Lecture du graphe :\")\n",
    "    print()\n",
    "    print(\"C'est un film : le temps avance de gauche à droite (0 → 600 steps).\")\n",
    "    print()\n",
    "    print(\"  Axe horizontal = le temps (nombre de steps d'expérience)\")\n",
    "    print(\"  Axe vertical   = le score de changement (= moyenne de U sur les cases récentes)\")\n",
    "    print(\"  Courbe bleue   = le score au fil du temps\")\n",
    "    print(f\"  Ligne rouge horizontale = seuil θ_change = {theta_change}\")\n",
    "    print(\"  Ligne orange verticale  = step 300, le moment où le passage est bloqué\")\n",
    "    print(\"  Zone rouge transparente = le score dépasse le seuil → changement détecté\")\n",
    "    print()\n",
    "    print(\"Le scénario en 2 phases :\")\n",
    "    print(\"  Steps 0–300   : l'agent explore la grille normalement.\")\n",
    "    print(\"                  Il apprend M, ses erreurs diminuent, le score baisse.\")\n",
    "    print(\"  Step 300       : le passage entre les deux pièces est bloqué par un mur.\")\n",
    "    print(\"                  Le M appris est maintenant faux → les erreurs remontent → le score monte.\")\n",
    "    print()\n",
    "    if post_change:\n",
    "        latency = post_change[0][0] - change_at\n",
    "        print(f\"  → Détection après {latency} steps (le score franchit le seuil rouge)\")\n",
    "    else:\n",
    "        print(f\"  → Pas de détection : le score ne franchit jamais le seuil (trop haut ?)\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  θ bas (0.2)  → détection très rapide, mais risque de fausses alarmes avant step 300\")\n",
    "    print(\"  θ haut (0.7) → détection lente ou absente, mais pas de fausses alarmes\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_change_detection,\n",
    "    theta_change=widgets.FloatSlider(value=0.5, min=0.1, max=0.8, step=0.05,\n",
    "                                      description='θ_change', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec9-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 — Signal \"je ne sais pas\" (IDK)\n",
    "\n",
    "### Le besoin\n",
    "\n",
    "En Section 6, on a construit $C(s)$ : une confiance entre 0 et 1 pour chaque case. Mais l'agent doit prendre une **décision binaire** : \"est-ce que je fais confiance à ma prédiction ici, ou pas ?\"\n",
    "\n",
    "C'est comme un médecin qui regarde une radio : il peut être plus ou moins sûr de son diagnostic, mais à un moment il doit trancher — \"je sais ce que c'est\" ou \"je ne sais pas, je demande un second avis\".\n",
    "\n",
    "### La règle\n",
    "\n",
    "$$\\text{IDK}(s) = \\mathbb{1}\\left(C(s) < \\theta_{idk}\\right)$$\n",
    "\n",
    "- Si $C(s) \\geq \\theta_{idk}$ → l'agent est assez confiant → il **exploite** (fait ce que $V$ lui dit)\n",
    "- Si $C(s) < \\theta_{idk}$ → l'agent n'est pas assez confiant → il signale **\"je ne sais pas\"** et explore à la place\n",
    "\n",
    "$\\theta_{idk}$ (défaut 0.3) est le **seuil de confiance minimale**. C'est un choix de design :\n",
    "- **$\\theta_{idk}$ bas** (0.1) → l'agent dit rarement \"je ne sais pas\" — il fait confiance même avec peu de certitude\n",
    "- **$\\theta_{idk}$ haut** (0.6) → l'agent est très prudent — il dit \"je ne sais pas\" dès que la confiance n'est pas élevée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sec9-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001bfa21b40a44f6a23bd2c148009d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.3, continuous_update=False, description='θ_idk', max=0.7, min=0.1, s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_idk_signal(theta_idk, beta, theta_C):\n",
    "    \"\"\"Visualise les zones où l'agent dit 'je ne sais pas'.\"\"\"\n",
    "    gamma = 0.95\n",
    "    M_star = grid.true_sr(gamma)\n",
    "\n",
    "    # Simuler U après apprentissage partiel (1000 steps, comme Sections 6-7)\n",
    "    M_partial = np.eye(grid.n_states)\n",
    "    traj = grid.random_walk(1000, seed=42)\n",
    "    error_sum = np.zeros(grid.n_states)\n",
    "    error_count = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t+1]\n",
    "        delta, M_partial = grid.td_update(M_partial, s, s_next, gamma, 0.1)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        all_norms.append(norm)\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = error_count > 0\n",
    "    mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "    U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    # Confiance\n",
    "    C = 1 / (1 + np.exp(beta * (U - theta_C)))\n",
    "\n",
    "    # IDK signal\n",
    "    idk = (C < theta_idk).astype(float)\n",
    "    pct_idk = 100 * idk.mean()\n",
    "    n_idk = int(idk.sum())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Gauche : C(s) sur la grille\n",
    "    grid.plot(values=C, ax=axes[0], show_goal=False,\n",
    "              title='C(s) — Confiance', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    # Droite : IDK binaire (vert = confiant, rouge = IDK)\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    cmap_idk = ListedColormap(['#2ecc71', '#e74c3c'])\n",
    "    idk_grid = grid.to_grid(idk)\n",
    "    axes[1].imshow(idk_grid, cmap=cmap_idk, vmin=0, vmax=1,\n",
    "                   origin='upper', interpolation='nearest')\n",
    "    axes[1].set_title(f'IDK (θ = {theta_idk}) — {n_idk}/{grid.n_states} cases en IDK')\n",
    "    for w in grid.walls:\n",
    "        axes[1].add_patch(plt.Rectangle((w[1]-0.5, w[0]-0.5), 1, 1,\n",
    "                          fill=True, color='gray', alpha=0.8))\n",
    "    axes[1].set_xticks(np.arange(-0.5, grid.cols, 1), minor=True)\n",
    "    axes[1].set_yticks(np.arange(-0.5, grid.rows, 1), minor=True)\n",
    "    axes[1].grid(which='minor', color='black', linewidth=0.5, alpha=0.3)\n",
    "    axes[1].tick_params(which='both', bottom=False, left=False,\n",
    "                        labelbottom=False, labelleft=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — C(s), la confiance par case (même carte que Section 6) :\")\n",
    "    print(\"  Vert  = confiance élevée (zone bien apprise)\")\n",
    "    print(\"  Rouge = confiance faible (zone mal connue)\")\n",
    "    print()\n",
    "    print(\"Droite — la décision binaire IDK :\")\n",
    "    print(f\"  Vert  = C ≥ {theta_idk} → l'agent fait confiance → il exploite\")\n",
    "    print(f\"  Rouge = C < {theta_idk} → l'agent dit \\\"je ne sais pas\\\" → il explore\")\n",
    "    print(f\"  Gris  = mur\")\n",
    "    print()\n",
    "    print(f\"  → {n_idk} cases en IDK sur {grid.n_states} ({pct_idk:.0f}%)\")\n",
    "    print()\n",
    "    print(\"Le lien entre les deux graphes :\")\n",
    "    print(f\"  Le seuil θ_idk = {theta_idk} coupe la carte de confiance en deux.\")\n",
    "    print(\"  Toutes les cases rouges/oranges à gauche (C faible) deviennent rouges à droite (IDK).\")\n",
    "    print(\"  Toutes les cases vertes à gauche (C élevée) restent vertes à droite.\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  θ_idk bas (0.1)  → presque tout est vert (l'agent fait confiance facilement)\")\n",
    "    print(\"  θ_idk haut (0.6) → beaucoup de rouge (l'agent est très prudent)\")\n",
    "    print(\"  β et θ_C changent la carte de confiance à gauche → ça change aussi l'IDK à droite\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_idk_signal,\n",
    "    theta_idk=widgets.FloatSlider(value=0.3, min=0.1, max=0.7, step=0.05,\n",
    "                                   description='θ_idk', continuous_update=False),\n",
    "    beta=widgets.FloatSlider(value=10, min=2, max=25, step=1,\n",
    "                              description='β (cf. Sec. 6)', continuous_update=False),\n",
    "    theta_C=widgets.FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05,\n",
    "                                 description='θ_C (cf. Sec. 6)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec10-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10 — Récapitulatif interactif\n",
    "\n",
    "Ce tableau de bord réunit les 4 cartes des sections précédentes sur une même simulation. Tout est lié : l'agent apprend $M$ par TD (Sec. 2), calcule $V = M \\cdot R$ (Sec. 3), mesure ses erreurs pour estimer $U$ (Sec. 4-5), et en déduit sa confiance $C$ (Sec. 6).\n",
    "\n",
    "| Carte | Section | Ce qu'elle montre |\n",
    "|-------|---------|------------------|\n",
    "| **M[s, :]** | Sec. 1-2 | Fréquence de visite prédite depuis l'état $s$ |\n",
    "| **V(s)** | Sec. 3 | Valeur de chaque case ($M \\cdot R$) |\n",
    "| **U(s)** | Sec. 4-5 | Incertitude (erreurs moyennes par case) |\n",
    "| **C(s)** | Sec. 6 | Confiance (sigmoïde inverse de U) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "sec10-widget",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9796298ba1a64198bfebd74eef3b2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.95, continuous_update=False, description='γ', max=0.99, min=0.5, ste…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_full_dashboard(gamma, alpha_M, n_steps, state, beta, theta_C):\n",
    "    \"\"\"Tableau de bord complet : M, V, U, C.\"\"\"\n",
    "    # Apprentissage TD\n",
    "    M_star = grid.true_sr(gamma)\n",
    "    M = np.eye(grid.n_states)\n",
    "\n",
    "    traj = grid.random_walk(n_steps, seed=42)\n",
    "    error_sums = np.zeros(grid.n_states)\n",
    "    visit_counts = np.zeros(grid.n_states)\n",
    "    all_norms = []\n",
    "\n",
    "    for t in range(len(traj) - 1):\n",
    "        s, s_next = traj[t], traj[t + 1]\n",
    "        delta, M = grid.td_update(M, s, s_next, gamma, alpha_M)\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sums[s] += norm\n",
    "        visit_counts[s] += 1\n",
    "        all_norms.append(norm)\n",
    "\n",
    "    # V = M · R\n",
    "    R = grid.reward_vector()\n",
    "    V = M @ R\n",
    "\n",
    "    # U = erreur moyenne normalisée (comme Sections 4-7)\n",
    "    p99 = np.percentile(all_norms, 99) if all_norms else 1.0\n",
    "    mean_err = np.zeros(grid.n_states)\n",
    "    visited = visit_counts > 0\n",
    "    mean_err[visited] = error_sums[visited] / visit_counts[visited]\n",
    "    U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "    # C = sigmoïde (comme Section 6)\n",
    "    C = 1 / (1 + np.exp(beta * (U - theta_C)))\n",
    "\n",
    "    # Normaliser M et V pour échelles fixes (comme Sections 1-3)\n",
    "    row = M[state]\n",
    "    row_norm = row / max(row.max(), 1e-8)\n",
    "    V_norm = V / max(V.max(), 1e-8)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 3.5))\n",
    "\n",
    "    # M[s, :] — plasma, normalisé (comme Section 1)\n",
    "    grid.plot(values=row_norm, ax=axes[0], show_goal=False,\n",
    "              title=f'M depuis s={state}', cmap='plasma', vmin=0, vmax=1)\n",
    "    pos = grid.idx_to_pos[state]\n",
    "    axes[0].plot(pos[1], pos[0], 'wo', markersize=10, zorder=5,\n",
    "                 markeredgecolor='black', markeredgewidth=1.5)\n",
    "\n",
    "    # V(s) — plasma (comme Section 3)\n",
    "    grid.plot(values=V_norm, ax=axes[1], show_goal=False,\n",
    "              title='V = M · R', cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "    # U(s) — YlOrRd (comme Section 4)\n",
    "    grid.plot(values=U, ax=axes[2], show_goal=False,\n",
    "              title='U(s)', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "    # C(s) — RdYlGn (comme Section 6)\n",
    "    grid.plot(values=C, ax=axes[3], show_goal=False,\n",
    "              title='C(s)', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    err_m = np.linalg.norm(M - M_star, 'fro') / grid.n_states\n",
    "    n_confident = (C > 0.5).sum()\n",
    "\n",
    "    print(\"Lecture des 4 cartes :\")\n",
    "    print()\n",
    "    print(f\"  M depuis s={state} : fréquence de visite prédite (jaune = souvent, violet = rarement)\")\n",
    "    print(f\"    ⚪ = état de départ choisi\")\n",
    "    print(f\"  V = M · R : valeur des cases (jaune = proche du goal, violet = loin)\")\n",
    "    print(f\"  U(s) : incertitude (rouge = mal connu, jaune pâle = bien appris)\")\n",
    "    print(f\"  C(s) : confiance (vert = confiant, rouge = pas confiant)\")\n",
    "    print()\n",
    "    print(f\"Résumé : erreur M = {err_m:.4f}, U moyen = {U.mean():.3f}, \"\n",
    "          f\"{n_confident}/{grid.n_states} cases confiantes\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  Steps bas (100)   → M mal appris, V faux, U élevé partout, C rouge partout\")\n",
    "    print(\"  Steps haut (3000) → M converge, V correct, U bas, C vert partout\")\n",
    "    print(\"  α_M grand (0.5)   → apprentissage instable, U reste élevé\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_full_dashboard,\n",
    "    gamma=widgets.FloatSlider(value=0.95, min=0.5, max=0.99, step=0.01,\n",
    "                              description='γ', continuous_update=False),\n",
    "    alpha_M=widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01,\n",
    "                                description='α_M (cf. Sec. 2)', continuous_update=False),\n",
    "    n_steps=widgets.IntSlider(value=500, min=50, max=3000, step=50,\n",
    "                               description='Steps', continuous_update=False),\n",
    "    state=widgets.IntSlider(value=0, min=0, max=grid.n_states-1,\n",
    "                            description='s (départ)'),\n",
    "    beta=widgets.FloatSlider(value=10, min=2, max=25, step=1,\n",
    "                              description='β (cf. Sec. 6)', continuous_update=False),\n",
    "    theta_C=widgets.FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05,\n",
    "                                 description='θ_C (cf. Sec. 6)', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Résumé des formules\n",
    "\n",
    "| Concept | Formule | Paramètres clés |\n",
    "|---------|---------|----------------|\n",
    "| SR (définition) | $M(s,s') = \\mathbb{E}[\\sum_t \\gamma^t \\mathbb{1}(s_t=s')]$ | $\\gamma$ |\n",
    "| SR (analytique) | $M^* = (I - \\gamma T)^{-1}$ | $\\gamma$, $T$ |\n",
    "| TD error | $\\delta_M = e(s') + \\gamma M(s',:) - M(s,:)$ | $\\gamma$ |\n",
    "| SR update | $M(s,:) \\leftarrow M(s,:) + \\alpha_M \\delta_M$ | $\\alpha_M$ |\n",
    "| Valeur | $V(s) = M(s,:) \\cdot R$ | |\n",
    "| Récompense | $R(s) \\leftarrow R(s) + \\alpha_R (r - R(s))$ | $\\alpha_R$ |\n",
    "| Incertitude | $U(s) = \\text{mean}(\\text{buffer}(s))$ | $K$, $U_{prior}$, $decay$ |\n",
    "| Confiance | $C(s) = \\frac{1}{1+\\exp(\\beta(U-\\theta_C))}$ | $\\beta$, $\\theta_C$ |\n",
    "| Epsilon adaptatif | $\\varepsilon(s) = \\varepsilon_{min} + (\\varepsilon_{max}-\\varepsilon_{min}) U(s)$ | $\\varepsilon_{min}$, $\\varepsilon_{max}$ |\n",
    "| Bonus exploration | $V_{explore} = V + \\lambda U$ | $\\lambda$ |\n",
    "| Changement | $\\text{score} = \\text{mean}(U(s_{recent}))$ | $\\theta_{change}$ |\n",
    "| IDK | $\\mathbb{1}(C(s) < \\theta_{idk})$ | $\\theta_{idk}$ |\n",
    "\n",
    "**Annexes** : [Spectral](00a_spectral_deep_dive.ipynb) | [Calibration](00b_calibration_methods.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
