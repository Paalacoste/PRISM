{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Appendix B — Calibration Methods\n",
    "\n",
    "### What is this appendix about?\n",
    "\n",
    "In the main notebook (Sections 5-6), we built $U(s)$ (uncertainty) and $C(s)$ (confidence). The agent says \"I am 80% confident about this cell\". But **is that true?** When it says 80%, is it actually correct 80% of the time?\n",
    "\n",
    "This is the question of **calibration**: does the agent's confidence reflect its true performance?\n",
    "\n",
    "This appendix presents the tools for measuring the quality of this calibration:\n",
    "- **ECE**: the mean gap between confidence and actual performance\n",
    "- **Reliability diagram**: the standard calibration visualization\n",
    "- **MI** (Metacognitive Index): does the agent know *where* it is wrong?\n",
    "\n",
    "**Prerequisites:** [00_prism_concepts.ipynb](00_prism_concepts.ipynb) (Sections 4-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import spearmanr\n",
    "from IPython.display import display\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from prism.pedagogy.toy_grid import ToyGrid\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is calibration?\n",
    "\n",
    "### The weather example\n",
    "\n",
    "A weather forecaster says \"80% chance of rain\". How do we know if they are reliable?\n",
    "\n",
    "We look at **all the days when they said \"80%\"**. If it actually rained about 80% of those days, they are **well calibrated**. If it only rained 50% of the time, they are **overconfident** — they announce probabilities that are too high.\n",
    "\n",
    "### For PRISM\n",
    "\n",
    "The agent says $C(s) = 0.8$ — \"I am 80% confident in my prediction for cell $s$\".\n",
    "\n",
    "We can verify: among all cells where the agent says $C \\approx 0.8$, is its SR prediction correct ~80% of the time?\n",
    "\n",
    "| Situation | What it means | Consequence |\n",
    "|-----------|-------------------|-------------|\n",
    "| $C(s) \\approx accuracy(s)$ | **Well calibrated** — confidence reflects reality | The agent can rely on its own signal C |\n",
    "| $C(s) > accuracy(s)$ | **Overconfident** — the agent believes it knows but is wrong | Dangerous: it does not explore enough |\n",
    "| $C(s) < accuracy(s)$ | **Underconfident** — the agent doubts when it is right | Less severe but wastes exploration |\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "The entire metacognitive chain in PRISM (adaptive exploration in Section 7, change detection in Section 8, IDK signal in Section 9) relies on $C(s)$. If $C$ does not reflect reality, these mechanisms make poor decisions.\n",
    "\n",
    "The graph below shows 3 synthetic examples (well calibrated, overconfident, underconfident)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec1-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des données synthétiques de calibration\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Cas 1 : bien calibré\n",
    "conf_good = np.random.beta(2, 2, n)\n",
    "acc_good = np.array([np.random.binomial(1, c) for c in conf_good])\n",
    "\n",
    "# Cas 2 : sur-confiant\n",
    "conf_over = np.random.beta(5, 1, n)\n",
    "acc_over = np.array([np.random.binomial(1, max(0, c - 0.3)) for c in conf_over])\n",
    "\n",
    "# Cas 3 : sous-confiant\n",
    "conf_under = np.random.beta(1, 3, n)\n",
    "acc_under = np.array([np.random.binomial(1, min(1, c + 0.3)) for c in conf_under])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "cases = [\n",
    "    ('Bien calibré', conf_good, acc_good),\n",
    "    ('Sur-confiant', conf_over, acc_over),\n",
    "    ('Sous-confiant', conf_under, acc_under),\n",
    "]\n",
    "\n",
    "for ax, (name, conf, acc) in zip(axes, cases):\n",
    "    n_bins = 8\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_centers = []\n",
    "    bin_accs = []\n",
    "    for i in range(n_bins):\n",
    "        mask = (conf >= bin_edges[i]) & (conf < bin_edges[i+1])\n",
    "        if i == n_bins - 1:\n",
    "            mask = (conf >= bin_edges[i]) & (conf <= bin_edges[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_centers.append(conf[mask].mean())\n",
    "            bin_accs.append(acc[mask].mean())\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Calibration parfaite')\n",
    "    ax.scatter(bin_centers, bin_accs, s=60, c='steelblue', edgecolors='white', zorder=3)\n",
    "    ax.plot(bin_centers, bin_accs, '-', color='steelblue', alpha=0.5)\n",
    "    ax.set_xlabel('Confiance de l\\'agent')\n",
    "    ax.set_ylabel('Accuracy réelle')\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lecture des 3 graphes :\")\n",
    "print()\n",
    "print(\"Chaque graphe montre un 'reliability diagram' :\")\n",
    "print(\"  Axe horizontal = ce que l'agent DIT (sa confiance)\")\n",
    "print(\"  Axe vertical   = ce qui est VRAI (son accuracy réelle)\")\n",
    "print(\"  Diagonale rouge = calibration parfaite (confiance = réalité)\")\n",
    "print(\"  Chaque point = un groupe de prédictions avec des confiances similaires\")\n",
    "print()\n",
    "print(\"Gauche — Bien calibré :\")\n",
    "print(\"  Les points suivent la diagonale.\")\n",
    "print(\"  Quand l'agent dit '80% confiant', il a raison ~80% du temps. ✓\")\n",
    "print()\n",
    "print(\"Centre — Sur-confiant :\")\n",
    "print(\"  Les points sont EN DESSOUS de la diagonale.\")\n",
    "print(\"  L'agent dit '90% confiant' mais n'a raison que ~60% du temps.\")\n",
    "print(\"  → Il se surestime. Dangereux : il n'explore pas assez.\")\n",
    "print()\n",
    "print(\"Droite — Sous-confiant :\")\n",
    "print(\"  Les points sont AU-DESSUS de la diagonale.\")\n",
    "print(\"  L'agent dit '20% confiant' mais a raison ~50% du temps.\")\n",
    "print(\"  → Il se sous-estime. Moins grave, mais gaspille de l'exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Expected Calibration Error (ECE)\n",
    "\n",
    "### The need\n",
    "\n",
    "We want a **single number** that summarizes \"how well calibrated the agent is\". This is the role of the ECE.\n",
    "\n",
    "### The intuition\n",
    "\n",
    "The idea is simple:\n",
    "1. Group predictions by confidence level (e.g., all cells where $C \\in [0.7, 0.8]$)\n",
    "2. For each group, compare the mean confidence with the mean accuracy\n",
    "3. Take the weighted average of the gaps, weighted by group size\n",
    "\n",
    "### The formula\n",
    "\n",
    "$$ECE = \\sum_{b=1}^{B} \\frac{|B_b|}{N} \\cdot |accuracy_b - confidence_b|$$\n",
    "\n",
    "- $B$ = number of groups (bins)\n",
    "- $B_b$ = set of predictions in bin $b$\n",
    "- $|B_b|/N$ = proportion of predictions in this bin (weight)\n",
    "- $|accuracy_b - confidence_b|$ = gap in this bin\n",
    "\n",
    "### How to read the result\n",
    "\n",
    "- **ECE = 0** → perfect calibration (each bin has accuracy = confidence)\n",
    "- **ECE = 0.05** → good (5% average gap)\n",
    "- **ECE = 0.20** → poor (20% gap)\n",
    "\n",
    "**PRISM target**: ECE < 0.15\n",
    "\n",
    "### The number of bins\n",
    "\n",
    "The choice of $B$ (number of bins) is a trade-off:\n",
    "- **Few bins** (3-5): each bin contains a lot of data → stable estimates, but resolution is lost\n",
    "- **Many bins** (15-20): fine resolution, but some bins may be nearly empty → noisy estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec2-ece-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(confidences, accuracies, n_bins=10):\n",
    "    \"\"\"Calcule l'ECE et les données du reliability diagram.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_confs = []\n",
    "    bin_accs = []\n",
    "    bin_counts = []\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n",
    "        mask = (confidences >= lo) & (confidences < hi)\n",
    "        if i == n_bins - 1:  # dernier bin inclut la borne sup\n",
    "            mask = (confidences >= lo) & (confidences <= hi)\n",
    "\n",
    "        count = mask.sum()\n",
    "        if count > 0:\n",
    "            bin_confs.append(confidences[mask].mean())\n",
    "            bin_accs.append(accuracies[mask].mean())\n",
    "            bin_counts.append(count)\n",
    "        else:\n",
    "            bin_confs.append(np.nan)\n",
    "            bin_accs.append(np.nan)\n",
    "            bin_counts.append(0)\n",
    "\n",
    "    bin_confs = np.array(bin_confs)\n",
    "    bin_accs = np.array(bin_accs)\n",
    "    bin_counts = np.array(bin_counts)\n",
    "\n",
    "    # ECE\n",
    "    valid = bin_counts > 0\n",
    "    n_total = bin_counts.sum()\n",
    "    ece = np.sum((bin_counts[valid] / n_total) * np.abs(bin_accs[valid] - bin_confs[valid]))\n",
    "\n",
    "    return ece, bin_confs, bin_accs, bin_counts\n",
    "\n",
    "# Test\n",
    "ece, _, _, _ = compute_ece(conf_good, acc_good)\n",
    "print(f\"ECE (bien calibr\\u00e9) : {ece:.4f}\")\n",
    "\n",
    "ece_over, _, _, _ = compute_ece(conf_over, acc_over)\n",
    "print(f\"ECE (sur-confiant) : {ece_over:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec2-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ece_interactive(n_bins, calibration_type):\n",
    "    \"\"\"Visualise l'ECE avec différents nombres de bins.\"\"\"\n",
    "    data = {\n",
    "        'Bien calibré': (conf_good, acc_good),\n",
    "        'Sur-confiant': (conf_over, acc_over),\n",
    "        'Sous-confiant': (conf_under, acc_under),\n",
    "    }\n",
    "    conf, acc = data[calibration_type]\n",
    "    ece, bin_confs, bin_accs, bin_counts = compute_ece(conf, acc, n_bins)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5))\n",
    "\n",
    "    # Reliability diagram (points)\n",
    "    bin_centers = np.linspace(1/(2*n_bins), 1 - 1/(2*n_bins), n_bins)\n",
    "    valid = bin_counts > 0\n",
    "\n",
    "    axes[0].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Parfait')\n",
    "    axes[0].scatter(bin_centers[valid], bin_accs[valid], s=60,\n",
    "                    c='steelblue', edgecolors='white', zorder=3, label='Accuracy')\n",
    "    axes[0].plot(bin_centers[valid], bin_accs[valid], '-', color='steelblue', alpha=0.5)\n",
    "    axes[0].set_xlabel('Confiance (bin moyen)')\n",
    "    axes[0].set_ylabel('Accuracy (bin moyen)')\n",
    "    axes[0].set_title(f'Reliability Diagram — ECE = {ece:.4f}')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Distribution par bin (barres — c'est un comptage)\n",
    "    width = 1.0 / n_bins\n",
    "    axes[1].bar(bin_centers, bin_counts, width=width*0.8,\n",
    "                color='lightcoral', edgecolor='white')\n",
    "    axes[1].set_xlabel('Confiance (bin)')\n",
    "    axes[1].set_ylabel('Nombre de prédictions')\n",
    "    axes[1].set_title(f'Distribution ({n_bins} bins, N={int(bin_counts.sum())})')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Détail par bin\n",
    "    delta_header = '|Δ|'\n",
    "    print(f\"{'Bin':>6} {'Conf':>8} {'Acc':>8} {'Count':>8} {delta_header:>8}\")\n",
    "    for i in range(n_bins):\n",
    "        if bin_counts[i] > 0:\n",
    "            gap = abs(bin_accs[i] - bin_confs[i])\n",
    "            print(f\"{i+1:>6} {bin_confs[i]:>8.3f} {bin_accs[i]:>8.3f} {int(bin_counts[i]):>8} {gap:>8.3f}\")\n",
    "\n",
    "    print()\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — reliability diagram :\")\n",
    "    print(\"  Chaque point = un groupe de prédictions avec des confiances similaires\")\n",
    "    print(\"  Position verticale = accuracy moyenne du groupe\")\n",
    "    print(\"  Diagonale rouge = calibration parfaite\")\n",
    "    print(\"  ECE = distance moyenne pondérée entre les points et la diagonale\")\n",
    "    print()\n",
    "    print(\"Droite — distribution des données par bin :\")\n",
    "    print(\"  Montre combien de prédictions tombent dans chaque bin\")\n",
    "    print(\"  Un bin avec peu de données → point peu fiable\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  Bins = 3  → peu de points, estimation stable mais grossière\")\n",
    "    print(\"  Bins = 20 → beaucoup de points, mais certains bins vides\")\n",
    "    print(\"  Changer le type de calibration pour voir comment la courbe change\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_ece_interactive,\n",
    "    n_bins=widgets.IntSlider(value=10, min=3, max=20, step=1,\n",
    "                              description='Bins', continuous_update=False),\n",
    "    calibration_type=widgets.Dropdown(\n",
    "        options=['Bien calibré', 'Sur-confiant', 'Sous-confiant'],\n",
    "        value='Bien calibré', description='Type')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Reliability Diagram: step-by-step construction on the SR\n",
    "\n",
    "### What is a reliability diagram?\n",
    "\n",
    "It is the standard visualization for calibration: we check whether the agent's confidence matches its true performance. We already saw it with the weather data (Section 1) and the ECE (Section 2).\n",
    "\n",
    "Here, we will **build it from scratch** on our ToyGrid, following the complete chain:\n",
    "\n",
    "### The 3 steps (= the 3 grids)\n",
    "\n",
    "| Step | What we compute | Grid |\n",
    "|------|-----------------|------|\n",
    "| **1. True error** | For each cell: $\\|M(s,:) - M^*(s,:)\\|$ | Left — where M is wrong |\n",
    "| **2. Ground truth** | We set a threshold $\\tau$ (median): correct if error < $\\tau$ | Center — green/red grid |\n",
    "| **3. What the agent believes** | The confidence $C(s)$ computed via $U \\to C$ | Right — green/red grid |\n",
    "\n",
    "**Compare center and right**: are the green zones in the same places? If so → the agent is well calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec3-step-by-step",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction pas-à-pas sur les données SR (ToyGrid)\n",
    "grid = ToyGrid.two_rooms()\n",
    "gamma = 0.95\n",
    "M_star = grid.true_sr(gamma)\n",
    "\n",
    "# Agent partiellement entraîné (1000 steps, comme le notebook principal)\n",
    "M = np.eye(grid.n_states)\n",
    "traj = grid.random_walk(1000, seed=42)\n",
    "\n",
    "# On accumule les erreurs seulement sur les derniers 500 pas\n",
    "warmup = 500\n",
    "error_sum = np.zeros(grid.n_states)\n",
    "error_count = np.zeros(grid.n_states)\n",
    "recent_norms = []\n",
    "\n",
    "for t in range(len(traj) - 1):\n",
    "    s, s_next = traj[t], traj[t+1]\n",
    "    delta, M = grid.td_update(M, s, s_next, gamma, 0.1)\n",
    "    if t >= warmup:\n",
    "        norm = np.linalg.norm(delta)\n",
    "        error_sum[s] += norm\n",
    "        error_count[s] += 1\n",
    "        recent_norms.append(norm)\n",
    "\n",
    "# --- Étape 1 : erreur réelle par état ---\n",
    "errors = np.array([np.linalg.norm(M[s] - M_star[s]) for s in range(grid.n_states)])\n",
    "# Min-max : la plus petite erreur → 0 (jaune), la plus grande → 1 (rouge)\n",
    "e_min, e_max = errors.min(), errors.max()\n",
    "errors_norm = (errors - e_min) / (e_max - e_min + 1e-8)\n",
    "\n",
    "# --- Étape 2 : accuracy binaire (seuil = médiane) ---\n",
    "tau = np.median(errors)\n",
    "accuracy = (errors < tau).astype(float)\n",
    "\n",
    "# --- Étape 3 : U et confiance C ---\n",
    "p99 = np.percentile(recent_norms, 99) if recent_norms else 1.0\n",
    "mean_err = np.zeros(grid.n_states)\n",
    "visited = error_count > 0\n",
    "mean_err[visited] = error_sum[visited] / error_count[visited]\n",
    "U = np.where(visited, np.clip(mean_err / max(p99, 1e-8), 0, 1), 0.8)\n",
    "\n",
    "# On centre la sigmoïde sur la médiane de U pour avoir du contraste\n",
    "theta_C = np.median(U)\n",
    "C = 1 / (1 + np.exp(8 * (U - theta_C)))\n",
    "\n",
    "# === Visualisation : 3 grilles ===\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "grid.plot(values=errors_norm, ax=axes[0], show_goal=False,\n",
    "          title='Étape 1 : erreur de M', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "grid.plot(values=accuracy, ax=axes[1], show_goal=False,\n",
    "          title='Étape 2 : la réalité',\n",
    "          cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "grid.plot(values=C, ax=axes[2], show_goal=False,\n",
    "          title='Étape 3 : ce que l\\'agent croit',\n",
    "          cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Explications ---\n",
    "print(\"Lecture des 3 grilles :\")\n",
    "print()\n",
    "print(\"GAUCHE — erreur réelle de M par case\")\n",
    "print(\"  Rouge foncé = M est très faux, jaune pâle = M est correct\")\n",
    "print()\n",
    "print(\"CENTRE — la réalité (accuracy)\")\n",
    "print(f\"  Seuil τ = médiane des erreurs = {tau:.3f}\")\n",
    "print(f\"  Vert = M correct ({int(accuracy.sum())} états), rouge = incorrect ({int((1 - accuracy).sum())} états)\")\n",
    "print()\n",
    "print(\"DROITE — ce que l'agent croit (confiance C)\")\n",
    "print(f\"  Sigmoïde centrée sur médiane(U) = {theta_C:.2f}\")\n",
    "print(\"  Vert = confiant (U bas), rouge = doute (U haut)\")\n",
    "print()\n",
    "print(\"Comparer CENTRE et DROITE :\")\n",
    "agree = ((accuracy == 1) & (C >= 0.5)) | ((accuracy == 0) & (C < 0.5))\n",
    "n_agree = int(agree.sum())\n",
    "print(f\"  Concordance : {n_agree}/{grid.n_states} ({100*n_agree/grid.n_states:.0f}%)\")\n",
    "print()\n",
    "if n_agree > grid.n_states * 0.7:\n",
    "    print(\"  → Bonne calibration : les zones vertes/rouges se ressemblent\")\n",
    "else:\n",
    "    print(\"  → Calibration imparfaite : les deux grilles ne se ressemblent pas assez\")\n",
    "print()\n",
    "print(\"  Même case verte des 2 côtés → l'agent sait ce qu'il sait ✓\")\n",
    "print(\"  Rouge au centre, verte à droite → SUR-CONFIANT\")\n",
    "print(\"  Verte au centre, rouge à droite → SOUS-CONFIANT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Metacognitive Index (MI)\n",
    "\n",
    "### The question\n",
    "\n",
    "The ECE measures whether overall confidence is well calibrated. But there is a finer question:\n",
    "\n",
    "> Does the agent know **where** it is wrong?\n",
    "\n",
    "Even with a correct ECE, the agent could have uniform confidence everywhere (e.g., $C = 0.5$ on all cells). That would be technically calibrated, but useless — it does not distinguish well-learned zones from poorly known zones.\n",
    "\n",
    "### The idea\n",
    "\n",
    "The **Metacognitive Index** (MI) measures whether the uncertainty $U(s)$ is **correlated** with the true SR errors.\n",
    "\n",
    "$$MI = \\rho_{Spearman}\\left(U(s),\\; \\|M(s,:) - M^*(s,:)\\|_2\\right)$$\n",
    "\n",
    "- We take the 21 states and their $U$ values\n",
    "- We take the 21 true errors $\\|M(s,:) - M^*(s,:)\\|$\n",
    "- We compute the rank correlation ($\\rho_{Spearman}$)\n",
    "\n",
    "### Why Spearman (rank) rather than Pearson?\n",
    "\n",
    "The Spearman correlation does not measure whether $U$ is proportional to the error, but whether **the ordering is the same**: are the cells with the largest $U$ also those with the largest error? This is more robust to extreme values.\n",
    "\n",
    "### How to read the result\n",
    "\n",
    "- **MI > 0.5** → the agent knows well where it is wrong (PRISM target)\n",
    "- **MI ≈ 0.3** → partial signal, but noisy\n",
    "- **MI ≈ 0** → $U$ does not reflect the true errors — the metacognitive signal is useless\n",
    "\n",
    "### What the widget shows\n",
    "\n",
    "We can add **noise** to $U$ to see how the MI degrades. Without noise, $U$ perfectly reflects the errors → high MI. With a lot of noise, $U$ becomes random → MI drops to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec4-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_interactive(noise_level):\n",
    "    \"\"\"Visualise l'effet du bruit sur le MI.\"\"\"\n",
    "    # Erreurs réelles\n",
    "    true_errors = errors.copy()\n",
    "\n",
    "    # U = erreurs + bruit\n",
    "    noise = np.random.RandomState(42).randn(grid.n_states) * noise_level\n",
    "    U_noisy = np.clip(true_errors / max(np.percentile(true_errors, 99), 1e-8) + noise, 0, 1)\n",
    "\n",
    "    # MI\n",
    "    rho, p_value = spearmanr(U_noisy, true_errors)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "    # Scatter U vs erreur\n",
    "    axes[0].scatter(true_errors, U_noisy, alpha=0.6, s=30, c='steelblue')\n",
    "    axes[0].set_xlabel('Erreur réelle ||M(s)-M*(s)||')\n",
    "    axes[0].set_ylabel('U(s) (avec bruit)')\n",
    "    axes[0].set_title(f'MI = {rho:.3f} (p = {p_value:.4f})')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Heatmap erreur réelle (min-max pour gradient complet)\n",
    "    te_min, te_max = true_errors.min(), true_errors.max()\n",
    "    err_norm = (true_errors - te_min) / (te_max - te_min + 1e-8)\n",
    "    grid.plot(values=err_norm, ax=axes[1], show_goal=False,\n",
    "              title='Erreur réelle', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "    # Heatmap U bruité\n",
    "    grid.plot(values=U_noisy, ax=axes[2], show_goal=False,\n",
    "              title=f'U(s) (bruit={noise_level:.1f})', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"Gauche — chaque point = un état :\")\n",
    "    print(\"  Axe X = erreur réelle de M, axe Y = incertitude U estimée par l'agent\")\n",
    "    print(\"  Si les points suivent une diagonale → U reflète bien les vraies erreurs (MI élevé)\")\n",
    "    print(\"  Si les points sont dispersés → U est du bruit (MI faible)\")\n",
    "    print()\n",
    "    print(\"Centre — erreur réelle sur la grille :\")\n",
    "    print(\"  Rouge = M est faux ici, jaune pâle = M est correct\")\n",
    "    print()\n",
    "    print(\"Droite — U estimé par l'agent :\")\n",
    "    print(\"  Doit ressembler au centre si l'agent sait ce qu'il ne sait pas\")\n",
    "    print()\n",
    "\n",
    "    if rho > 0.5:\n",
    "        verdict = f\"MI = {rho:.3f} → Forte corrélation : l'agent sait ce qu'il ne sait pas\"\n",
    "    elif rho > 0:\n",
    "        verdict = f\"MI = {rho:.3f} → Corrélation faible : signal métacognitif partiel\"\n",
    "    else:\n",
    "        verdict = f\"MI = {rho:.3f} → Pas de corrélation : le signal U est du bruit\"\n",
    "    print(verdict)\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  Bruit = 0   → MI maximal (U = erreur réelle, les deux cartes sont identiques)\")\n",
    "    print(\"  Bruit = 0.5 → MI diminue (les deux cartes divergent)\")\n",
    "    print(\"  Bruit = 1.0 → MI ≈ 0 (U est aléatoire, plus aucun lien avec l'erreur)\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_mi_interactive,\n",
    "    noise_level=widgets.FloatSlider(value=0.0, min=0.0, max=1.0, step=0.1,\n",
    "                                     description='Bruit', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Defining accuracy in the SR context\n",
    "\n",
    "### The problem\n",
    "\n",
    "To build a reliability diagram, we need a **binary accuracy** per state: \"is the agent's prediction correct at $s$, yes or no?\"\n",
    "\n",
    "But the SR is not a classifier — there is no obvious binary \"correct answer\". The error $\\|M(s,:) - M^*(s,:)\\|$ is a continuous number. How do we turn it into \"correct / incorrect\"?\n",
    "\n",
    "### The method: median threshold\n",
    "\n",
    "1. Compute the true error for each state: $error(s) = \\|M(s,:) - M^*(s,:)\\|$\n",
    "2. Take the **median** of all errors as the threshold $\\tau$\n",
    "3. Define: $accuracy(s) = 1$ if $error(s) < \\tau$, otherwise $0$\n",
    "\n",
    "### Why the median?\n",
    "\n",
    "- It guarantees ~50% of states are \"correct\" and ~50% \"incorrect\" → the reliability diagram has enough data on both sides\n",
    "- A threshold that is too low (e.g., 10th percentile) would make almost everything \"incorrect\" → the diagram would be trivial\n",
    "- A threshold that is too high (e.g., 90th percentile) would make almost everything \"correct\" → same problem\n",
    "\n",
    "The widget below shows how the choice of percentile affects the diagram.\n",
    "\n",
    "### Important note\n",
    "\n",
    "The MI (Section 4) uses rank correlation, which does **not** depend on the threshold. This is an advantage: the MI is robust to the choice of $\\tau$, unlike the ECE which depends on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec5-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_effect(quantile):\n",
    "    \"\"\"Montre l'effet du choix de seuil sur la comparaison réalité vs croyance.\"\"\"\n",
    "    tau_q = np.percentile(errors, quantile)\n",
    "    acc = (errors < tau_q).astype(float)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "    # Gauche : distribution des erreurs + seuil\n",
    "    axes[0].hist(errors, bins=15, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "    axes[0].axvline(x=tau_q, color='red', linewidth=2, linestyle='--',\n",
    "                    label=f'seuil = {tau_q:.3f}')\n",
    "    axes[0].set_xlabel('Erreur ||M(s)-M*(s)||')\n",
    "    axes[0].set_ylabel(\"Nombre d'états\")\n",
    "    axes[0].set_title(f'Distribution (percentile {quantile})')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Centre : la réalité (accuracy) — CHANGE avec le slider\n",
    "    grid.plot(values=acc, ax=axes[1], show_goal=False,\n",
    "              title=f'La réalité ({acc.mean()*100:.0f}% correct)',\n",
    "              cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    # Droite : ce que l'agent croit (C) — NE CHANGE PAS\n",
    "    grid.plot(values=C, ax=axes[2], show_goal=False,\n",
    "              title='Ce que l\\'agent croit (fixe)',\n",
    "              cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Concordance\n",
    "    agree = ((acc == 1) & (C >= 0.5)) | ((acc == 0) & (C < 0.5))\n",
    "    n_agree = int(agree.sum())\n",
    "    n_disagree = grid.n_states - n_agree\n",
    "\n",
    "    print(\"Lecture des graphes :\")\n",
    "    print()\n",
    "    print(\"GAUCHE — distribution des erreurs de M :\")\n",
    "    print(f\"  Ligne rouge = seuil τ (percentile {quantile} = {tau_q:.3f})\")\n",
    "    print(\"  Les états à gauche du seuil sont 'corrects', ceux à droite 'incorrects'\")\n",
    "    print()\n",
    "    print(\"CENTRE — la réalité (change avec le slider) :\")\n",
    "    print(f\"  Vert = correct ({int(acc.sum())} états), rouge = incorrect ({int((1-acc).sum())} états)\")\n",
    "    print()\n",
    "    print(\"DROITE — ce que l'agent croit (fixe, ne change pas) :\")\n",
    "    print(\"  Vert = confiant, rouge = pas confiant\")\n",
    "    print()\n",
    "    print(f\"Concordance centre ↔ droite : {n_agree}/{grid.n_states} ({100*n_agree/grid.n_states:.0f}%)\")\n",
    "    print()\n",
    "    print(\"À essayer :\")\n",
    "    print(\"  Bougez le slider : seul le CENTRE change, la DROITE reste fixe.\")\n",
    "    print(\"  Percentile 50 → ~50/50, bon équilibre pour juger la calibration\")\n",
    "    print(\"  Percentile 80 → presque tout vert au centre, concordance artificielle\")\n",
    "    print(\"  Percentile 20 → presque tout rouge au centre, concordance faible\")\n",
    "    print(\"  → Le MI (Section 4) ne dépend pas de ce choix, c'est son avantage\")\n",
    "\n",
    "widgets.interact(\n",
    "    plot_threshold_effect,\n",
    "    quantile=widgets.IntSlider(value=50, min=10, max=90, step=5,\n",
    "                                description='Percentile τ', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Hybrid demo: calibration on real data\n",
    "\n",
    "Uses PRISM code to compute ECE and MI on a real FourRooms agent.\n",
    "\n",
    "**Requires**: `pip install -e .` + MiniGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sec6-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import minigrid\n",
    "    import gymnasium as gym\n",
    "    from prism.agent.prism_agent import PRISMAgent\n",
    "    from prism.env.state_mapper import StateMapper\n",
    "    from prism.env.dynamics_wrapper import DynamicsWrapper\n",
    "    from prism.analysis.calibration import (\n",
    "        sr_errors, sr_accuracies,\n",
    "        expected_calibration_error,\n",
    "        metacognitive_index,\n",
    "        reliability_diagram_data,\n",
    "        plot_reliability_diagram\n",
    "    )\n",
    "\n",
    "    # Setup\n",
    "    env = DynamicsWrapper(gym.make(\"MiniGrid-FourRooms-v0\", max_steps=500))\n",
    "    agent = PRISMAgent(env)\n",
    "\n",
    "    # Entra\\u00eener 200 \\u00e9pisodes\n",
    "    for ep in range(200):\n",
    "        agent.train_episode()\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            print(f\"Episode {ep+1}/200\")\n",
    "\n",
    "    # Calculer M*\n",
    "    T = env.compute_transition_matrix(agent.mapper)\n",
    "    M_star = np.linalg.inv(np.eye(agent.mapper.n_states) - 0.95 * T)\n",
    "\n",
    "    # M\\u00e9triques\n",
    "    errors = sr_errors(agent.sr.M, M_star)\n",
    "    accuracies = sr_accuracies(errors)\n",
    "    confidences = agent.meta.all_confidences()\n",
    "    uncertainties = agent.meta.all_uncertainties()\n",
    "\n",
    "    ece = expected_calibration_error(confidences, accuracies)\n",
    "    rho, p_val = metacognitive_index(uncertainties, agent.sr.M, M_star)\n",
    "\n",
    "    print(f\"\\nECE = {ece:.4f} (cible < 0.15)\")\n",
    "    print(f\"MI = {rho:.4f} (p = {p_val:.4f}, cible > 0.5)\")\n",
    "\n",
    "    # Reliability diagram\n",
    "    fig = plot_reliability_diagram(confidences, accuracies, label='PRISM')\n",
    "    plt.show()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"D\\u00e9pendance manquante : {e}\")\n",
    "    print(\"Pour ex\\u00e9cuter cette cellule : pip install minigrid gymnasium\")\n",
    "    print(\"Puis : cd PRISM && pip install -e .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Metric | Formula | Interpretation | Target |\n",
    "|--------|---------|----------------|--------|\n",
    "| ECE | $\\sum_b \\frac{|B_b|}{N} |acc_b - conf_b|$ | Mean confidence/accuracy gap | < 0.15 |\n",
    "| MI | $\\rho_{Spearman}(U, error)$ | The agent knows where it is wrong | > 0.5 |\n",
    "| Accuracy | $\\mathbb{1}(\\|M-M^*\\| < \\tau)$ | Binary \"correct\" prediction | ~50% baseline |\n",
    "\n",
    "$\\leftarrow$ [Back to the main notebook](00_prism_concepts.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
