# PRISM â€” Checkpoints humains

> Quand regarder, quoi regarder, quoi lancer, quand s'inquiÃ©ter.
> Tout le reste, laisse Claude tourner.

---

## Vue d'ensemble

```
Semaine  1 â”€â”€â”€â”€ 2 â”€â”€â”€â”€ 3 â”€â”€â”€â”€ 4 â”€â”€â”€â”€ 5 â”€â”€â”€â”€ 6 â”€â”€â”€â”€ 7 â”€â”€â”€â”€ 8
         â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Phase 1  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
         â”‚    CP1      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Phase 2  â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â”‚      â”‚      â”‚      â”‚
         â”‚      â”‚      â”‚  CP2 â”‚ CP3  â”‚      â”‚      â”‚      â”‚
Phase 3  â”‚      â”‚      â”‚      â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â”‚
         â”‚      â”‚      â”‚      â”‚      â”‚    CP4 â”‚  CP5â”‚      â”‚
                                                     â””â”€ RÃ©daction

CP = Checkpoint humain
Temps humain total estimÃ© : ~3 jours sur 8 semaines
```

---

## Checkpoint 1 â€” Sanity check SR (fin semaine 2)

> **Statut :** EXÃ‰CUTABLE â€” le script ci-dessous fonctionne avec le code actuel. Alternative : le notebook `notebooks/01_sr_validation.ipynb` couvre les mÃªmes vÃ©rifications + calibration.

**DurÃ©e :** 30 min â€“ 1h
**PrÃ©requis :** Phase 1 terminÃ©e, `pytest tests/ -v` passe au vert.

### Ce que Claude a fait

- MiniGrid FourRooms tourne
- `state_mapper.py` convertit positions â†” indices
- `sr_layer.py` apprend M par TD(0)
- `dynamics_wrapper.py` peut perturber l'env
- `spectral.py` calcule les eigenvectors de M

### Ce que tu vÃ©rifies

**1. La matrice M a-t-elle convergÃ© ?**

```python
# checkpoints/cp1_sr_convergence.py
"""
Lance 300 Ã©pisodes d'apprentissage SR dans FourRooms.
Affiche : convergence de M, heatmaps, eigenvectors.
Temps d'exÃ©cution : ~2 min.
"""
import numpy as np
import matplotlib.pyplot as plt
from prism.env.dynamics_wrapper import DynamicsWrapper
from prism.env.state_mapper import StateMapper
from prism.agent.sr_layer import SRLayer
from prism.analysis.spectral import sr_eigenvectors
import minigrid  # noqa: F401
import gymnasium as gym
import os

os.makedirs("results", exist_ok=True)

# --- Setup ---
env = gym.make("MiniGrid-FourRooms-v0", max_steps=500)
mapper = StateMapper(env)
n_states = mapper.n_states
agent = SRLayer(n_states, gamma=0.95, alpha_M=0.1, alpha_R=0.3)

# --- Train ---
M_snapshots = []
for ep in range(300):
    obs, _ = env.reset()
    s = mapper.get_index(env.agent_pos)
    done = False
    while not done:
        action = env.action_space.sample()  # random policy pour apprentissage latent
        obs, reward, terminated, truncated, _ = env.step(action)
        s_next = mapper.get_index(env.agent_pos)
        agent.update(s, s_next, reward)
        s = s_next
        done = terminated or truncated
    if ep % 50 == 0:
        M_snapshots.append((ep, agent.M.copy()))

# --- Figure 1 : Convergence ---
fig, axes = plt.subplots(1, len(M_snapshots), figsize=(4*len(M_snapshots), 4))
for i, (ep, M) in enumerate(M_snapshots):
    axes[i].imshow(M, cmap="viridis", aspect="auto")
    axes[i].set_title(f"M @ Ã©pisode {ep}")
    axes[i].set_xlabel("Ã©tat s'")
    if i == 0:
        axes[i].set_ylabel("Ã©tat s")
plt.suptitle("CP1 â€” Convergence de la matrice SR", fontsize=14)
plt.tight_layout()
plt.savefig("results/cp1_convergence.png", dpi=150)
plt.show()

# --- Figure 2 : Heatmap M pour quelques Ã©tats sources ---
fig, axes = plt.subplots(2, 3, figsize=(14, 9))
source_states = [0, n_states//4, n_states//2, 3*n_states//4, n_states-1, n_states//3]
grid_shape = mapper.get_grid_shape()  # (rows, cols)

for ax, src in zip(axes.flat, source_states[:6]):
    row_vec = agent.M[src, :]
    heatmap = mapper.to_grid(row_vec)
    im = ax.imshow(heatmap, cmap="hot", interpolation="nearest")
    src_pos = mapper.get_pos(src)
    ax.plot(src_pos[1], src_pos[0], 'c*', markersize=15)
    ax.set_title(f"M({src}, :)")
    plt.colorbar(im, ax=ax, fraction=0.046)

plt.suptitle("CP1 â€” Successor representation depuis diffÃ©rents Ã©tats", fontsize=14)
plt.tight_layout()
plt.savefig("results/cp1_heatmaps.png", dpi=150)
plt.show()

# --- Figure 3 : Top-6 eigenvectors ---
eigenvalues, eigenvectors = sr_eigenvectors(agent.M, k=6)

fig, axes = plt.subplots(2, 3, figsize=(14, 9))
for i, ax in enumerate(axes.flat):
    grid = mapper.to_grid(eigenvectors[:, i])
    im = ax.imshow(grid, cmap="RdBu_r", interpolation="nearest")
    ax.set_title(f"Eigenvector {i+1} (Î»={eigenvalues[i]:.3f})")
    plt.colorbar(im, ax=ax, fraction=0.046)

plt.suptitle("CP1 â€” DÃ©composition spectrale de M (â‰ˆ grid cells)", fontsize=14)
plt.tight_layout()
plt.savefig("results/cp1_eigenvectors.png", dpi=150)
plt.show()

# --- MÃ©triques numÃ©riques ---
print("\n" + "="*60)
print("CP1 â€” MÃ‰TRIQUES DE CONVERGENCE")
print("="*60)
M_diff = np.linalg.norm(M_snapshots[-1][1] - M_snapshots[-2][1])
print(f"  ||M_300 - M_250|| = {M_diff:.6f}  (< 0.1 = convergÃ©)")
print(f"  Rang effectif de M : {np.linalg.matrix_rank(agent.M, tol=0.01)}")
print(f"  Nombre d'Ã©tats : {n_states}")
print(f"  Top-3 eigenvalues : {eigenvalues[:3]}")
diag = np.diag(agent.M)
print(f"  Diagonale M â€” min: {diag.min():.3f}, max: {diag.max():.3f}, mean: {diag.mean():.3f}")
print(f"  (diag devrait Ãªtre > 0 partout â€” c'est P(rester/revenir))")
```

### Ce que tu cherches visuellement

**Heatmaps de M :** depuis un Ã©tat source, la chaleur doit se diffuser vers les Ã©tats accessibles et dÃ©croÃ®tre avec la distance. Les murs doivent bloquer la diffusion. Si la chaleur "passe Ã  travers" les murs â†’ bug dans le state mapper ou le wrapper.

**Eigenvectors :** les 6 premiers doivent montrer des patterns spatiaux cohÃ©rents â€” basses frÃ©quences spatiales, sÃ©paration entre piÃ¨ces. Compare mentalement avec la Figure 3 de Stachenfeld 2017. Pas besoin d'un match parfait, mais la structure doit Ãªtre lÃ . Si Ã§a ressemble Ã  du bruit â†’ M n'a pas convergÃ© ou le discount Î³ est trop bas.

### Feu vert / Feu rouge

| Signal | âœ… Go | âŒ Stop |
|--------|------|--------|
| `â€–Mâ‚ƒâ‚€â‚€ - Mâ‚‚â‚…â‚€â€–` | < 0.1 | > 1.0 |
| Heatmaps | Diffusion bloquÃ©e par murs | Chaleur traverse les murs |
| Eigenvectors | Patterns spatiaux lisses | Bruit ou uniformes |
| Diagonale M | > 0 partout | ZÃ©ros ou nÃ©gatifs |

### Si Ã§a ne passe pas

- Heatmaps traversent les murs â†’ vÃ©rifier `state_mapper.py`, les murs ne sont peut-Ãªtre pas encodÃ©s comme Ã©tats inaccessibles
- M ne converge pas â†’ augmenter le nombre d'Ã©pisodes Ã  500, ou baisser Î±_M Ã  0.05
- Eigenvectors bruiteux â†’ augmenter Î³ vers 0.98 (horizon trop court)

---

## Checkpoint 2 â€” Sweep hyperparamÃ¨tres mÃ©ta-SR (milieu semaine 4)

> **Statut :** TEMPLATE â€” le sweep sera lancÃ© en Phase 3. Le script ci-dessous sera exÃ©cutable une fois `experiments/exp_a_calibration.py` implÃ©mentÃ©.

**DurÃ©e :** 1-2h
**PrÃ©requis :** `meta_sr.py` et `calibration.py` fonctionnels.

### Ce que Claude a fait

- 81 configurations testÃ©es (4 params Ã— 3 valeurs Ã— 10 runs)
- Tableau triÃ© par ECE sur la phase d'apprentissage stable
- Fichier CSV complet dans `results/sweep/`

### Ce que tu vÃ©rifies

```python
# checkpoints/cp2_sweep_review.py
"""
Charge les rÃ©sultats du sweep et affiche les diagnostics.
Temps d'exÃ©cution : ~10 sec (analyse uniquement, pas de simulation).
"""
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# --- Charger les rÃ©sultats ---
df = pd.read_csv("results/sweep/sweep_results.csv")
# Colonnes attendues : U_prior, decay, beta, theta_C, run, ECE, MI

print("="*60)
print("CP2 â€” RÃ‰SULTATS DU SWEEP HYPERPARAMÃˆTRES")
print("="*60)

# --- Top 10 configs par ECE mÃ©dian ---
summary = df.groupby(["U_prior", "decay", "beta", "theta_C"]).agg(
    ECE_median=("ECE", "median"),
    ECE_std=("ECE", "std"),
    MI_median=("MI", "median"),
    MI_std=("MI", "std"),
).reset_index().sort_values("ECE_median")

print("\nTop 10 configs (ECE mÃ©dian le plus bas) :")
print(summary.head(10).to_string(index=False))

best = summary.iloc[0]
print(f"\nâ˜… Meilleure config :")
print(f"  U_prior = {best.U_prior}")
print(f"  decay   = {best.decay}")
print(f"  Î²       = {best.beta}")
print(f"  Î¸_C     = {best.theta_C}")
print(f"  ECE     = {best.ECE_median:.4f} Â± {best.ECE_std:.4f}")
print(f"  MI      = {best.MI_median:.4f} Â± {best.MI_std:.4f}")

# --- ALERTES DE SENS PHYSIQUE ---
print("\n" + "-"*60)
print("ALERTES DE SENS PHYSIQUE :")
alerts = []
if best.beta > 30:
    alerts.append(f"âš ï¸  Î² = {best.beta} â†’ sigmoÃ¯de quasi-binaire, pas de zone de transition graduelle")
if best.beta < 3:
    alerts.append(f"âš ï¸  Î² = {best.beta} â†’ sigmoÃ¯de trop plate, confiance presque uniforme")
if best.decay > 0.97:
    alerts.append(f"âš ï¸  decay = {best.decay} â†’ le prior met ~100 visites Ã  disparaÃ®tre (trop lent)")
if best.decay < 0.5:
    alerts.append(f"âš ï¸  decay = {best.decay} â†’ le prior disparaÃ®t en ~2 visites (trop rapide)")
if best.U_prior < 0.3:
    alerts.append(f"âš ï¸  U_prior = {best.U_prior} â†’ Ã©tats non visitÃ©s considÃ©rÃ©s peu incertains (dangereux)")
if best.theta_C > 0.7:
    alerts.append(f"âš ï¸  Î¸_C = {best.theta_C} â†’ presque tout est classÃ© 'haute confiance'")

if alerts:
    for a in alerts:
        print(f"  {a}")
    print("  â†’ ConsidÃ¨re la 2e ou 3e meilleure config si les alertes sont sÃ©rieuses")
else:
    print("  âœ… Aucune alerte â€” les paramÃ¨tres sont dans des plages raisonnables")

# --- Figure 1 : SensibilitÃ© par paramÃ¨tre ---
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
params = ["U_prior", "decay", "beta", "theta_C"]
for ax, param in zip(axes.flat, params):
    param_summary = df.groupby(param)["ECE"].agg(["median", "std"]).reset_index()
    ax.errorbar(param_summary[param], param_summary["median"], 
                yerr=param_summary["std"], marker="o", capsize=4, color="#2E75B6")
    ax.set_xlabel(param, fontsize=12)
    ax.set_ylabel("ECE", fontsize=12)
    ax.set_title(f"SensibilitÃ© Ã  {param}")
    ax.axhline(y=0.15, color="red", linestyle="--", alpha=0.5, label="seuil 0.15")
    ax.legend()

plt.suptitle("CP2 â€” SensibilitÃ© de l'ECE aux hyperparamÃ¨tres mÃ©ta-SR", fontsize=14)
plt.tight_layout()
plt.savefig("results/cp2_sensitivity.png", dpi=150)
plt.show()

# --- Figure 2 : ECE vs MI pour toutes les configs ---
plt.figure(figsize=(8, 6))
scatter = plt.scatter(summary.ECE_median, summary.MI_median, 
                      c=summary.beta, cmap="viridis", s=60, alpha=0.7)
plt.colorbar(scatter, label="Î²")
plt.axvline(x=0.15, color="red", linestyle="--", alpha=0.5, label="ECE cible < 0.15")
plt.axhline(y=0.5, color="green", linestyle="--", alpha=0.5, label="MI cible > 0.5")
plt.xlabel("ECE mÃ©dian (â†“ meilleur)")
plt.ylabel("MI mÃ©dian (â†‘ meilleur)")
plt.title("CP2 â€” Espace ECE Ã— MI â€” chaque point = une config")
plt.legend()
plt.tight_layout()
plt.savefig("results/cp2_ece_vs_mi.png", dpi=150)
plt.show()

# --- StabilitÃ© : la meilleure config est-elle robuste ? ---
best_runs = df[
    (df.U_prior == best.U_prior) & (df.decay == best.decay) &
    (df.beta == best.beta) & (df.theta_C == best.theta_C)
]
print(f"\nStabilitÃ© de la meilleure config (10 runs) :")
print(f"  ECE : {best_runs.ECE.values}")
print(f"  Coefficient de variation ECE : {best_runs.ECE.std()/best_runs.ECE.mean():.2f}")
print(f"  (< 0.3 = stable, > 0.5 = inquiÃ©tant)")
```

### Ce que tu cherches

**SensibilitÃ© :** si l'ECE change drastiquement pour une petite variation d'un paramÃ¨tre, c'est un signal que le rÃ©sultat sera fragile. IdÃ©alement, il devrait y avoir un "plateau" autour de la meilleure config.

**CohÃ©rence ECE â†” MI :** les meilleures configs en ECE devraient aussi Ãªtre bonnes en MI. Si elles divergent (bon ECE, mauvais MI), la confiance est calibrÃ©e mais ne reflÃ¨te pas la vraie erreur â€” le mÃ©canisme est suspect.

**StabilitÃ© :** le coefficient de variation sur 10 runs doit Ãªtre < 0.3. Si la mÃªme config donne ECE = 0.08 sur un run et ECE = 0.35 sur un autre, c'est inutilisable.

### Feu vert / Feu rouge

| Signal | âœ… Go | âŒ Stop |
|--------|------|--------|
| Meilleur ECE | < 0.15 | > 0.25 |
| MI associÃ© | > 0.4 | < 0.2 |
| Alertes physiques | 0 | â‰¥ 2 alertes sÃ©rieuses |
| CV stabilitÃ© | < 0.3 | > 0.5 |
| CorrÃ©lation ECEâ†”MI | Concordants | Divergents |

### Si Ã§a ne passe pas

- ECE systÃ©matiquement > 0.20 â†’ le mÃ©canisme de compression scalaire perd peut-Ãªtre trop d'info. Tester un U(s) basÃ© sur la variance du buffer plutÃ´t que la moyenne.
- MI faible partout â†’ les erreurs TD ne reflÃ¨tent pas la vraie erreur de M. VÃ©rifier que `get_true_transition_matrix()` est correctement implÃ©mentÃ©.
- InstabilitÃ© â†’ augmenter K (taille buffer) Ã  30 ou 40 pour lisser le signal.

---

## Checkpoint 3 â€” RÃ©sultats Exp A (fin semaine 5)

> **Statut :** TEMPLATE â€” sera exÃ©cutable aprÃ¨s implÃ©mentation d'Exp A (Phase 3). Les fonctions `hosmer_lemeshow_test` et `bootstrap_ci` sont Ã  crÃ©er dans `prism/analysis/calibration.py`.

**DurÃ©e :** une demi-journÃ©e
**PrÃ©requis :** Exp A terminÃ©e, 100 runs Ã— 5 conditions.
**C'est le checkpoint le plus important. Si Exp A ne tient pas, le projet a un problÃ¨me fondamental.**

### Ce que Claude a fait

- 500 runs au total (100 runs Ã— 5 conditions)
- ECE, MI, reliability diagrams, heatmaps U, tests statistiques
- RÃ©sultats dans `results/exp_a/`

### Ce que tu vÃ©rifies

```python
# checkpoints/cp3_exp_a_review.py
"""
Revue complÃ¨te de l'Exp A â€” calibration et iso-structuralitÃ©.
Temps d'exÃ©cution : ~30 sec (analyse uniquement).

âš ï¸ ATTENTION : Ce script est un TEMPLATE Ã©crit avant l'implÃ©mentation d'Exp A.
   IncompatibilitÃ©s connues Ã  corriger avant exÃ©cution :
   - hosmer_lemeshow_test() n'existe pas encore dans calibration.py
   - bootstrap_ci() est dans metrics.py, pas calibration.py
   - expected_calibration_error() et reliability_diagram_data() acceptent
     (confidences, accuracies), pas un DataFrame
   - plot_uncertainty_map() accepte (U, state_mapper, grid_shape), pas des paths
   Ces issues seront corrigÃ©es lors de l'implÃ©mentation d'Exp A (Phase 3).
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import pandas as pd
from scipy import stats
from prism.analysis.calibration import (
    expected_calibration_error, reliability_diagram_data,
    sr_errors, sr_accuracies, metacognitive_index,
)
# Ã€ crÃ©er en Phase 3 :
# from prism.analysis.calibration import hosmer_lemeshow_test, bootstrap_ci
from prism.analysis.visualization import plot_uncertainty_map

# --- Charger les rÃ©sultats ---
results = {}
conditions = ["PRISM", "SR-Global", "SR-Count", "SR-Bayesian", "Random-Conf"]
for cond in conditions:
    results[cond] = pd.read_csv(f"results/exp_a/{cond}_results.csv")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FIGURE MAÃTRESSE â€” 6 panneaux
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
fig = plt.figure(figsize=(18, 12))
gs = gridspec.GridSpec(2, 3, hspace=0.35, wspace=0.3)

# --- Panel A : Reliability diagram (toutes conditions) ---
ax_rel = fig.add_subplot(gs[0, 0])
colors = {"PRISM": "#E87722", "SR-Global": "#2E75B6", "SR-Count": "#888888",
          "SR-Bayesian": "#5B9E3A", "Random-Conf": "#CCCCCC"}

for cond in conditions:
    bins_conf, bins_acc, bins_count = reliability_diagram_data(results[cond])
    mask = bins_count > 5  # ne tracer que les bins peuplÃ©s
    ax_rel.plot(bins_conf[mask], bins_acc[mask], "o-", color=colors[cond], 
                label=cond, linewidth=2, markersize=6)

ax_rel.plot([0, 1], [0, 1], "k--", alpha=0.3, label="calibration parfaite")
ax_rel.set_xlabel("Confiance dÃ©clarÃ©e C(s)")
ax_rel.set_ylabel("Accuracy observÃ©e")
ax_rel.set_title("A â€” Reliability diagram")
ax_rel.legend(fontsize=8)
ax_rel.set_xlim(0, 1)
ax_rel.set_ylim(0, 1)

# --- Panel B : Barplot ECE par condition ---
ax_ece = fig.add_subplot(gs[0, 1])
ece_values = {}
ece_cis = {}
for cond in conditions:
    eces = [expected_calibration_error(results[cond][results[cond].run == r]) 
            for r in results[cond].run.unique()]
    ece_values[cond] = np.median(eces)
    ece_cis[cond] = bootstrap_ci(eces)

x_pos = range(len(conditions))
bars = ax_ece.bar(x_pos, [ece_values[c] for c in conditions],
                  color=[colors[c] for c in conditions], alpha=0.8)
for i, cond in enumerate(conditions):
    lo, hi = ece_cis[cond]
    ax_ece.errorbar(i, ece_values[cond], yerr=[[ece_values[cond]-lo], [hi-ece_values[cond]]], 
                    color="black", capsize=4)
ax_ece.axhline(y=0.15, color="red", linestyle="--", alpha=0.5, label="seuil 0.15")
ax_ece.set_xticks(x_pos)
ax_ece.set_xticklabels([c.replace("SR-", "") for c in conditions], rotation=30, ha="right")
ax_ece.set_ylabel("ECE (â†“ meilleur)")
ax_ece.set_title("B â€” Expected Calibration Error")
ax_ece.legend()

# --- Panel C : Barplot MI par condition ---
ax_mi = fig.add_subplot(gs[0, 2])
mi_values = {}
for cond in conditions:
    mis = [results[cond][results[cond].run == r][["U_s", "true_error"]].corr().iloc[0,1]
           for r in results[cond].run.unique()]
    mi_values[cond] = np.median(mis)

ax_mi.bar(x_pos, [mi_values[c] for c in conditions],
          color=[colors[c] for c in conditions], alpha=0.8)
ax_mi.axhline(y=0.5, color="green", linestyle="--", alpha=0.5, label="seuil 0.5")
ax_mi.set_xticks(x_pos)
ax_mi.set_xticklabels([c.replace("SR-", "") for c in conditions], rotation=30, ha="right")
ax_mi.set_ylabel("Metacognitive Index (â†‘ meilleur)")
ax_mi.set_title("C â€” MI = corr(U(s), erreur rÃ©elle)")
ax_mi.legend()

# --- Panel D : Heatmap U(s) superposÃ©e au monde (phase apprentissage) ---
ax_u1 = fig.add_subplot(gs[1, 0])
plot_uncertainty_map(ax_u1, "results/exp_a/PRISM_U_phase1.npy",
                     "results/exp_a/world_layout.npy",
                     title="D â€” Carte U aprÃ¨s apprentissage (300 Ã©ps)")

# --- Panel E : Heatmap U(s) aprÃ¨s ouverture 5e piÃ¨ce ---
ax_u2 = fig.add_subplot(gs[1, 1])
plot_uncertainty_map(ax_u2, "results/exp_a/PRISM_U_phase2.npy",
                     "results/exp_a/world_layout_expanded.npy",
                     title="E â€” Carte U aprÃ¨s ouverture nouvelle zone")

# --- Panel F : Ã‰volution temporelle U(s) dans la nouvelle zone ---
ax_time = fig.add_subplot(gs[1, 2])
u_timeline = np.load("results/exp_a/PRISM_U_timeline_newzone.npy")
# shape : (n_episodes, n_states_new_zone)
mean_u = u_timeline.mean(axis=1)
std_u = u_timeline.std(axis=1)
episodes = np.arange(len(mean_u))
ax_time.fill_between(episodes, mean_u - std_u, mean_u + std_u, alpha=0.2, color="#E87722")
ax_time.plot(episodes, mean_u, color="#E87722", linewidth=2)
ax_time.axvline(x=0, color="red", linestyle="--", alpha=0.5, label="ouverture zone")
ax_time.set_xlabel("Ã‰pisodes aprÃ¨s ouverture")
ax_time.set_ylabel("U(s) moyen dans nouvelle zone")
ax_time.set_title("F â€” DÃ©croissance de l'incertitude")
ax_time.legend()

plt.suptitle("CHECKPOINT 3 â€” Exp A : Calibration mÃ©tacognitive", fontsize=16, y=1.01)
plt.savefig("results/cp3_exp_a_master.png", dpi=150, bbox_inches="tight")
plt.show()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTS STATISTIQUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "="*60)
print("CP3 â€” TESTS STATISTIQUES")
print("="*60)

# ECE : PRISM vs chaque baseline
prism_eces = [expected_calibration_error(results["PRISM"][results["PRISM"].run == r]) 
              for r in results["PRISM"].run.unique()]

for cond in ["SR-Global", "SR-Count", "SR-Bayesian"]:
    other_eces = [expected_calibration_error(results[cond][results[cond].run == r]) 
                  for r in results[cond].run.unique()]
    stat, p = stats.mannwhitneyu(prism_eces, other_eces, alternative="less")
    d = (np.mean(other_eces) - np.mean(prism_eces)) / np.sqrt(
        (np.std(prism_eces)**2 + np.std(other_eces)**2) / 2)
    print(f"\n  PRISM vs {cond} (ECE) :")
    print(f"    Mann-Whitney U : p = {p:.6f} {'âœ…' if p < 0.05 else 'âŒ'}")
    print(f"    Taille d'effet (Cohen's d) : {d:.3f}")
    print(f"    ECE mÃ©dian : PRISM={np.median(prism_eces):.4f} vs {cond}={np.median(other_eces):.4f}")

# Hosmer-Lemeshow
for cond in ["PRISM", "SR-Global", "SR-Count"]:
    hl_stat, hl_p = hosmer_lemeshow_test(results[cond])
    print(f"\n  Hosmer-Lemeshow ({cond}) : Ï‡Â²={hl_stat:.2f}, p={hl_p:.4f} "
          f"{'âœ… calibrÃ©' if hl_p > 0.05 else 'âŒ mal calibrÃ©'}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VERDICT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "="*60)
print("CP3 â€” VERDICT")
print("="*60)
ece_ok = np.median(prism_eces) < 0.15
mi_ok = mi_values["PRISM"] > 0.5
beats_global = ece_values["PRISM"] < ece_values["SR-Global"]
beats_count = ece_values["PRISM"] < ece_values["SR-Count"]

checks = [
    ("ECE(PRISM) < 0.15", ece_ok),
    ("MI(PRISM) > 0.5", mi_ok),
    ("ECE(PRISM) < ECE(SR-Global)", beats_global),
    ("ECE(PRISM) < ECE(SR-Count)", beats_count),
]
for label, passed in checks:
    print(f"  {'âœ…' if passed else 'âŒ'} {label}")

all_pass = all(p for _, p in checks)
print(f"\n  {'ğŸŸ¢ GO â€” passer Ã  Phase 3' if all_pass else 'ğŸ”´ STOP â€” diagnostic nÃ©cessaire'}")
```

### Ce que tu cherches visuellement

**Reliability diagram (Panel A) :** la courbe PRISM (orange) doit Ãªtre la plus proche de la diagonale pointillÃ©e. Si SR-Bayesian est aussi bon ou meilleur, c'est un rÃ©sultat intÃ©ressant mais Ã§a affaiblit la thÃ¨se â€” note-le. VÃ©rifie que les bins sont peuplÃ©s : des marqueurs sans points = artÃ©fact.

**Carte U (Panels D-E) :** aprÃ¨s ouverture de la 5e piÃ¨ce, la nouvelle zone doit Ãªtre rouge vif (haute incertitude). Les piÃ¨ces dÃ©jÃ  apprises doivent Ãªtre vertes/bleues. Les portes doivent montrer une lÃ©gÃ¨re incertitude rÃ©siduelle (elles sont des points de transition). Si tout est uniformÃ©ment jaune, U n'a pas de structure spatiale.

**DÃ©croissance (Panel F) :** l'incertitude dans la nouvelle zone doit dÃ©croÃ®tre progressivement sur 20-50 Ã©pisodes. Si elle tombe Ã  zÃ©ro en 2 Ã©pisodes, le buffer est trop court. Si elle ne descend pas aprÃ¨s 100 Ã©pisodes, l'apprentissage ne se fait pas.

### Si Ã§a ne passe pas

- ECE > 0.15 mais MI > 0.5 â†’ la confiance contient de l'info mais la transformation C(s) = sigmoid(U(s)) est mal calibrÃ©e. Ajuster Î² et Î¸_C.
- MI < 0.3 â†’ problÃ¨me fondamental : U(s) ne corrÃ¨le pas avec l'erreur rÃ©elle de M. VÃ©rifier que les erreurs TD sont calculÃ©es correctement. PossibilitÃ© que la norme L2 perde trop d'info â†’ tester variance du buffer au lieu de la moyenne.
- SR-Bayesian bat PRISM sur tout â†’ rÃ©sultat nÃ©gatif pour la thÃ¨se, mais honnÃªte. Documenter et discuter.

---

## Checkpoint 4 â€” RÃ©sultats Exp B (milieu semaine 7)

> **Statut :** TEMPLATE â€” sera exÃ©cutable aprÃ¨s implÃ©mentation d'Exp B (Phase 3).

**DurÃ©e :** 1-2h
**PrÃ©requis :** Exp B terminÃ©e, 100 runs Ã— 8 conditions.

### Ce que Claude a fait

- Grand monde 19Ã—19 avec 4 goals cachÃ©s
- 8 conditions comparÃ©es
- RÃ©sultats dans `results/exp_b/`

### Ce que tu vÃ©rifies

```python
# checkpoints/cp4_exp_b_review.py
"""
Revue Exp B â€” exploration dirigÃ©e par incertitude.
Temps d'exÃ©cution : ~20 sec.
"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats

# --- Charger ---
df = pd.read_csv("results/exp_b/exploration_results.csv")
# Colonnes : condition, run, steps, goals_found, all_found, coverage, redundancy,
#             discovery_1, discovery_2, discovery_3, discovery_4

conditions_order = ["SR-Oracle", "PRISM", "SR-Posterior", "SR-Count-Bonus",
                    "SR-Norm-Bonus", "SR-e-greedy", "SR-e-decay", "Random"]
colors = {"SR-Oracle": "#2E75B6", "PRISM": "#E87722", "SR-Posterior": "#5B9E3A",
          "SR-Count-Bonus": "#888888", "SR-Norm-Bonus": "#AAAAAA",
          "SR-e-greedy": "#CCCCCC", "SR-e-decay": "#DDDDDD", "Random": "#EEEEEE"}

# --- Figure 1 : Boxplot steps ---
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Panel A : Steps pour trouver les 4 goals
data_steps = [df[df.condition == c].steps.values for c in conditions_order]
bp = axes[0].boxplot(data_steps, labels=[c.replace("SR-", "") for c in conditions_order],
                     patch_artist=True, showfliers=False)
for patch, cond in zip(bp["boxes"], conditions_order):
    patch.set_facecolor(colors[cond])
axes[0].set_ylabel("Steps pour 4 goals (â†“ meilleur)")
axes[0].set_title("A â€” EfficacitÃ© d'exploration")
axes[0].tick_params(axis="x", rotation=35)

# Panel B : Couverture finale
data_cov = [df[df.condition == c].coverage.astype(float).values for c in conditions_order]
bp2 = axes[1].boxplot(data_cov, labels=[c.replace("SR-", "") for c in conditions_order],
                      patch_artist=True, showfliers=False)
for patch, cond in zip(bp2["boxes"], conditions_order):
    patch.set_facecolor(colors[cond])
axes[1].set_ylabel("Couverture finale (â†‘ meilleur)")
axes[1].set_title("B â€” Couverture")
axes[1].tick_params(axis="x", rotation=35)

# Panel C : Efficiency ratio
prism_steps = df[df.condition == "PRISM"].steps.median()
oracle_steps = df[df.condition == "SR-Oracle"].steps.median()
random_steps = df[df.condition == "Random"].steps.median()
eff_ratio = (random_steps - prism_steps) / (random_steps - oracle_steps)

for cond in conditions_order:
    med = df[df.condition == cond].steps.median()
    r = (random_steps - med) / (random_steps - oracle_steps)
    axes[2].barh(cond.replace("SR-", ""), r, color=colors[cond], alpha=0.8)
axes[2].axvline(x=0.5, color="green", linestyle="--", alpha=0.5)
axes[2].axvline(x=1.0, color="blue", linestyle="--", alpha=0.3)
axes[2].set_xlabel("Efficiency ratio (1.0 = Oracle)")
axes[2].set_title("C â€” Fraction du gain thÃ©orique capturÃ©")

plt.suptitle("CHECKPOINT 4 â€” Exp B : Exploration dirigÃ©e", fontsize=14)
plt.tight_layout()
plt.savefig("results/cp4_exp_b.png", dpi=150)
plt.show()

# --- Tests statistiques ---
print("\n" + "="*60)
print("CP4 â€” TESTS STATISTIQUES")
print("="*60)

prism_data = df[df.condition == "PRISM"].steps.values
for cond in ["SR-e-greedy", "SR-Count-Bonus", "SR-Norm-Bonus"]:
    other = df[df.condition == cond].steps.values
    stat, p = stats.mannwhitneyu(prism_data, other, alternative="less")
    improvement = 1 - np.median(prism_data) / np.median(other)
    print(f"\n  PRISM vs {cond} :")
    print(f"    Mann-Whitney p = {p:.6f} {'âœ…' if p < 0.05 else 'âŒ'}")
    print(f"    AmÃ©lioration mÃ©diane : {improvement*100:.1f}%")

print(f"\n  Efficiency ratio PRISM : {eff_ratio:.3f} "
      f"{'âœ…' if eff_ratio > 0.5 else 'âš ï¸'} (cible > 0.5)")

# --- Trajectoires individuelles Ã  inspecter ---
print("\n" + "-"*60)
print("TRAJECTOIRES Ã€ INSPECTER VISUELLEMENT :")
print("(vÃ©rifie que les trajectoires ont du sens spatial)")
prism_runs = df[df.condition == "PRISM"].sort_values("steps")
print(f"  Meilleur run  : run #{prism_runs.iloc[0].run:.0f} ({prism_runs.iloc[0].steps:.0f} steps)")
print(f"  Run mÃ©dian    : run #{prism_runs.iloc[len(prism_runs)//2].run:.0f}")
print(f"  Pire run      : run #{prism_runs.iloc[-1].run:.0f} ({prism_runs.iloc[-1].steps:.0f} steps)")
print(f"\n  Lance : python checkpoints/cp4_show_trajectory.py --run <N>")
```

```python
# checkpoints/cp4_show_trajectory.py
"""
Visualise la trajectoire d'un run spÃ©cifique.
Usage : python checkpoints/cp4_show_trajectory.py --run 42
"""
import argparse
import numpy as np
import matplotlib.pyplot as plt
from prism.env.state_mapper import StateMapper

parser = argparse.ArgumentParser()
parser.add_argument("--run", type=int, required=True)
parser.add_argument("--condition", default="PRISM")
args = parser.parse_args()

traj = np.load(f"results/exp_b/trajectories/{args.condition}_run{args.run}.npy")
world = np.load("results/exp_b/world_layout_large.npy")
goals = np.load("results/exp_b/goal_positions.npy")

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
ax.imshow(world, cmap="gray_r", alpha=0.3)

# Trajectoire colorÃ©e par temps
n = len(traj)
for i in range(n - 1):
    color = plt.cm.plasma(i / n)
    ax.plot([traj[i, 1], traj[i+1, 1]], [traj[i, 0], traj[i+1, 0]], 
            color=color, linewidth=0.5, alpha=0.6)

# Goals
for g in goals:
    ax.plot(g[1], g[0], "g*", markersize=20)

# DÃ©part
ax.plot(traj[0, 1], traj[0, 0], "rs", markersize=12, label="dÃ©part")

ax.set_title(f"{args.condition} â€” Run #{args.run} ({n} steps)\n"
             f"Couleur : violet=dÃ©but â†’ jaune=fin")
ax.legend()
plt.tight_layout()
plt.savefig(f"results/cp4_trajectory_{args.condition}_{args.run}.png", dpi=150)
plt.show()
```

### Ce que tu cherches

**Boxplots :** PRISM doit Ãªtre clairement Ã  gauche (moins de steps) que Îµ-greedy et Count-Bonus. Si PRISM est dans le mÃªme paquet, la structure n'aide pas.

**Trajectoires :** le meilleur run PRISM devrait montrer un pattern d'exploration systÃ©matique â€” visiter une piÃ¨ce, puis se diriger vers la suivante. Le pire run peut Ãªtre chaotique, mais le run mÃ©dian doit Ãªtre cohÃ©rent. Si mÃªme le meilleur run zigzague alÃ©atoirement â†’ la carte U ne guide pas rÃ©ellement le comportement.

**Efficiency ratio :** > 0.5 signifie que PRISM capture plus de la moitiÃ© du gain que l'oracle (qui connaÃ®t les vraies erreurs) obtient. C'est ambitieux mais atteignable.

---

## Checkpoint 5 â€” RÃ©sultats Exp C + synthÃ¨se (fin semaine 7)

> **Statut :** TEMPLATE â€” sera exÃ©cutable aprÃ¨s implÃ©mentation d'Exp C (Phase 3).

**DurÃ©e :** 2-3h
**PrÃ©requis :** Exp C terminÃ©e.

### Ce que tu vÃ©rifies

```python
# checkpoints/cp5_exp_c_review.py
"""
Revue Exp C â€” adaptation au changement + asymÃ©trie R/M.
Temps d'exÃ©cution : ~20 sec.
"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv("results/exp_c/adaptation_results.csv")
# Colonnes : condition, run, phase, episode, reward, U_mean, 
#            change_detected, ECE_window

# --- Phase timeline ---
phase_boundaries = {
    "stable": (0, 200),
    "perturb_R": (200, 300),
    "restab_1": (300, 400),
    "perturb_M": (400, 500),
    "restab_2": (500, 600),
}

fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)

for cond, color in [("PRISM", "#E87722"), ("SR-Blind", "#2E75B6"), ("Q-Learning", "#888888")]:
    cond_df = df[df.condition == cond].groupby("episode").agg(
        reward_mean=("reward", "mean"),
        reward_std=("reward", "std"),
        U_mean=("U_mean", "mean"),
        ECE_mean=("ECE_window", "mean"),
    ).reset_index()
    
    ep = cond_df.episode
    
    # Panel A : Performance (reward)
    axes[0].plot(ep, cond_df.reward_mean, color=color, label=cond, linewidth=1.5)
    axes[0].fill_between(ep, cond_df.reward_mean - cond_df.reward_std,
                         cond_df.reward_mean + cond_df.reward_std, alpha=0.1, color=color)
    
    # Panel B : Incertitude moyenne
    if "U_mean" in cond_df.columns and cond != "Q-Learning":
        axes[1].plot(ep, cond_df.U_mean, color=color, label=cond, linewidth=1.5)
    
    # Panel C : ECE glissant
    axes[2].plot(ep, cond_df.ECE_mean, color=color, label=cond, linewidth=1.5)

# Phases
for ax in axes:
    for phase, (start, end) in phase_boundaries.items():
        if "perturb" in phase:
            ax.axvspan(start, end, alpha=0.1, color="red")
    ax.axvline(200, color="red", linestyle="--", alpha=0.3)
    ax.axvline(400, color="red", linestyle="--", alpha=0.3)

axes[0].set_ylabel("Reward moyen")
axes[0].set_title("A â€” Performance")
axes[0].legend()
axes[1].set_ylabel("U(s) moyen")
axes[1].set_title("B â€” Incertitude")
axes[1].legend()
axes[2].set_ylabel("ECE (fenÃªtre 20 Ã©ps)")
axes[2].set_title("C â€” Calibration dynamique")
axes[2].axhline(y=0.20, color="red", linestyle=":", alpha=0.5, label="seuil 0.20")
axes[2].legend()
axes[2].set_xlabel("Ã‰pisode")

plt.suptitle("CHECKPOINT 5 â€” Exp C : Adaptation au changement", fontsize=14)
plt.tight_layout()
plt.savefig("results/cp5_exp_c.png", dpi=150)
plt.show()

# --- AsymÃ©trie R/M ---
print("\n" + "="*60)
print("CP5 â€” ASYMÃ‰TRIE R/M")
print("="*60)

prism_df = df[df.condition == "PRISM"]

# Latence adaptation R (Ã©pisodes 200-300)
pre_perf = prism_df[(prism_df.episode >= 180) & (prism_df.episode < 200)].reward.mean()
threshold_R = 0.8 * pre_perf
post_R = prism_df[(prism_df.episode >= 200) & (prism_df.episode < 300)]
for ep in post_R.episode.unique():
    ep_perf = post_R[post_R.episode == ep].reward.mean()
    if ep_perf >= threshold_R:
        latence_R = ep - 200
        break
else:
    latence_R = 100  # pas convergÃ©

# Latence adaptation M (Ã©pisodes 400-500)
pre_perf_M = prism_df[(prism_df.episode >= 380) & (prism_df.episode < 400)].reward.mean()
threshold_M = 0.8 * pre_perf_M
post_M = prism_df[(prism_df.episode >= 400) & (prism_df.episode < 600)]
for ep in post_M.episode.unique():
    ep_perf = post_M[post_M.episode == ep].reward.mean()
    if ep_perf >= threshold_M:
        latence_M = ep - 400
        break
else:
    latence_M = 200

ratio = latence_M / max(latence_R, 1)
print(f"  Latence adaptation R : {latence_R} Ã©pisodes")
print(f"  Latence adaptation M : {latence_M} Ã©pisodes")
print(f"  Ratio M/R : {ratio:.1f}Ã—")
print(f"  Plage prÃ©dite : 15â€“40Ã—")
if 10 < ratio < 60:
    print(f"  âœ… Compatible avec la signature SR")
elif ratio < 5:
    print(f"  âš ï¸  Ratio trop bas â€” possiblement model-based plutÃ´t que SR")
else:
    print(f"  âš ï¸  Ratio trop haut â€” M ne se rÃ©apprend peut-Ãªtre pas correctement")

# --- Latence de dÃ©tection ---
detect_R = prism_df[(prism_df.episode >= 200) & (prism_df.change_detected == True)].episode.min() - 200
detect_M = prism_df[(prism_df.episode >= 400) & (prism_df.change_detected == True)].episode.min() - 400
print(f"\n  Latence dÃ©tection R : {detect_R} Ã©pisodes {'âœ…' if detect_R < 10 else 'âŒ'}")
print(f"  Latence dÃ©tection M : {detect_M} Ã©pisodes {'âœ…' if detect_M < 10 else 'âŒ'}")

# --- ECE pendant transitions ---
ece_trans_R = prism_df[(prism_df.episode >= 200) & (prism_df.episode < 220)].ECE_window.mean()
ece_trans_M = prism_df[(prism_df.episode >= 400) & (prism_df.episode < 420)].ECE_window.mean()
print(f"\n  ECE pendant transition R : {ece_trans_R:.3f} {'âœ…' if ece_trans_R < 0.20 else 'âŒ'}")
print(f"  ECE pendant transition M : {ece_trans_M:.3f} {'âœ…' if ece_trans_M < 0.20 else 'âŒ'}")

# --- Verdict global ---
print("\n" + "="*60)
print("CP5 â€” VERDICT GLOBAL DU PROJET")
print("="*60)
exp_a = pd.read_csv("results/exp_a/summary.csv")  # rÃ©sumÃ© gÃ©nÃ©rÃ© par Exp A
exp_b = pd.read_csv("results/exp_b/summary.csv")

verdicts = [
    ("Exp A â€” ECE < 0.15", exp_a[exp_a.condition == "PRISM"].ECE_median.iloc[0] < 0.15),
    ("Exp A â€” MI > 0.5", exp_a[exp_a.condition == "PRISM"].MI_median.iloc[0] > 0.5),
    ("Exp A â€” PRISM bat SR-Global", 
     exp_a[exp_a.condition == "PRISM"].ECE_median.iloc[0] < exp_a[exp_a.condition == "SR-Global"].ECE_median.iloc[0]),
    ("Exp B â€” PRISM bat e-greedy",
     exp_b[exp_b.condition == "PRISM"].steps_median.iloc[0] < exp_b[exp_b.condition == "SR-e-greedy"].steps_median.iloc[0]),
    ("Exp B â€” PRISM bat Count-Bonus",
     exp_b[exp_b.condition == "PRISM"].steps_median.iloc[0] < exp_b[exp_b.condition == "SR-Count-Bonus"].steps_median.iloc[0]),
    ("Exp C â€” DÃ©tection < 10 Ã©ps", max(detect_R, detect_M) < 10),
    ("Exp C â€” AsymÃ©trie R/M dans [10, 60]", 10 < ratio < 60),
    ("Exp C â€” ECE transitions < 0.20", max(ece_trans_R, ece_trans_M) < 0.20),
]

n_pass = sum(1 for _, v in verdicts if v)
for label, passed in verdicts:
    print(f"  {'âœ…' if passed else 'âŒ'} {label}")

print(f"\n  Score : {n_pass}/{len(verdicts)}")
if n_pass == len(verdicts):
    print("  ğŸŸ¢ RÃ©sultats solides â€” rÃ©diger le rapport avec confiance")
elif n_pass >= 6:
    print("  ğŸŸ¡ RÃ©sultats partiels â€” rÃ©diger en nuanÃ§ant les points faibles")
elif n_pass >= 4:
    print("  ğŸŸ  RÃ©sultats mitigÃ©s â€” discuter honnÃªtement les limites")
else:
    print("  ğŸ”´ RÃ©sultats nÃ©gatifs â€” pivoter vers l'analyse de pourquoi Ã§a ne marche pas")
```

### Ce que tu cherches

**Panel A (performance) :** le reward doit chuter au moment des perturbations (Ã©pisodes 200 et 400) puis remonter. PRISM doit remonter plus vite que SR-Blind. Q-Learning servira de contraste.

**Panel B (incertitude) :** U(s) doit montrer un pic net Ã  chaque perturbation puis redescendre. Si U ne rÃ©agit pas au changement de type R (Ã©pisode 200), c'est normal â€” la SR n'est pas affectÃ©e. Si U ne rÃ©agit pas au changement de type M (Ã©pisode 400), c'est un bug.

**AsymÃ©trie :** c'est le test le plus informatif thÃ©oriquement. Le ratio 15-40Ã— est dÃ©rivÃ© de l'architecture. Un ratio dans cette plage confirme que l'agent fonctionne bien comme un agent SR.

---

## MÃ©mo â€” ce que tu ne vÃ©rifies PAS

Pour clarifier, voici ce qui ne demande pas ton attention :

- Tests unitaires (automatisÃ©s, `pytest`)
- Baselines SB3 (wrapper standard)
- Calculs statistiques (Mann-Whitney, bootstrap, Holm-Bonferroni â€” automatisÃ©s dans les scripts ci-dessus)
- Logging et sauvegarde des rÃ©sultats
- Formatage des figures non-critiques
- DÃ©composition spectrale (code adaptÃ©, validÃ© au CP1)
- GÃ©nÃ©ration des CSVs de rÃ©sultats

---

*DerniÃ¨re mise Ã  jour : 2026-02-14*
